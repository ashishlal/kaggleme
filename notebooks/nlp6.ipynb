{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# mainly forking from notebook\n",
    "# https://www.kaggle.com/johnfarrell/simple-rnn-with-keras-script\n",
    "\n",
    "# ADDED\n",
    "# 5x scaled test set\n",
    "# category name embedding\n",
    "# some small changes like lr, decay, batch_size~\n",
    "\n",
    "# In[ ]:\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#KERAS MODEL DEFINITION\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, \\\n",
    "    Activation, concatenate, GRU, LSTM, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "NAME_MIN_DF = 10\n",
    "MAX_FEATURES_ITEM_DESCRIPTION = 2 ** 14\n",
    "NUM_PARTITIONS = 12 #number of partitions to split dataframe\n",
    "NUM_CORES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    import math\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 \\\n",
    "              for i, pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n",
    "\n",
    "def print_memory_usage():\n",
    "    print('cpu: {}'.format(psutil.cpu_percent()))\n",
    "    print('consuming {:.2f}GB RAM'.format(\n",
    "           psutil.Process(os.getpid()).memory_info().rss / 1073741824),\n",
    "          flush=True)\n",
    "\n",
    "dr = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu: 99.8\n",
      "consuming 1.01GB RAM\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.tsv', sep='\\t')\n",
    "test = pd.read_csv('../data/test.tsv', sep='\\t')\n",
    "\n",
    "train = train[train.price != 0]\n",
    "train['target'] = np.log1p(train['price'])\n",
    "test_id = test.test_id.values\n",
    "y = np.log1p(train['price']).values\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481661, 9)\n",
      "5 folds scaling the test_df\n",
      "(693359, 7)\n",
      "new shape  (3493359, 7)\n",
      "[12.857431650161743] Finished scaling test set...\n",
      "cpu: 99.9\n",
      "consuming 1.22GB RAM\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print('5 folds scaling the test_df')\n",
    "print(test.shape)\n",
    "test_len = test.shape[0]\n",
    "def simulate_test(test):\n",
    "    if test.shape[0] < 800000:\n",
    "        indices = np.random.choice(test.index.values, 2800000)\n",
    "        test_ = pd.concat([test, test.iloc[indices]], axis=0)\n",
    "        return test_.copy()\n",
    "    else:\n",
    "        return test\n",
    "test = simulate_test(test)\n",
    "print('new shape ', test.shape)\n",
    "print('[{}] Finished scaling test set...'.format(time.time() - start_time))\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "(1481661, 9)\n",
      "(3493359, 7)\n",
      "[14.575852394104004] Finished handling missing data...\n",
      "cpu: 100.0\n",
      "consuming 1.22GB RAM\n"
     ]
    }
   ],
   "source": [
    "#HANDLE MISSING VALUES\n",
    "print(\"Handling missing values...\")\n",
    "def handle_missing(dataset):\n",
    "    dataset.category_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.item_description.fillna(value=\"missing\", inplace=True)\n",
    "    return (dataset)\n",
    "\n",
    "train = handle_missing(train)\n",
    "test = handle_missing(test)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "print('[{}] Finished handling missing data...'.format(time.time() - start_time))\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76.2453544139862] Finished PROCESSING CATEGORICAL DATA...\n",
      "cpu: 97.0\n",
      "consuming 1.31GB RAM\n"
     ]
    }
   ],
   "source": [
    "#PROCESS CATEGORICAL DATA\n",
    "\n",
    "# print(\"Handling categorical variables...\")\n",
    "# # le = LabelBinarizer(sparse_output=True)\n",
    "\n",
    "# lb = LabelBinarizer(sparse_output=True)\n",
    "# X_brand = lb.fit_transform(merge['brand_name'])\n",
    "# print('[{}] Finished label binarize `brand_name`'.format(time.time() - start_time))\n",
    "# print(X_brand.shape)\n",
    "# del merge['brand_name']\n",
    "# print_memory_usage()\n",
    "\n",
    "# lb = LabelBinarizer(sparse_output=True)\n",
    "# X_bci = lb.fit_transform(merge['bci'])\n",
    "# print('[{}] Finished label binarize `bci`'.format(time.time() - start_time))\n",
    "# print(X_bci.shape)\n",
    "# del merge['bci']\n",
    "# print_memory_usage()\n",
    "\n",
    "# lb = LabelBinarizer(sparse_output=True)\n",
    "# X_bcis = lb.fit_transform(merge['bcis'])\n",
    "# print('[{}] Finished label binarize `bcis`'.format(time.time() - start_time))\n",
    "# print(X_bcis.shape)\n",
    "# del merge['bcis']\n",
    "# gc.collect()\n",
    "# print_memory_usage()\n",
    "\n",
    "# lb = LabelBinarizer(sparse_output=True)\n",
    "# X_bcs = lb.fit_transform(merge['bcs'])\n",
    "# print('[{}] Finished label binarize `bcs`'.format(time.time() - start_time))\n",
    "# print(X_bcs.shape)\n",
    "# del merge['bcs']\n",
    "# gc.collect()\n",
    "# print_memory_usage()\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# X_cat_name = lb.fit_transform(merge['category_name'])\n",
    "# scaler = MaxAbsScaler()\n",
    "# X_cat_name = scaler.fit_transform(X_cat_name)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(np.hstack([train.category_name, test.category_name]))\n",
    "train['category'] = le.transform(train.category_name)\n",
    "test['category'] = le.transform(test.category_name)\n",
    "\n",
    "le.fit(np.hstack([train.brand_name, test.brand_name]))\n",
    "train['brand'] = le.transform(train.brand_name)\n",
    "test['brand'] = le.transform(test.brand_name)\n",
    "del le, train['brand_name'], test['brand_name']\n",
    "\n",
    "print('[{}] Finished PROCESSING CATEGORICAL DATA...'.format(time.time() - start_time))\n",
    "print_memory_usage()\n",
    "# train.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train_id                                 name  item_condition_id  \\\n",
      "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
      "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
      "2         2                       AVA-VIV Blouse                  1   \n",
      "\n",
      "                                       category_name  price  shipping  \\\n",
      "0                                  Men/Tops/T-shirts   10.0         1   \n",
      "1  Electronics/Computers & Tablets/Components & P...   52.0         0   \n",
      "2                        Women/Tops & Blouses/Blouse   10.0         1   \n",
      "\n",
      "                                    item_description    target  category  \\\n",
      "0                                 No description yet  2.397895       829   \n",
      "1  This keyboard is in great condition and works ...  3.970292        86   \n",
      "2  Adorable top with a hint of lace and a key hol...  2.397895      1277   \n",
      "\n",
      "   brand  \n",
      "0   5263  \n",
      "1   3887  \n",
      "2   4586  \n"
     ]
    }
   ],
   "source": [
    "print(train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to seq process...\n",
      "   Fitting tokenizer...\n",
      "   Transforming text to seq...\n",
      "   train_id                                 name  item_condition_id  \\\n",
      "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
      "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
      "2         2                       AVA-VIV Blouse                  1   \n",
      "\n",
      "                                       category_name  price  shipping  \\\n",
      "0                                  Men/Tops/T-shirts   10.0         1   \n",
      "1  Electronics/Computers & Tablets/Components & P...   52.0         0   \n",
      "2                        Women/Tops & Blouses/Blouse   10.0         1   \n",
      "\n",
      "                                    item_description    target  category  \\\n",
      "0                                 No description yet  2.397895       829   \n",
      "1  This keyboard is in great condition and works ...  3.970292        86   \n",
      "2  Adorable top with a hint of lace and a key hol...  2.397895      1277   \n",
      "\n",
      "   brand           seq_category_name  \\\n",
      "0   5263            [77, 41, 71, 72]   \n",
      "1   3887  [62, 922, 828, 3281, 1381]   \n",
      "2   4586            [2, 41, 75, 277]   \n",
      "\n",
      "                                seq_item_description  \\\n",
      "0                                      [13, 88, 102]   \n",
      "1  [33, 2747, 11, 8, 50, 18, 1, 257, 65, 21, 1219...   \n",
      "2  [702, 74, 10, 5, 5465, 12, 244, 1, 5, 992, 140...   \n",
      "\n",
      "                             seq_name  \n",
      "0  [2491, 8914, 6992, 71, 99, 7, 199]  \n",
      "1         [10839, 25624, 16431, 2747]  \n",
      "2                  [7728, 10643, 277]  \n",
      "[610.447954416275] Finished PROCESSING TEXT DATA...\n",
      "cpu: 99.2\n",
      "consuming 4.67GB RAM\n"
     ]
    }
   ],
   "source": [
    "#PROCESS TEXT: RAW\n",
    "print(\"Text to seq process...\")\n",
    "print(\"   Fitting tokenizer...\")\n",
    "\n",
    "raw_text = np.hstack([train.category_name.str.lower(), \n",
    "                      train.item_description.str.lower(), \n",
    "                      train.name.str.lower()])\n",
    "\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "print(\"   Transforming text to seq...\")\n",
    "train[\"seq_category_name\"] = tok_raw.texts_to_sequences(train.category_name.str.lower())\n",
    "test[\"seq_category_name\"] = tok_raw.texts_to_sequences(test.category_name.str.lower())\n",
    "train[\"seq_item_description\"] = tok_raw.texts_to_sequences(train.item_description.str.lower())\n",
    "test[\"seq_item_description\"] = tok_raw.texts_to_sequences(test.item_description.str.lower())\n",
    "train[\"seq_name\"] = tok_raw.texts_to_sequences(train.name.str.lower())\n",
    "test[\"seq_name\"] = tok_raw.texts_to_sequences(test.name.str.lower())\n",
    "print(train.head(3))\n",
    "\n",
    "print('[{}] Finished PROCESSING TEXT DATA...'.format(time.time() - start_time))\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1466844, 13)\n",
      "(14817, 13)\n",
      "cpu: 99.9\n",
      "consuming 4.89GB RAM\n"
     ]
    }
   ],
   "source": [
    "#EXTRACT DEVELOPTMENT TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "dtrain, dvalid = train_test_split(train, random_state=233, train_size=0.99)\n",
    "print(dtrain.shape)\n",
    "print(dvalid.shape)\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[615.648087978363] Finished EMBEDDINGS MAX VALUE...\n",
      "cpu: 99.9\n",
      "consuming 4.89GB RAM\n"
     ]
    }
   ],
   "source": [
    "#EMBEDDINGS MAX VALUE\n",
    "#Base on the histograms, we select the next lengths\n",
    "MAX_NAME_SEQ = 20 #17\n",
    "MAX_ITEM_DESC_SEQ = 60 #269\n",
    "MAX_CATEGORY_NAME_SEQ = 20 #8\n",
    "MAX_TEXT = np.max([np.max(train.seq_name.max())\n",
    "                   , np.max(test.seq_name.max())\n",
    "                   , np.max(train.seq_category_name.max())\n",
    "                   , np.max(test.seq_category_name.max())\n",
    "                   , np.max(train.seq_item_description.max())\n",
    "                   , np.max(test.seq_item_description.max())])+2\n",
    "MAX_CATEGORY = np.max([train.category.max(), test.category.max()])+1\n",
    "MAX_BRAND = np.max([train.brand.max(), test.brand.max()])+1\n",
    "MAX_CONDITION = np.max([train.item_condition_id.max(), \n",
    "                        test.item_condition_id.max()])+1\n",
    "\n",
    "print('[{}] Finished EMBEDDINGS MAX VALUE...'.format(time.time() - start_time))\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[777.1734340190887] Finished DATA PREPARARTION...\n",
      "cpu: 99.8\n",
      "consuming 6.94GB RAM\n"
     ]
    }
   ],
   "source": [
    "#KERAS DATA DEFINITION\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_keras_data(dataset):\n",
    "    X = {\n",
    "        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n",
    "        ,'item_desc': pad_sequences(dataset.seq_item_description\n",
    "                                    , maxlen=MAX_ITEM_DESC_SEQ)\n",
    "        ,'brand': np.array(dataset.brand)\n",
    "        ,'category': np.array(dataset.category)\n",
    "        ,'category_name': pad_sequences(dataset.seq_category_name\n",
    "                                        , maxlen=MAX_CATEGORY_NAME_SEQ)\n",
    "        ,'item_condition': np.array(dataset.item_condition_id)\n",
    "        ,'num_vars': np.array(dataset[[\"shipping\"]])\n",
    "    }\n",
    "    return X\n",
    "\n",
    "X_train = get_keras_data(dtrain)\n",
    "X_valid = get_keras_data(dvalid)\n",
    "X_test = get_keras_data(test)\n",
    "\n",
    "print('[{}] Finished DATA PREPARARTION...'.format(time.time() - start_time))\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1466844, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['name'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1466844, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['category_name'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[777.4661664962769] Finished DEFINEING MODEL...\n",
      "cpu: 99.4\n",
      "consuming 6.94GB RAM\n"
     ]
    }
   ],
   "source": [
    "dr = 0.2\n",
    "\n",
    "def get_model():\n",
    "    #params\n",
    "    dr_r = dr\n",
    "    \n",
    "    #Inputs\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n",
    "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand = Input(shape=[1], name=\"brand\")\n",
    "    category = Input(shape=[1], name=\"category\")\n",
    "    category_name = Input(shape=[X_train[\"category_name\"].shape[1]], \n",
    "                          name=\"category_name\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    \n",
    "    #Embeddings layers\n",
    "    emb_size = 60\n",
    "    \n",
    "    emb_name = Embedding(MAX_TEXT, emb_size//3)(name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, emb_size)(item_desc)\n",
    "    emb_category_name = Embedding(MAX_TEXT, emb_size//3)(category_name)\n",
    "    emb_brand = Embedding(MAX_BRAND, 10)(brand)\n",
    "    emb_category = Embedding(MAX_CATEGORY, 10)(category)\n",
    "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "    \n",
    "    rnn_layer1 = GRU(16) (emb_item_desc)\n",
    "    rnn_layer2 = GRU(8) (emb_category_name)\n",
    "    rnn_layer3 = GRU(8) (emb_name)\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_brand)\n",
    "        , Flatten() (emb_category)\n",
    "        , Flatten() (emb_item_condition)\n",
    "        , rnn_layer1\n",
    "        , rnn_layer2\n",
    "        , rnn_layer3\n",
    "        , num_vars\n",
    "    ])\n",
    "    \n",
    "    main_l = Dropout(dr)(Dense(512,activation='relu') (main_l))\n",
    "    main_l = Dropout(dr)(Dense(64,activation='relu') (main_l))\n",
    "#     main_l = Dropout(dr)(Dense(32,activation='relu') (main_l))\n",
    "    \n",
    "    #output\n",
    "    output = Dense(1,activation=\"linear\") (main_l)\n",
    "    \n",
    "    #model\n",
    "    model = Model([name, item_desc, brand\n",
    "                   , category, category_name\n",
    "                   , item_condition, num_vars], output)\n",
    "    #optimizer = optimizers.RMSprop()\n",
    "    optimizer = optimizers.Adam()\n",
    "    model.compile(loss=\"mse\", \n",
    "                  optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def eval_model(model):\n",
    "    val_preds = model.predict(X_valid)\n",
    "    val_preds = np.expm1(val_preds)\n",
    "    \n",
    "    y_true = np.array(dvalid.price.values)\n",
    "    y_pred = val_preds[:, 0]\n",
    "    v_rmsle = rmsle(y_true, y_pred)\n",
    "    print(\" RMSLE error on dev test: \"+str(v_rmsle))\n",
    "    return v_rmsle\n",
    "#fin_lr=init_lr * (1/(1+decay))**(steps-1)\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "\n",
    "print('[{}] Finished DEFINEING MODEL...'.format(time.time() - start_time))\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1452175 samples, validate on 14669 samples\n",
      "Epoch 1/6\n",
      "Epoch 2/6\n",
      "Epoch 3/6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d49124c05be3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0;31m#, callbacks=[TensorBoard('./logs/'+log_subdir)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearlystop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     \u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     )\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[{}] Finished FITTING MODEL...'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gc.collect()\n",
    "#FITTING THE MODEL\n",
    "epochs = 3\n",
    "BATCH_SIZE = 512 * 3\n",
    "steps = int(len(X_train['name'])/BATCH_SIZE) * epochs\n",
    "lr_init, lr_fin = 0.013, 0.009\n",
    "# lr_init, lr_fin = 0.023, 0.01\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "log_subdir = '_'.join(['ep', str(epochs),\n",
    "                    'bs', str(BATCH_SIZE),\n",
    "                    'lrI', str(lr_init),\n",
    "                    'lrF', str(lr_fin),\n",
    "                    'dr', str(dr)])\n",
    "\n",
    "model = get_model()\n",
    "K.set_value(model.optimizer.lr, lr_init)\n",
    "K.set_value(model.optimizer.decay, lr_decay)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=1, \\\n",
    "                          verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "history = model.fit(X_train, dtrain.target\n",
    "                    , epochs=epochs\n",
    "                    , batch_size=BATCH_SIZE\n",
    "                    , validation_split=0.01\n",
    "                    #, callbacks=[TensorBoard('./logs/'+log_subdir)]\n",
    "                    , callbacks = [earlystop]\n",
    "                    , verbose=100\n",
    "                    )\n",
    "print('[{}] Finished FITTING MODEL...'.format(time.time() - start_time))\n",
    "#EVLUEATE THE MODEL ON DEV TEST\n",
    "v_rmsle = eval_model(model)\n",
    "print('[{}] Finished predicting valid set...'.format(time.time() - start_time))\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8383.536375522614] Finished predicting test set...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../submissions/subep_4_bs_1536_lrI_0.023_lrF_0.01_dr_0.1_0.443357.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-8d6643937176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"price\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../submissions/sub\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlog_subdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_{:.6}.csv\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_rmsle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[{}] Finished submission...'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1401\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m   1576\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m                                      compression=self.compression)\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Python 3 and no explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;31m# Python 3 and binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../submissions/subep_4_bs_1536_lrI_0.023_lrF_0.01_dr_0.1_0.443357.csv'"
     ]
    }
   ],
   "source": [
    "#CREATE PREDICTIONS\n",
    "preds = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "preds = np.expm1(preds) -1\n",
    "print('[{}] Finished predicting test set...'.format(time.time() - start_time))\n",
    "submission = test[[\"test_id\"]][:test_len]\n",
    "submission[\"price\"] = preds[:test_len]\n",
    "submission.to_csv(\"../submissions/sub_\"+log_subdir+\"_{:.6}.csv\".format(v_rmsle), index=False)\n",
    "print('[{}] Finished submission...'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kme-cpu",
   "language": "python",
   "name": "kme-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
