{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481661, 9)\n",
      "5 folds scaling the test_df\n",
      "(693359, 7)\n",
      "new shape  (3493359, 7)\n",
      "[8.966451406478882] Finished scaling test set...\n",
      "cpu: 18.6\n",
      "consuming 1.14GB RAM\n",
      "Handling missing values...\n",
      "(1481661, 9)\n",
      "(3493359, 7)\n",
      "[9.963890790939331] Finished handling missing data...\n",
      "cpu: 18.0\n",
      "consuming 1.14GB RAM\n",
      "Handling categorical variables...\n",
      "[45.445863246917725] Finished PROCESSING CATEGORICAL DATA...\n",
      "cpu: 17.5\n",
      "consuming 1.23GB RAM\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# mainly forking from notebook\n",
    "# https://www.kaggle.com/johnfarrell/simple-rnn-with-keras-script\n",
    "\n",
    "# ADDED\n",
    "# 5x scaled test set\n",
    "# category name embedding\n",
    "# some small changes like lr, decay, batch_size~\n",
    "\n",
    "# In[ ]:\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import psutil\n",
    "\n",
    "def print_memory_usage():\n",
    "    print('cpu: {}'.format(psutil.cpu_percent()))\n",
    "    print('consuming {:.2f}GB RAM'.format(psutil.Process(os.getpid()).memory_info().rss / 1073741824),\n",
    "          flush=True)\n",
    "    \n",
    "train = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "test = pd.read_csv('../input/test.tsv', sep='\\t')\n",
    "\n",
    "train = train[train.price != 0]\n",
    "train['target'] = np.log1p(train['price'])\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print('5 folds scaling the test_df')\n",
    "print(test.shape)\n",
    "test_len = test.shape[0]\n",
    "def simulate_test(test):\n",
    "    if test.shape[0] < 800000:\n",
    "        indices = np.random.choice(test.index.values, 2800000)\n",
    "        test_ = pd.concat([test, test.iloc[indices]], axis=0)\n",
    "        return test_.copy()\n",
    "    else:\n",
    "        return test\n",
    "test = simulate_test(test)\n",
    "print('new shape ', test.shape)\n",
    "print('[{}] Finished scaling test set...'.format(time.time() - start_time))\n",
    "print_memory_usage()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#HANDLE MISSING VALUES\n",
    "print(\"Handling missing values...\")\n",
    "def handle_missing(dataset):\n",
    "    dataset.category_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.item_description.fillna(value=\"missing\", inplace=True)\n",
    "    return (dataset)\n",
    "\n",
    "train = handle_missing(train)\n",
    "test = handle_missing(test)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "print('[{}] Finished handling missing data...'.format(time.time() - start_time))\n",
    "print_memory_usage()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#PROCESS CATEGORICAL DATA\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "print(\"Handling categorical variables...\")\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(np.hstack([train.category_name, test.category_name]))\n",
    "train['category'] = le.transform(train.category_name)\n",
    "test['category'] = le.transform(test.category_name)\n",
    "\n",
    "le.fit(np.hstack([train.brand_name, test.brand_name]))\n",
    "train['brand'] = le.transform(train.brand_name)\n",
    "test['brand'] = le.transform(test.brand_name)\n",
    "del le, train['brand_name'], test['brand_name']\n",
    "\n",
    "print('[{}] Finished PROCESSING CATEGORICAL DATA...'.format(time.time() - start_time))\n",
    "# train.head(3)\n",
    "print_memory_usage()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train_id                                 name  item_condition_id  \\\n",
      "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
      "\n",
      "       category_name  price  shipping item_description    target  category  \\\n",
      "0  Men/Tops/T-shirts   10.0         1  description yet  2.397895       829   \n",
      "\n",
      "   brand  \n",
      "0   5263  \n",
      "[960.4209070205688] Finished text_to_wordlist...\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "def text_to_wordlist(text, remove_stopwords=True, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "train['item_description'] = train['item_description'].map(lambda x: text_to_wordlist(x))\n",
    "test['item_description'] = test['item_description'].map(lambda x: text_to_wordlist(x))\n",
    "print(train.head(1))\n",
    "print('[{}] Finished text_to_wordlist...'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to seq process...\n",
      "   Fitting tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size= 216567\n",
      "   Transforming text to seq...\n",
      "[1176.8381216526031] Finished PROCESSING TEXT DATA...\n",
      "cpu: 17.5\n",
      "consuming 4.67GB RAM\n"
     ]
    }
   ],
   "source": [
    "#PROCESS TEXT: RAW\n",
    "print(\"Text to seq process...\")\n",
    "print(\"   Fitting tokenizer...\")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "raw_text = np.hstack([train.category_name.str.lower(), \n",
    "                      train.item_description.str.lower(), \n",
    "                      train.name.str.lower()])\n",
    "\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "vocab_size = len(tok_raw.word_index) + 1\n",
    "print('vocab_size=', vocab_size)\n",
    "\n",
    "print(\"   Transforming text to seq...\")\n",
    "train[\"seq_category_name\"] = tok_raw.texts_to_sequences(train.category_name.str.lower())\n",
    "test[\"seq_category_name\"] = tok_raw.texts_to_sequences(test.category_name.str.lower())\n",
    "train[\"seq_item_description\"] = tok_raw.texts_to_sequences(train.item_description.str.lower())\n",
    "test[\"seq_item_description\"] = tok_raw.texts_to_sequences(test.item_description.str.lower())\n",
    "train[\"seq_name\"] = tok_raw.texts_to_sequences(train.name.str.lower())\n",
    "test[\"seq_name\"] = tok_raw.texts_to_sequences(test.name.str.lower())\n",
    "# train.head(3)\n",
    "\n",
    "print('[{}] Finished PROCESSING TEXT DATA...'.format(time.time() - start_time))\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "[1182.579567193985] Finished Embedding matrix...\n",
      "cpu: 17.7\n",
      "consuming 4.87GB RAM\n",
      "CPU times: user 5.67 s, sys: 124 ms, total: 5.79 s\n",
      "Wall time: 5.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#  with open(\"glove.6B/glove.6B.50d.txt\", \"rb\") as lines:\n",
    "#         w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "#                for line in lines}\n",
    "embeddings_index = dict()\n",
    "f = open('../input/glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "for word, i in tok_raw.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('[{}] Finished Embedding matrix...'.format(time.time() - start_time))\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216567, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from collections import defaultdict\n",
    "# class TfidfEmbeddingVectorizer(object):\n",
    "#     def __init__(self, word2vec):\n",
    "#         self.word2vec = word2vec\n",
    "#         self.word2weight = None\n",
    "# #         self.dim = len(word2vec.itervalues().next())\n",
    "#         self.dim = len(word2vec.values())\n",
    "# #         self.dim = 50\n",
    "\n",
    "#     def fit(self, X, y='optional'):\n",
    "#         tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "#         tfidf.fit(X)\n",
    "#         # if a word was never seen - it must be at least as infrequent\n",
    "#         # as any of the known words - so the default idf is the max of \n",
    "#         # known idf's\n",
    "#         max_idf = max(tfidf.idf_)\n",
    "#         self.word2weight = defaultdict(\n",
    "#             lambda: max_idf,\n",
    "#             [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return np.array([\n",
    "#                 np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "#                          for w in words if w in self.word2vec] or\n",
    "#                         [np.zeros(self.dim)], axis=0)\n",
    "#                 for words in X\n",
    "#             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfEmbeddingVectorizer(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf.fit(np.hstack([train.name, test.name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['tfidf_name'] = tfidf.transform(train.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['tfidf_name'] = tfidf.transform(test.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "from keras.layers import TimeDistributed\n",
    "# class AttLayer(Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         self.init = initializers.get('normal')\n",
    "#         #self.input_spec = [InputSpec(ndim=3)]\n",
    "#         super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         assert len(input_shape)==3\n",
    "#         #self.W = self.init((input_shape[-1],1))\n",
    "#         self.W = self.init((input_shape[-1],))\n",
    "#         #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "#         self.trainable_weights = [self.W]\n",
    "#         super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "#     def call(self, x, mask=None):\n",
    "#         eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "#         ai = K.exp(eij)\n",
    "#         weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "#         weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "#         return weighted_input.sum(axis=1)\n",
    "\n",
    "#     def get_output_shape_for(self, input_shape):\n",
    "#         return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1466844, 13)\n",
      "(14817, 13)\n",
      "216565 216557\n",
      "428 428\n",
      "169504 169504\n",
      "MAX_NAME= 216566\n",
      "MAX_CAT_NAME= 429\n",
      "MAX_ITEM_DESC= 216365\n",
      "MAX_TEXT= 216567\n",
      "MAX_CATEGORY= 1311\n",
      "MAX_BRAND= 5288\n",
      "MAX_CONDITION= 6\n",
      "[1190.0684022903442] Finished EMBEDDINGS MAX VALUE...\n",
      "cpu: 18.2\n",
      "consuming 5.10GB RAM\n",
      "[1256.1259169578552] Finished DATA PREPARARTION...\n",
      "cpu: 17.6\n",
      "consuming 7.16GB RAM\n",
      "[1256.1284539699554] Finished DEFINEING MODEL...\n",
      "cpu: 33.3\n",
      "consuming 7.16GB RAM\n",
      "Train on 1452175 samples, validate on 14669 samples\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Epoch 3/3\n",
      "[2102.917825937271] Finished FITTING MODEL...\n",
      " RMSLE error on dev test: 0.43787504155300155\n",
      "[2106.420909881592] Finished predicting valid set...\n",
      "cpu: 80.1\n",
      "consuming 7.62GB RAM\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# In[ ]:\n",
    "\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "#EXTRACT DEVELOPTMENT TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "dtrain, dvalid = train_test_split(train, random_state=233, train_size=0.99)\n",
    "print(dtrain.shape)\n",
    "print(dvalid.shape)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#EMBEDDINGS MAX VALUE\n",
    "#Base on the histograms, we select the next lengths\n",
    "MAX_NAME_SEQ = 20 #17\n",
    "MAX_ITEM_DESC_SEQ = 60 #269\n",
    "MAX_CATEGORY_NAME_SEQ = 20 #8\n",
    "print(np.max(train.seq_name.max()), np.max(test.seq_name.max()))\n",
    "print(np.max(train.seq_category_name.max()), np.max(test.seq_category_name.max()))\n",
    "print(np.max(train.seq_item_description.max()), np.max(train.seq_item_description.max()))\n",
    "\n",
    "MAX_NAME = np.max([np.max(train.seq_name.max()), np.max(test.seq_name.max())])+1\n",
    "MAX_CAT_NAME = np.max([np.max(train.seq_category_name.max()), np.max(test.seq_category_name.max())])+1\n",
    "MAX_ITEM_DESC = np.max([np.max(train.seq_item_description.max()), np.max(test.seq_item_description.max())])+1\n",
    "\n",
    "print('MAX_NAME=',MAX_NAME)\n",
    "print('MAX_CAT_NAME=',MAX_CAT_NAME)\n",
    "print('MAX_ITEM_DESC=',MAX_ITEM_DESC)\n",
    "\n",
    "MAX_TEXT = np.max([np.max(train.seq_name.max())\n",
    "                   , np.max(test.seq_name.max())\n",
    "                   , np.max(train.seq_category_name.max())\n",
    "                   , np.max(test.seq_category_name.max())\n",
    "                   , np.max(train.seq_item_description.max())\n",
    "                   , np.max(test.seq_item_description.max())])+2\n",
    "MAX_CATEGORY = np.max([train.category.max(), test.category.max()])+1\n",
    "MAX_BRAND = np.max([train.brand.max(), test.brand.max()])+1\n",
    "MAX_CONDITION = np.max([train.item_condition_id.max(), \n",
    "                        test.item_condition_id.max()])+1\n",
    "MAX_TEXT=max(MAX_TEXT, vocab_size)\n",
    "print('MAX_TEXT=',MAX_TEXT)\n",
    "print('MAX_CATEGORY=',MAX_CATEGORY)\n",
    "print('MAX_BRAND=',MAX_BRAND)\n",
    "print('MAX_CONDITION=',MAX_CONDITION)\n",
    "\n",
    "print('[{}] Finished EMBEDDINGS MAX VALUE...'.format(time.time() - start_time))\n",
    "print_memory_usage()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#KERAS DATA DEFINITION\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_keras_data(dataset):\n",
    "    X = {\n",
    "        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n",
    "        ,'item_desc': pad_sequences(dataset.seq_item_description\n",
    "                                    , maxlen=MAX_ITEM_DESC_SEQ)\n",
    "        ,'brand': np.array(dataset.brand)\n",
    "        ,'category': np.array(dataset.category)\n",
    "        ,'category_name': pad_sequences(dataset.seq_category_name\n",
    "                                        , maxlen=MAX_CATEGORY_NAME_SEQ)\n",
    "        ,'item_condition': np.array(dataset.item_condition_id)\n",
    "        ,'num_vars': np.array(dataset[[\"shipping\"]])\n",
    "    }\n",
    "    return X\n",
    "\n",
    "X_train = get_keras_data(dtrain)\n",
    "X_valid = get_keras_data(dvalid)\n",
    "X_test = get_keras_data(test)\n",
    "\n",
    "print('[{}] Finished DATA PREPARARTION...'.format(time.time() - start_time))\n",
    "print_memory_usage()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#KERAS MODEL DEFINITION\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, \\\n",
    "    Activation, concatenate, GRU, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    import math\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 \\\n",
    "              for i, pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n",
    "\n",
    "dr = 0.25\n",
    "\n",
    "def get_model():\n",
    "    #params\n",
    "    dr_r = dr\n",
    "    \n",
    "    #Inputs\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n",
    "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand = Input(shape=[1], name=\"brand\")\n",
    "    category = Input(shape=[1], name=\"category\")\n",
    "    category_name = Input(shape=[X_train[\"category_name\"].shape[1]], \n",
    "                          name=\"category_name\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    \n",
    "    #Embeddings layers\n",
    "    emb_size = 60\n",
    "    \n",
    "    emb_name = Embedding(MAX_TEXT, 50, weights=[embedding_matrix], trainable=True)(name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, 50, weights=[embedding_matrix], trainable=True)(item_desc)\n",
    "    emb_category_name = Embedding(MAX_TEXT, emb_size//3)(category_name)\n",
    "#     emb_category_name = Embedding(MAX_TEXT, 100, weights=[embedding_matrix], trainable=True)(category_name)\n",
    "    emb_brand = Embedding(MAX_BRAND, 10)(brand)\n",
    "    emb_category = Embedding(MAX_CATEGORY, 10)(category)\n",
    "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "    \n",
    "#     emb_name = Embedding(MAX_TEXT, emb_size//3)(name)\n",
    "#     emb_item_desc = Embedding(MAX_TEXT, emb_size)(item_desc)\n",
    "#     emb_category_name = Embedding(MAX_TEXT, emb_size//3)(category_name)\n",
    "#     emb_brand = Embedding(MAX_BRAND, 10)(brand)\n",
    "#     emb_category = Embedding(MAX_CATEGORY, 10)(category)\n",
    "#     emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "    \n",
    "    rnn_layer1 = GRU(20, return_sequences=True, dropout=0.2, recurrent_dropout=0.2) (emb_item_desc)\n",
    "    att1 = Attention(MAX_ITEM_DESC_SEQ)(rnn_layer1)\n",
    "    \n",
    "    rnn_layer2 = GRU(6, return_sequences=True, dropout=0.2, recurrent_dropout=0.2) (emb_category_name)\n",
    "    att2 = Attention(MAX_CATEGORY_NAME_SEQ)(rnn_layer2)\n",
    "    \n",
    "#     rnn_layer3 = GRU(20, return_sequences=True) (emb_name)\n",
    "    rnn_layer3 = GRU(15, return_sequences=True, dropout=0.2, recurrent_dropout=0.2) (emb_name)\n",
    "    att3 = Attention(MAX_NAME_SEQ)(rnn_layer3)\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_brand)\n",
    "        , Flatten() (emb_category)\n",
    "        , Flatten() (emb_item_condition)    \n",
    "        , att1\n",
    "        , att2\n",
    "        , att3\n",
    "        , num_vars\n",
    "    ])\n",
    "    \n",
    "    main_l = Dropout(0.1)(Dense(512,activation='relu') (main_l))\n",
    "    main_l = Dropout(0.1)(Dense(64,activation='relu') (main_l))\n",
    "    \n",
    "    # main_l = Dropout(0.1)(Dense(512,activation='relu') (main_l))\n",
    "    # main_l = Dropout(0.25)(Dense(256,activation='relu') (main_l))\n",
    "    # main_l = Dropout(0.5)(Dense(32,activation='relu') (main_l))\n",
    "    #output\n",
    "    output = Dense(1,activation=\"linear\") (main_l)\n",
    "    \n",
    "    #model\n",
    "    model = Model([name, item_desc, brand\n",
    "                   , category, category_name\n",
    "                   , item_condition, num_vars], output)\n",
    "    #optimizer = optimizers.RMSprop()\n",
    "    optimizer = optimizers.Adam()\n",
    "    model.compile(loss=\"mse\", \n",
    "                  optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def eval_model(model):\n",
    "    val_preds = model.predict(X_valid)\n",
    "    val_preds = np.expm1(val_preds)\n",
    "    \n",
    "    y_true = np.array(dvalid.price.values)\n",
    "    y_pred = val_preds[:, 0]\n",
    "    v_rmsle = rmsle(y_true, y_pred)\n",
    "    print(\" RMSLE error on dev test: \"+str(v_rmsle))\n",
    "    return v_rmsle\n",
    "#fin_lr=init_lr * (1/(1+decay))**(steps-1)\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "\n",
    "print('[{}] Finished DEFINEING MODEL...'.format(time.time() - start_time))\n",
    "print_memory_usage()\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "gc.collect()\n",
    "#FITTING THE MODEL\n",
    "epochs = 3\n",
    "BATCH_SIZE = 512 * 5\n",
    "steps = int(len(X_train['name'])//BATCH_SIZE) * epochs\n",
    "lr_init, lr_fin = 0.013, 0.009\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "log_subdir = '_'.join(['ep', str(epochs),\n",
    "                    'bs', str(BATCH_SIZE),\n",
    "                    'lrI', str(lr_init),\n",
    "                    'lrF', str(lr_fin),\n",
    "                    'dr', str(dr)])\n",
    "\n",
    "model = get_model()\n",
    "K.set_value(model.optimizer.lr, lr_init)\n",
    "K.set_value(model.optimizer.decay, lr_decay)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=1, \\\n",
    "                          verbose=1, mode='auto')\n",
    "                          \n",
    "history = model.fit(X_train, dtrain.target\n",
    "                    , epochs=epochs\n",
    "                    , batch_size=BATCH_SIZE\n",
    "                    , validation_split=0.01\n",
    "                    , callbacks=[earlystop]\n",
    "                    , verbose=10\n",
    "                    )\n",
    "print('[{}] Finished FITTING MODEL...'.format(time.time() - start_time))\n",
    "#EVLUEATE THE MODEL ON DEV TEST\n",
    "v_rmsle = eval_model(model)\n",
    "print('[{}] Finished predicting valid set...'.format(time.time() - start_time))\n",
    "print_memory_usage()\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#CREATE PREDICTIONS\n",
    "# preds = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "# preds = np.expm1(preds)\n",
    "# print('[{}] Finished predicting test set...'.format(time.time() - start_time))\n",
    "# submission = test[[\"test_id\"]][:test_len]\n",
    "# submission[\"price\"] = preds[:test_len]\n",
    "# submission.to_csv(\"../cache/myNN\"+log_subdir+\"_{:.6}.csv\".format(v_rmsle), index=False)\n",
    "# print('[{}] Finished submission...'.format(time.time() - start_time))\n",
    "# print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "798.51865845156215"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.category.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883.4210065595303"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.brand.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 attentions:\n",
    "# RMSLE error on dev test: 0.43621085979926566, 0.43431049774899066,\n",
    "#  0.4338830005110395 with dropout 0.1 (24, 8, 15)\n",
    "# 0.43218515551974823 (24, 8, 15, 0.2)\n",
    "# 0.4287631303060977 (20, 6, 15, 0.2), 0.4394812043885648 (15, 6, 10, 0.2)\n",
    "# 0.43787504155300155 (20, 6, 16, 0.2)\n",
    "\n",
    "# attention on name: 50, 10, 10, 5\n",
    "# RMSLE error on dev test: 0.4347287568069609\n",
    "    \n",
    "# 50.txt, 10, 10, 5\n",
    "# 0.4245, 0.426444\n",
    "\n",
    "# glove.6B.50.txt instead of 100,\n",
    "# RMSLE error on dev test: 0.4275900360816371\n",
    "\n",
    "# emb_brand = Embedding(MAX_BRAND, 20)(brand)\n",
    "# 0.42693247595039546\n",
    "\n",
    "# when 20 embedding length, 6 for category for brand\n",
    "# 0.42482011805535536\n",
    "\n",
    "# when 2 embedded weights can be trained,\n",
    "# RMSLE error on dev test: 4242845146375708\n",
    "\n",
    "# when 3 embedded weights can be trained,\n",
    "# RMSLE error on dev test: 0.4347576557957351\n",
    "\n",
    "# org: \n",
    "# [2123.6134021282196] Finished FITTING MODEL...\n",
    "# RMSLE error on dev test: 0.43528846581148545\n",
    "# [2355.4364523887634] Finished submission..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kme-cpu",
   "language": "python",
   "name": "kme-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
