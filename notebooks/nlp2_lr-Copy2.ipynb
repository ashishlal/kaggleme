{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from textblob import TextBlob\n",
    "import lightgbm as lgb\n",
    "import os, psutil\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "NAME_MIN_DF = 10\n",
    "MAX_FEATURES_ITEM_DESCRIPTION = 2 ** 14\n",
    "NUM_PARTITIONS = 12 #number of partitions to split dataframe\n",
    "NUM_CORES = 8 #number of cores on your machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n",
    "\n",
    "def split_cat(text):\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['category_name'] = dataset['category_name'].astype('category')\n",
    "    dataset['brand_name'] = dataset['brand_name'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n",
    "\n",
    "def print_memory_usage():\n",
    "    print('cpu: {}'.format(psutil.cpu_percent()))\n",
    "    print('consuming {:.2f}GB RAM'.format(\n",
    "    \t   psutil.Process(os.getpid()).memory_info().rss / 1073741824),\n",
    "    \t  flush=True)\n",
    "\n",
    "def _sigmoid(score):\n",
    "    p = 1. / (1. + np.exp(-score))\n",
    "    return p\n",
    "\n",
    "\n",
    "def _logit(p):\n",
    "    return np.log(p/(1.-p))\n",
    "\n",
    "\n",
    "def _softmax(score):\n",
    "    score = np.asarray(score, dtype=float)\n",
    "    score = np.exp(score - np.max(score))\n",
    "    score /= np.sum(score, axis=1)[:,np.newaxis]\n",
    "    return score\n",
    "\n",
    "\n",
    "def _cast_proba_predict(proba):\n",
    "    N = proba.shape[1]\n",
    "    w = np.arange(1,N+1)\n",
    "    pred = proba * w[np.newaxis,:]\n",
    "    pred = np.sum(pred, axis=1)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def _one_hot_label(label, n_classes):\n",
    "    num = label.shape[0]\n",
    "    tmp = np.zeros((num, n_classes), dtype=int)\n",
    "    tmp[np.arange(num),label.astype(int)] = 1\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def _majority_voting(x, weight=None):\n",
    "    ## apply weight\n",
    "    if weight is not None:\n",
    "    \tassert len(weight) == len(x)\n",
    "    \tx = np.repeat(x, weight)\n",
    "    c = Counter(x)\n",
    "    value, count = c.most_common()[0]\n",
    "    return value\n",
    "\n",
    "\n",
    "def _voter(x, weight=None):\n",
    "    idx = np.isfinite(x)\n",
    "    if sum(idx) == 0:\n",
    "    \tvalue = config.MISSING_VALUE_NUMERIC\n",
    "    else:\n",
    "    \tif weight is not None:\n",
    "    \t\tvalue = _majority_voting(x[idx], weight[idx])\n",
    "    \telse:\n",
    "    \t\tvalue = _majority_voting(x[idx])\n",
    "    return value\n",
    "\n",
    "\n",
    "def _array_majority_voting(X, weight=None):\n",
    "    y = np.apply_along_axis(_voter, axis=1, arr=X, weight=weight)\n",
    "    return y\n",
    "\n",
    "\n",
    "def _mean(x):\n",
    "    idx = np.isfinite(x)\n",
    "    if sum(idx) == 0:\n",
    "    \tvalue = float(config.MISSING_VALUE_NUMERIC) # cast it to float to accommodate the np.mean\n",
    "    else:\n",
    "    \tvalue = np.mean(x[idx]) # this is float!\n",
    "    return value\n",
    "\n",
    "\n",
    "def _array_mean(X):\n",
    "    y = np.apply_along_axis(_mean, axis=1, arr=X)\n",
    "    return y\n",
    "\n",
    "\n",
    "def _corr(x, y_train):\n",
    "    if _dim(x) == 1:\n",
    "    \tcorr = pearsonr(x.flatten(), y_train)[0]\n",
    "    \tif str(corr) == \"nan\":\n",
    "    \t\tcorr = 0.\n",
    "    else:\n",
    "    \tcorr = 1.\n",
    "    return corr\n",
    "\n",
    "\n",
    "def _dim(x):\n",
    "    d = 1 if len(x.shape) == 1 else x.shape[1]\n",
    "    return d\n",
    "\n",
    "\n",
    "def _entropy(proba):\n",
    "    entropy = -np.sum(proba*np.log(proba))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def _try_divide(x, y, val=0.0):\n",
    "    \"\"\"try to divide two numbers\"\"\"\n",
    "    if y != 0.0:\n",
    "    \tval = float(x) / y\n",
    "    return val\n",
    "\n",
    "def entropy(obs, token_pattern=' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    counter = Counter(obs_tokens)\n",
    "    count = np.asarray(list(counter.values()))\n",
    "    proba = count/np.sum(count)\n",
    "    del obs_tokens\n",
    "    return _entropy(proba)\n",
    "        \n",
    "def digit_count(obs):\n",
    "    return len(re.findall(r\"\\d\", obs))\n",
    "\n",
    "def digit_ratio(obs, token_pattern = ' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    return _try_divide(len(re.findall(r\"\\d\", obs)), len(obs_tokens))\n",
    "\n",
    "def emoji_count(obs):\n",
    "    return len(re.findall(r'[^\\w\\s,]', obs))\n",
    "\n",
    "def emoji_ratio(obs, token_pattern = ' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    return _try_divide(len(re.findall(r'[^\\w\\s,]', obs)), len(obs_tokens))\n",
    "\n",
    "def _unigrams(words):\n",
    "    \"\"\"\n",
    "    \tInput: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "    \tOutput: a list of unigram\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "\n",
    "\n",
    "def _bigrams(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "       Output: a list of bigram, e.g., [\"I_am\", \"am_Denny\"]\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "    \tlst = []\n",
    "    \tfor i in range(L-1):\n",
    "    \t\tfor k in range(1,skip+2):\n",
    "    \t\t\tif i+k < L:\n",
    "    \t\t\t\tlst.append( join_string.join([words[i], words[i+k]]) )\n",
    "    else:\n",
    "    \t# set it as unigram\n",
    "    \tlst = _unigrams(words)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def _trigrams(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "       Output: a list of trigram, e.g., [\"I_am_Denny\"]\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "    \tlst = []\n",
    "    \tfor i in range(L-2):\n",
    "    \t\tfor k1 in range(1,skip+2):\n",
    "    \t\t\tfor k2 in range(1,skip+2):\n",
    "    \t\t\t\tif i+k1 < L and i+k1+k2 < L:\n",
    "    \t\t\t\t\tlst.append( join_string.join([words[i], words[i+k1], words[i+k1+k2]]) )\n",
    "    else:\n",
    "    \t# set it as bigram\n",
    "    \tlst = _bigrams(words, join_string, skip)\n",
    "    return lst\n",
    "\n",
    "def UniqueCount_Ngram(obs, count, token_pattern=' '):\n",
    "    obs_tokens = obs.lower().split(token_pattern)\n",
    "    obs_ngrams = _ngrams(obs_tokens, count)\n",
    "    l = len(set(obs_ngrams))\n",
    "    del obs_tokens\n",
    "    del obs_ngrams\n",
    "    return l\n",
    "\n",
    "def UniqueRatio_Ngram(obs, count, token_pattern=' '):\n",
    "    obs_tokens = obs.lower().split(token_pattern)\n",
    "    obs_ngrams = _ngrams(obs_tokens, count)\n",
    "    r = _try_divide(len(set(obs_ngrams)), len(obs_ngrams))\n",
    "    del obs_tokens\n",
    "    del obs_ngrams\n",
    "    return r\n",
    "\n",
    "def _ngrams(words, ngram, join_string=\" \"):\n",
    "    \"\"\"wrapper for ngram\"\"\"\n",
    "    if ngram == 1:\n",
    "    \treturn _unigrams(words)\n",
    "    elif ngram == 2:\n",
    "    \treturn _bigrams(words, join_string)\n",
    "    elif ngram == 3:\n",
    "    \treturn _trigrams(words, join_string)\n",
    "    elif ngram == 4:\n",
    "    \treturn _fourgrams(words, join_string)\n",
    "    elif ngram == 12:\n",
    "    \tunigram = _unigrams(words)\n",
    "    \tbigram = [x for x in _bigrams(words, join_string) if len(x.split(join_string)) == 2]\n",
    "    \treturn unigram + bigram\n",
    "    elif ngram == 123:\n",
    "    \tunigram = _unigrams(words)\n",
    "    \tbigram = [x for x in _bigrams(words, join_string) if len(x.split(join_string)) == 2]\n",
    "    \ttrigram = [x for x in _trigrams(words, join_string) if len(x.split(join_string)) == 3]\n",
    "    \treturn unigram + bigram + trigram\n",
    "    \t\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, NUM_PARTITIONS)\n",
    "    pool = Pool(NUM_CORES)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def get_sentiment_score(df):\n",
    "    df['sentiment_score'] = df['item_description'].map(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.126489877700806] Finished to load data\n",
      "Train shape:  (1482535, 8)\n",
      "Test shape:  (693359, 7)\n",
      "Test shape  (3466795, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def main():\n",
    "start_time = time.time()\n",
    "\n",
    "train = pd.read_table('../input/train.tsv', engine='c')\n",
    "test = pd.read_table('../input/test.tsv', engine='c')\n",
    "print('[{}] Finished to load data'.format(time.time() - start_time))\n",
    "print('Train shape: ', train.shape)\n",
    "print('Test shape: ', test.shape)\n",
    "\n",
    "nrow_test = test.shape[0]\n",
    "\n",
    "test_id = test['test_id'].values\n",
    "submission: pd.DataFrame = test[['test_id']]\n",
    "\n",
    "if nrow_test < 700000:\n",
    "    test = pd.concat([test,test,test,test,test])\n",
    "    print('Test shape ', test.shape)\n",
    "\n",
    "\n",
    "nrow_train = train.shape[0]\n",
    "y = np.log1p(train[\"price\"])\n",
    "del train['price']\n",
    "merge: pd.DataFrame = pd.concat([train, test])\n",
    "\n",
    "train_cols = set(train.columns)\n",
    "del train\n",
    "del test\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.382498264312744] Handle missing completed.\n"
     ]
    }
   ],
   "source": [
    "handle_missing_inplace(merge)\n",
    "print('[{}] Handle missing completed.'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188.5318615436554] Finished basic creation for name, bn, item_desc\n"
     ]
    }
   ],
   "source": [
    "def get_doclen_name(df):\n",
    "    df['name_doclen'] = df['name'].map(lambda x: len(str(x).lower().split(' ')))\n",
    "    return df\n",
    "\n",
    "def get_doclen_itemdesc(df):\n",
    "    df['item_description_doclen'] = df['item_description'].map(lambda x: len(str(x).lower().split(' ')))\n",
    "    return df\n",
    "\n",
    "def get_doclen_brand_name(df):\n",
    "    df['brand_name_doclen'] = df['brand_name'].map(lambda x: len(str(x).lower().split(' ')))\n",
    "    return df\n",
    "\n",
    "def get_entropy_name(df):\n",
    "    df['name_entropy'] = df['name'].map(lambda x: entropy(str(x).lower(), ' '))\n",
    "    return df\n",
    "\n",
    "def get_entropy_itemdesc(df):\n",
    "    df['item_description_entropy'] = \\\n",
    "    \tdf['item_description'].map(lambda x: entropy(str(x).lower(), ' '))\n",
    "    return df\n",
    "\n",
    "def get_entropy_brand_name(df):\n",
    "    df['brand_name_entropy'] = \\\n",
    "    \tdf['brand_name'].map(lambda x: entropy(str(x).lower(), ' '))\n",
    "    return df\n",
    "\n",
    "def get_digit_count_name(df):\n",
    "    df['name_dc'] = df['name'].map(lambda x: digit_count(str(x).lower()))\n",
    "    return df\n",
    "\n",
    "def get_digit_count_itemdesc(df):\n",
    "    df['item_description_dc'] = \\\n",
    "    \tdf['item_description'].map(lambda x: digit_count(str(x).lower()))\n",
    "    return df\n",
    "\n",
    "def get_digit_count_brand_name(df):\n",
    "    df['brand_name_dc'] = \\\n",
    "    \tdf['brand_name'].map(lambda x: digit_count(str(x).lower()))\n",
    "    return df\n",
    "\n",
    "def get_digit_ratio_name(df):\n",
    "    df['name_dr'] = df['name'].map(lambda x: digit_ratio(str(x).lower()))\n",
    "    return df\n",
    "\n",
    "def get_digit_ratio_itemdesc(df):\n",
    "    df['item_description_dr'] = \\\n",
    "    \tdf['item_description'].map(lambda x: digit_ratio(str(x).lower()))\n",
    "    return df\n",
    "\n",
    "def get_digit_ratio_brand_name(df):\n",
    "    df['brand_name_dr'] = \\\n",
    "    \tdf['brand_name'].map(lambda x: digit_ratio(str(x).lower()))\n",
    "    return df\n",
    "\n",
    "def get_emoji_count_name(df):\n",
    "    df['name_ec'] = df['name'].map(lambda x: emoji_count(str(x).lower()))\n",
    "    return df\n",
    "\n",
    "def get_emoji_count_itemdesc(df):\n",
    "    df['item_description_ec'] = \\\n",
    "    \tdf['item_description'].map(lambda x: emoji_count(str(x).lower()))\n",
    "    return df\n",
    "\n",
    "def get_emoji_count_brand_name(df):\n",
    "    df['brand_name_ec'] = \\\n",
    "    \tdf['brand_name'].map(lambda x: emoji_count(str(x).lower()))\n",
    "    return df\n",
    "        \n",
    "def get_emoji_ratio_name(df):\n",
    "    df['name_er'] = df['name'].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "    return df\n",
    "\n",
    "def get_emoji_ratio_itemdesc(df):\n",
    "    df['item_description_er'] = \\\n",
    "    \tdf['item_description'].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "    return df\n",
    "\n",
    "def get_emoji_ratio_brand_name(df):\n",
    "    df['brand_name_er'] = \\\n",
    "    \tdf['brand_name'].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "    return df\n",
    "\n",
    "cols1 = set(merge.columns)\n",
    "cols = []\n",
    "obs_fields = ['name', 'brand_name', 'item_description']\n",
    "merge = parallelize_dataframe(merge, get_doclen_name)\n",
    "merge = parallelize_dataframe(merge, get_doclen_itemdesc)\n",
    "merge = parallelize_dataframe(merge, get_doclen_brand_name)\n",
    "\n",
    "merge = parallelize_dataframe(merge, get_entropy_name)\n",
    "merge = parallelize_dataframe(merge, get_entropy_itemdesc)\n",
    "merge = parallelize_dataframe(merge, get_entropy_brand_name)\n",
    "\n",
    "merge = parallelize_dataframe(merge, get_digit_count_name)\n",
    "merge = parallelize_dataframe(merge, get_digit_count_itemdesc)\n",
    "merge = parallelize_dataframe(merge, get_digit_count_brand_name)\n",
    "\n",
    "merge = parallelize_dataframe(merge, get_digit_ratio_name)\n",
    "merge = parallelize_dataframe(merge, get_digit_ratio_itemdesc)\n",
    "merge = parallelize_dataframe(merge, get_digit_ratio_brand_name)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_emoji_count_name)\n",
    "# merge = parallelize_dataframe(merge, get_emoji_count_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_emoji_count_brand_name)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_emoji_ratio_name)\n",
    "# merge = parallelize_dataframe(merge, get_emoji_ratio_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_emoji_ratio_brand_name)\n",
    "\n",
    "print('[{}] Finished basic creation for name, bn, item_desc'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in obs_fields:\n",
    "    counter = Counter(merge[f].values)\n",
    "    merge[f+'_docfreq'] = merge[f].map(lambda x: counter[x])\n",
    "    \n",
    "    cols.append(f+'_doclen')\n",
    "    cols.append(f+'_docfreq')\n",
    "    cols.append(f+'_docEntropy')\n",
    "    cols.append(f+'_digitCount')\n",
    "    cols.append(f+'_digitRatio')\n",
    "    # cols.append(f+'_emojiCount')\n",
    "    # cols.append(f+'_emojiRatio')\n",
    "\n",
    "f = 'category_name'\n",
    "def get_category_name_doclen(df):\n",
    "    df[f+'_doclen'] = df[f].map(lambda x: len(str(x).lower().split('/')))\n",
    "    return df\n",
    "\n",
    "merge = parallelize_dataframe(merge, get_category_name_doclen)\n",
    "\n",
    "counter = Counter(merge[f].values)\n",
    "merge[f+'_docfreq'] = merge[f].map(lambda x: counter[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[265.31650257110596] Finished basic creation for category_name\n"
     ]
    }
   ],
   "source": [
    "token_pattern = '/'\n",
    "\n",
    "def get_category_name_entropy(df):\n",
    "\tdf[f+'_docEntropy'] = df[f].map(lambda x: entropy(str(x).lower(),token_pattern))\n",
    "\treturn df\n",
    "merge = parallelize_dataframe(merge, get_category_name_entropy)\n",
    "\n",
    "def get_category_name_dc(df):\n",
    "\tdf[f+'_dc'] = df[f].map(lambda x: digit_count(str(x).lower()))\n",
    "\treturn df\n",
    "merge = parallelize_dataframe(merge, get_category_name_dc)\n",
    "\n",
    "def get_category_name_dr(df):\n",
    "\tdf[f+'_dr'] = df[f].map(lambda x: digit_ratio(str(x).lower(), token_pattern))\n",
    "\treturn df\n",
    "merge = parallelize_dataframe(merge, get_category_name_dr)\n",
    "\n",
    "def get_category_name_ec(df):\n",
    "\tdf[f+'_emojiCount'] = df[f].map(lambda x: emoji_count(str(x).lower()))\n",
    "\treturn df\n",
    "# merge = parallelize_dataframe(merge, get_category_name_ec)\n",
    "\n",
    "def get_category_name_er(df):\n",
    "\tdf[f+'_emojiRatio'] = df[f].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "\treturn df\n",
    "# merge = parallelize_dataframe(merge, get_category_name_er)\n",
    "\n",
    "cols.append(f+'_doclen')\n",
    "cols.append(f+'_docfreq')\n",
    "cols.append(f+'_docEntropy')\n",
    "cols.append(f+'_digitCount')\n",
    "cols.append(f+'_digitRatio')\n",
    "# cols.append(f+'_emojiCount')\n",
    "# cols.append(f+'_emojiRatio')\n",
    "\n",
    "print('[{}] Finished basic creation for category_name'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "24\n",
      "[362.87214159965515] Finished X_basic1\n",
      "basic:  606204864\n",
      "[367.25429248809814] Finished X_basic2\n",
      "cpu: 31.7\n",
      "consuming 7.24GB RAM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "obs_fields = [\"name\", \"item_description\"]\n",
    "\n",
    "# def get_onegram_uc_name(df):\n",
    "# \tdf['name_1_uc'] = df['name'].map(lambda x: UniqueCount_Ngram(str(x), 1))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_onegram_uc_name)\n",
    "\n",
    "# def get_onegram_uc_item_desc(df):\n",
    "# \tdf['item_desc_1_uc'] = \\\n",
    "# \t\tdf['item_description'].map(lambda x: UniqueCount_Ngram(str(x), 1))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_onegram_uc_item_desc)\n",
    "\n",
    "# def get_onegram_ur_name(df):\n",
    "# \tdf['name_1_ur'] = df['name'].map(lambda x: UniqueRatio_Ngram(str(x), 1))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_onegram_ur_name)\n",
    "\n",
    "# def get_onegram_ur_item_desc(df):\n",
    "# \tdf['item_desc_1_ur'] = \\\n",
    "# \t\tdf['item_description'].map(lambda x: UniqueRatio_Ngram(str(x), 1))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_onegram_ur_item_desc)\n",
    "\n",
    "def get_bigram_uc_name(df):\n",
    "\tdf['name_2_uc'] = df['name'].map(lambda x: UniqueCount_Ngram(str(x), 2))\n",
    "\treturn df\n",
    "merge = parallelize_dataframe(merge, get_bigram_uc_name)\n",
    "\n",
    "def get_bigram_uc_item_desc(df):\n",
    "\tdf['item_desc_2_uc'] = \\\n",
    "\t\tdf['item_description'].map(lambda x: UniqueCount_Ngram(str(x), 2))\n",
    "\treturn df\n",
    "merge = parallelize_dataframe(merge, get_bigram_uc_item_desc)\n",
    "\n",
    "def get_bigram_ur_name(df):\n",
    "\tdf['name_2_ur'] = df['name'].map(lambda x: UniqueRatio_Ngram(str(x), 2))\n",
    "\treturn df\n",
    "merge = parallelize_dataframe(merge, get_bigram_ur_name)\n",
    "\n",
    "def get_bigram_ur_item_desc(df):\n",
    "\tdf['item_desc_2_ur'] = \\\n",
    "\t\tdf['item_description'].map(lambda x: UniqueRatio_Ngram(str(x), 2))\n",
    "\treturn df\n",
    "merge = parallelize_dataframe(merge, get_bigram_ur_item_desc)\n",
    "\n",
    "# def get_trigram_uc_name(df):\n",
    "# \tdf['name_3_uc'] = df['name'].map(lambda x: UniqueCount_Ngram(str(x), 3))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_trigram_uc_name)\n",
    "\n",
    "# def get_trigram_uc_item_desc(df):\n",
    "# \tdf['item_desc_3_uc'] = \\\n",
    "# \t\tdf['item_description'].map(lambda x: UniqueCount_Ngram(str(x), 3))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_trigram_uc_item_desc)\n",
    "\n",
    "# def get_trigram_ur_name(df):\n",
    "# \tdf['name_3_ur'] = df['name'].map(lambda x: UniqueRatio_Ngram(str(x), 3))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_trigram_ur_name)\n",
    "\n",
    "# def get_trigram_ur_item_desc(df):\n",
    "# \tdf['item_desc_3_ur'] = \\\n",
    "# \t\tdf['item_description'].map(lambda x: UniqueRatio_Ngram(str(x), 3))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_trigram_ur_item_desc)\n",
    "\n",
    "# print('[{}] Finished ngram count for name, item_desc'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# ngrams = [1,2,3]\n",
    "# token_pattern =' '\n",
    "# for f in obs_fields:\n",
    "# \tfor n in ngrams:\n",
    "# \t\tcols.append(f+'_{}_uc'.format(n))\n",
    "# \t\tcols.append(f+'_{}_ur'.format(n))\n",
    "\n",
    "# f = 'category_name'\n",
    "# merge[f+'_{}_uc'.format(n)] = merge[f].map(lambda x: UniqueCount_Ngram(str(x), n, '/'))\n",
    "# merge[f+'_{}_ur'.format(n)] = merge[f].map(lambda x: UniqueRatio_Ngram(str(x), n, '/'))\n",
    "# cols.append(f+'_{}_uc'.format(n))\n",
    "# cols.append(f+'_{}_ur'.format(n))\n",
    "\t\t\n",
    "# remove constatnt cols\n",
    "merge =  merge.loc[:, (merge != merge.iloc[0]).any()]\n",
    "print(len(cols))\n",
    "del cols\n",
    "cols = list(set(merge.columns) - cols1)\n",
    "print(len(cols))\n",
    "\n",
    "X_b = merge[cols]\n",
    "\n",
    "print('[{}] Finished X_basic1'.format(time.time() - start_time))\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_b = scaler.fit_transform(X_b)\n",
    "X_basic = csr_matrix(X_b)\n",
    "print('basic: ', X_basic.data.nbytes)\n",
    "print('[{}] Finished X_basic2'.format(time.time() - start_time))\n",
    "del X_b\n",
    "for c in cols:\n",
    "    merge = merge.drop(c, axis=1)\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[452.7408776283264] Finished tag\n",
      "cpu: 10.4\n",
      "consuming 4.88GB RAM\n",
      "[463.88344526290894] Finished to cut\n",
      "[464.9071350097656] Finished to convert categorical\n"
     ]
    }
   ],
   "source": [
    "\n",
    "abbr = {}\n",
    "abbr['BNWT'] = ['bnwt', 'brand new with tags']\n",
    "abbr['NWT'] = ['nwt', 'new with tags']\n",
    "abbr['BNWOT'] = ['bnwot', 'brand new with out tags', 'brand new without tags']\n",
    "abbr['NWOT'] = ['nwot', 'new with out tags', 'new without tags']\n",
    "abbr['BNIP'] = ['bnip', 'brand new in packet', 'brand new in packet']\n",
    "abbr['NIP'] = ['nip', 'new in packet', 'new in packet']\n",
    "abbr['BNIB'] = ['bnib', 'brand new in box']\n",
    "abbr['NIB'] = ['nib', 'new in box']\n",
    "abbr['MIB'] = ['mib', 'mint in box']\n",
    "abbr['MWOB'] = ['mwob', 'mint with out box', 'mint without box']\n",
    "abbr['MIP'] = ['mip', 'mint in packet']\n",
    "abbr['MWOP'] = ['mwop', 'mint with out packet', 'mint without packet']\n",
    "\n",
    "merge['tag'] = merge['item_description'].map(lambda a: 'BNWT' if any(x in a.lower() for x in abbr['BNWT'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'NWT' if any(x in a.lower() for x in abbr['NWT'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'BNWOT' if any(x in a.lower() for x in abbr['BNWOT'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'NWOT' if any(x in a.lower() for x in abbr['NWOT'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'BNIP' if any(x in a.lower() for x in abbr['BNIP'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'NIP' if any(x in a.lower() for x in abbr['NIP'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'BNIB' if any(x in a.lower() for x in abbr['BNIB'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'NIB' if any(x in a.lower() for x in abbr['NIB'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'MIB' if any(x in a.lower() for x in abbr['MIB'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'MWOB' if any(x in a.lower() for x in abbr['MWOB'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'MIP' if any(x in a.lower() for x in abbr['MIP'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'MWOP' if any(x in a.lower() for x in abbr['MWOP'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'None')\n",
    "print('[{}] Finished tag'.format(time.time() - start_time))\n",
    "del abbr\n",
    "print_memory_usage()\n",
    "\n",
    "# merge['bci'] = merge['brand_name'].astype('str') + ' ' + merge['category_name'].astype('str') + ' ' + \\\n",
    "# \t\t\tmerge['item_condition_id'].astype('str')\n",
    "\n",
    "# merge['bc'] = merge['brand_name'].astype('str') + ' ' + merge['category_name'].astype('str')\n",
    "\n",
    "# merge['bcis'] = merge['brand_name'].astype('str') + ' ' \\\n",
    "# \t\t\t\t+ merge['category_name'].astype('str') + ' ' + \\\n",
    "# \t\t\t\tmerge['item_condition_id'].astype('str') + ' ' + \\\n",
    "# \t\t\t\tmerge['shipping'].astype('str')\n",
    "\n",
    "# merge['bcs'] = merge['brand_name'].astype('str') + ' ' + \\\n",
    "# \t\t\t\tmerge['category_name'].astype('str') + ' ' + \\\n",
    "# \t\t\t\tmerge['shipping'].astype('str')\n",
    "\n",
    "# # merge['bi'] = merge['brand_name'].astype('str') + '_' +   merge['item_condition_id'].astype('str')\n",
    "\t\n",
    "# # merge['ci'] = merge['category_name'].astype('str') + '_' + merge['item_condition_id'].astype('str')\n",
    "\n",
    "# print('[{}] Finished creating bci bc bi ci bcs bcis'.format(time.time() - start_time))\n",
    "# print_memory_usage()\n",
    "\n",
    "\n",
    "# merge.drop(['bci', 'bc'], axis=1, inplace=True)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_sentiment_score)\n",
    "# merge['sentiment_score'] = merge['item_description'].map(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# print('[{}] Finished sentiment score'.format(time.time() - start_time))\n",
    "# a = merge['sentiment_score'].values\n",
    "# print(np.min(a))\n",
    "# print(np.max(a))\n",
    "\n",
    "# print_memory_usage()\n",
    "# merge['sentiment'] = merge['sentiment_score'].map(lambda x: 'VPos' if x > 0.5 \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# else 'Pos' if (x <= 0.5) and (x > 0)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# else 'Neu' if  x == 0 \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# else 'Neg' if (x < 0) and (x >= -0.5)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# else 'VNeg')\n",
    "\n",
    "# print('[{}] Finished sentiment'.format(time.time() - start_time))\n",
    "\n",
    "cutting(merge)\n",
    "print('[{}] Finished to cut'.format(time.time() - start_time))\n",
    "\n",
    "to_categorical(merge)\n",
    "print('[{}] Finished to convert categorical'.format(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_desc_dict = dict()\n",
    "item_desc_dict['14 k'] = '14k'\n",
    "item_desc_dict['14 K'] = '14K'\n",
    "item_desc_dict['8 gb'] = '8gb'\n",
    "item_desc_dict['16 gb'] = '16gb'\n",
    "item_desc_dict['32 gb'] = '32gb'\n",
    "item_desc_dict['64 gb'] = '64gb'\n",
    "item_desc_dict['128 gb'] = '128gb'\n",
    "item_desc_dict['256 gb'] = '256gb'\n",
    "item_desc_dict['512 gb'] = '512gb'\n",
    "item_desc_dict['14 kt'] = '14kt'\n",
    "item_desc_dict['18 k'] = '18k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'128 gb': '128gb',\n",
       " '14 K': '14K',\n",
       " '14 k': '14k',\n",
       " '14 kt': '14kt',\n",
       " '16 gb': '16gb',\n",
       " '18 k': '18k',\n",
       " '256 gb': '256gb',\n",
       " '32 gb': '32gb',\n",
       " '512 gb': '512gb',\n",
       " '64 gb': '64gb',\n",
       " '8 gb': '8gb'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_desc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 s, sys: 0 ns, total: 11.3 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merge['item_description'] = merge['item_description'].replace(item_desc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[988.7923526763916] Finished count vectorize `name`\n",
      "(4949330, 13549588)\n",
      "0.0\n",
      "1.0\n",
      "cpu: 9.6\n",
      "consuming 13.45GB RAM\n",
      "[1012.995099067688] Finished count vectorize `category_name`\n",
      "(4949330, 1022)\n",
      "0.0\n",
      "1.0\n",
      "cpu: 9.4\n",
      "consuming 11.25GB RAM\n"
     ]
    }
   ],
   "source": [
    "# tv = TfidfVectorizer(max_features=2 ** 14,\n",
    "#                      min_df=NAME_MIN_DF,\n",
    "# \t\t\t\t\t ngram_range=(1, 3),\n",
    "# \t\t\t\t\t stop_words='english')\n",
    "# X_name1 = tv.fit_transform(merge['name'])\n",
    "# print('[{}] Finished TFIDF vectorize `name`'.format(time.time() - start_time))\n",
    "# print(X_name1.shape)\n",
    "# print(np.min(X_name1))\n",
    "# print(np.max(X_name1))\n",
    "# # del merge['item_description']\n",
    "# print_memory_usage()\n",
    "\n",
    "# cv = CountVectorizer(min_df=NAME_MIN_DF, stop_words='english')\n",
    "# cv = CountVectorizer(analyzer='char', ngram_range=(2,11), max_features=31) # trans 3\n",
    "# X_name1 = cv.fit_transform(merge['name'])\n",
    "# norm = Normalizer()\n",
    "# X_name1 = norm.fit_transform(X_name1)\n",
    "# print('[{}] Finished count vectorize `name`'.format(time.time() - start_time))\n",
    "# print(X_name1.shape)\n",
    "# print(np.min(X_name1))\n",
    "# print(np.max(X_name1))\n",
    "# # del merge['name']\n",
    "# print_memory_usage()\n",
    "\n",
    "# cv = CountVectorizer(analyzer='char', ngram_range=(4,9), max_features=23064) # trans 3\n",
    "# cv = CountVectorizer(analyzer='char', ngram_range=(4,9)) # trans 4\n",
    "# cv = CountVectorizer(min_df=NAME_MIN_DF, stop_words='english', analyzer='char', ngram_range=(4,9)) # trans 5\n",
    "# cv = CountVectorizer(analyzer='char', ngram_range=(4,8)) # trans 6\n",
    "cv = CountVectorizer(analyzer='char', ngram_range=(3,8)) # trans 7\n",
    "# cv = CountVectorizer(analyzer='char', ngram_range=(3,7)) # trans 8\n",
    "# cv = CountVectorizer(min_df=NAME_MIN_DF,analyzer='char', ngram_range=(3,7)) # trans 9\n",
    "# cv = CountVectorizer(min_df=10,analyzer='char',ngram_range=(3,8)) # trans 10\n",
    "# cv = CountVectorizer(min_df=25,analyzer='char',ngram_range=(3,8)) # trans 11\n",
    "X_name = cv.fit_transform(merge['name'])\n",
    "norm = Normalizer()\n",
    "X_name = norm.fit_transform(X_name)\n",
    "print('[{}] Finished count vectorize `name`'.format(time.time() - start_time))\n",
    "print(X_name.shape)\n",
    "print(np.min(X_name))\n",
    "print(np.max(X_name))\n",
    "del merge['name']\n",
    "print_memory_usage()\n",
    "\n",
    "# cv = CountVectorizer(analyzer='char',ngram_range=(3,8)) # ct 1\n",
    "# cv = CountVectorizer(analyzer='char',ngram_range=(12,30)) # ct 2\n",
    "cv = CountVectorizer()\n",
    "X_category = cv.fit_transform(merge['category_name'])\n",
    "norm = Normalizer()\n",
    "X_category = norm.fit_transform(X_category)\n",
    "print('[{}] Finished count vectorize `category_name`'.format(time.time() - start_time))\n",
    "print(X_category.shape)\n",
    "print(np.min(X_category))\n",
    "print(np.max(X_category))\n",
    "del merge['category_name']\n",
    "gc.collect()\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1332.505126953125] Finished TFIDF vectorize `item_description`\n",
      "(4949330, 16384)\n",
      "0.0\n",
      "1.0\n",
      "cpu: 9.5\n",
      "consuming 13.87GB RAM\n",
      "[1355.9372987747192] Finished label binarize `brand_name`\n",
      "(4949330, 4001)\n",
      "cpu: 9.6\n",
      "consuming 14.05GB RAM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cv = CountVectorizer()\n",
    "# X_bci_cv = cv.fit_transform(merge['bci'])\n",
    "# norm = Normalizer()\n",
    "# X_bci_cv = norm.fit_transform(X_bci_cv)\n",
    "# print('[{}] Finished count vectorize `X_bci_cv`'.format(time.time() - start_time))\n",
    "# print(X_bci_cv.shape)\n",
    "# print(np.min(X_bci_cv))\n",
    "# print(np.max(X_bci_cv))\n",
    "# del merge['bci']\n",
    "# gc.collect()\n",
    "# print_memory_usage()\n",
    "\n",
    "tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION, # tv2\n",
    "\t\t\t\t\t ngram_range=(1, 3),\n",
    "\t\t\t\t\t stop_words='english')\n",
    "\n",
    "# tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION, # tv1\n",
    "#                      min_df=NAME_MIN_DF,\n",
    "# \t\t\t\t\t ngram_range=(1, 2),\n",
    "# \t\t\t\t\t stop_words='english')\n",
    "X_description = tv.fit_transform(merge['item_description'])\n",
    "print('[{}] Finished TFIDF vectorize `item_description`'.format(time.time() - start_time))\n",
    "print(X_description.shape)\n",
    "print(np.min(X_description))\n",
    "print(np.max(X_description))\n",
    "del merge['item_description']\n",
    "print_memory_usage()\n",
    "\n",
    "# X_cos = cosine_similarity(X_description, dense_output=False)\n",
    "# X_cos = squareform(pdist(np.asarray(X_description.toarray()), 'cosine'))\n",
    "# print(X_cos.shape)\n",
    "# print('[{}] Finished cosine similarity'.format(time.time() - start_time))\n",
    "# print_memory_usage()\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb.fit_transform(merge['brand_name'])\n",
    "print('[{}] Finished label binarize `brand_name`'.format(time.time() - start_time))\n",
    "print(X_brand.shape)\n",
    "del merge['brand_name']\n",
    "print_memory_usage()\n",
    "\n",
    "# lb = LabelBinarizer(sparse_output=True)\n",
    "# X_bci = lb.fit_transform(merge['bci'])\n",
    "# print('[{}] Finished label binarize `bci`'.format(time.time() - start_time))\n",
    "# print(X_bci.shape)\n",
    "# del merge['bci']\n",
    "# print_memory_usage()\n",
    "\n",
    "# # lb = LabelBinarizer(sparse_output=True)\n",
    "# # X_bc = lb.fit_transform(merge['bc'])\n",
    "# # print('[{}] Finished label binarize `bc`'.format(time.time() - start_time))\n",
    "# # print(X_bc.shape)\n",
    "# # del merge['bc']\n",
    "# # print_memory_usage()\n",
    "\n",
    "# lb = LabelBinarizer(sparse_output=True)\n",
    "# X_bcis = lb.fit_transform(merge['bcis'])\n",
    "# print('[{}] Finished label binarize `bcis`'.format(time.time() - start_time))\n",
    "# print(X_bcis.shape)\n",
    "# del merge['bcis']\n",
    "# gc.collect()\n",
    "# print_memory_usage()\n",
    "\n",
    "# lb = LabelBinarizer(sparse_output=True)\n",
    "# X_bcs = lb.fit_transform(merge['bcs'])\n",
    "# print('[{}] Finished label binarize `bcs`'.format(time.time() - start_time))\n",
    "# print(X_bcs.shape)\n",
    "# del merge['bcs']\n",
    "# gc.collect()\n",
    "# print_memory_usage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1374.4303419589996] Finished to get dummies on `item_condition_id` and `shipping`\n",
      "(4949330, 17)\n",
      "cpu: 9.6\n",
      "consuming 14.28GB RAM\n",
      "cpu: 10.1\n",
      "consuming 13.97GB RAM\n"
     ]
    }
   ],
   "source": [
    "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping', \n",
    "\t\t\t\t\t\t\t\t\t\t\t'tag']], sparse=True).values)\n",
    "print('[{}] Finished to get dummies on `item_condition_id` and `shipping`'.format(time.time() - start_time))\n",
    "print(X_dummies.shape)\n",
    "print_memory_usage()\n",
    "\n",
    "del merge\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic:  606204864\n",
      "dummies:  96911280\n",
      "description:  753868392\n",
      "brand:  39594640\n",
      "category:  158521288\n",
      "name:  5039315152\n",
      "[1394.6717190742493] Finished to create sparse merge\n",
      "cpu: 9.7\n",
      "consuming 13.92GB RAM\n",
      "(1482535, 13571036)\n",
      "cpu: 9.5\n",
      "consuming 23.27GB RAM\n",
      "cpu: 9.7\n",
      "consuming 13.92GB RAM\n"
     ]
    }
   ],
   "source": [
    "print('basic: ', X_basic.data.nbytes)\n",
    "# print('bcis: ', X_bcis.data.nbytes)\n",
    "# print('bci: ', X_bci.data.nbytes)\n",
    "print('dummies: ', X_dummies.data.nbytes)\n",
    "print('description: ', X_description.data.nbytes)\n",
    "print('brand: ', X_brand.data.nbytes)\n",
    "print('category: ', X_category.data.nbytes)\n",
    "print('name: ', X_name.data.nbytes)\n",
    "# print('name1: ', X_name1.data.nbytes)\n",
    "\n",
    "sparse_merge = hstack((X_basic,X_dummies, X_description, X_brand, X_category, X_name)).tocsr()\n",
    "print('[{}] Finished to create sparse merge'.format(time.time() - start_time))\n",
    "\n",
    "del X_basic,  X_dummies, X_description, X_brand, X_category, X_name\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "X = sparse_merge[:nrow_train]\n",
    "X_test = sparse_merge[nrow_train:]\n",
    "\n",
    "print(X.shape)\n",
    "print_memory_usage()\n",
    "\n",
    "del sparse_merge\n",
    "gc.collect()\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu: 9.8\n",
      "consuming 16.67GB RAM\n",
      "cpu: 0.0\n",
      "consuming 16.67GB RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[246]\ttraining's rmse: 0.413519\tvalid_1's rmse: 0.464266\n",
      "[4617.26814866066] Finished to train lgbm\n",
      "[4845.154832839966] Finished to train predict lgbm\n",
      "cpu: 34.0\n",
      "consuming 25.57GB RAM\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.01, random_state = 0) \n",
    "print_memory_usage()\n",
    "# d_train = lgb.Dataset(X, label=y, max_bin=8192)\n",
    "d_train = lgb.Dataset(train_X, label=train_y, max_bin=8192)\n",
    "d_valid = lgb.Dataset(valid_X, label=valid_y, max_bin=8192)\n",
    "watchlist = [d_train, d_valid]\n",
    "print_memory_usage()\n",
    "\n",
    "params = {\n",
    "\t'learning_rate': 0.75,\n",
    "\t'application': 'regression',\n",
    "\t'max_depth': 3,\n",
    "\t'num_leaves': 100,\n",
    "\t'verbosity': -1,\n",
    "\t'metric': 'RMSE',\n",
    "\t'num_threads': 4\n",
    "}\n",
    "\n",
    "\n",
    "params = {\n",
    "    'num_leaves': 100,\n",
    "    'objective': 'regression',\n",
    "#     'min_data_in_leaf': 300,\n",
    "    'learning_rate': 0.75,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 2,\n",
    "    'metric': 'RMSE',\n",
    "    'num_threads': 4\n",
    "}\n",
    "\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, valid_sets=watchlist,\n",
    "\t\t\t\t\tnum_boost_round=5000,early_stopping_rounds=100,verbose_eval=500) \n",
    "print('[{}] Finished to train lgbm'.format(time.time() - start_time))\n",
    "preds = model.predict(X_test)\n",
    "print('[{}] Finished to train predict lgbm'.format(time.time() - start_time))\n",
    "del model, d_train, d_valid\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# org: [3384]\ttraining's rmse: 0.427716\tvalid_1's rmse: 0.451961\n",
    "# [2405]\ttraining's rmse: 0.436488\tvalid_1's rmse: 0.461458 # trans3\n",
    "# [2702]\ttraining's rmse: 0.428634\tvalid_1's rmse: 0.451875 # trans4\n",
    "# [2909]\ttraining's rmse: 0.42591\tvalid_1's rmse: 0.452233 # trans5\n",
    "# [2770]\ttraining's rmse: 0.427799\tvalid_1's rmse: 0.452547 # trans6\n",
    "# [3766]\ttraining's rmse: 0.415937\tvalid_1's rmse: 0.448262 $ trans7 (4600 seconds!!!)\n",
    "# [2657]\ttraining's rmse: 0.427097\tvalid_1's rmse: 0.450507 # trans8\n",
    "# [3002]\ttraining's rmse: 0.423921\tvalid_1's rmse: 0.451377 # trans 9 + (1,2)\n",
    "# [3002]\ttraining's rmse: 0.423921\tvalid_1's rmse: 0.452078 # trans 10 + ct1 + tv1\n",
    "# [2334]\ttraining's rmse: 0.430973\tvalid_1's rmse: 0.45343 # trans 10 + ct2 + tv1\n",
    "# [2287]\ttraining's rmse: 0.432902\tvalid_1's rmse: 0.453576 # trans10 + tv1\n",
    "# [3942]\ttraining's rmse: 0.415455\tvalid_1's rmse: 0.452935 # trans11 + tv2\n",
    "# [3316]\ttraining's rmse: 0.420618\tvalid_1's rmse: 0.44665 # trans7 + tv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# submission=pd.DataFrame()\n",
    "# submission['test_id'] = test_id\n",
    "# submission['price'] = np.expm1(preds)\n",
    "# submission.to_csv(\"submission_lgbm_nlp2.csv\", index=False)\n",
    "preds *= 0.6\n",
    "# print('[{}] Finished submission lgbm'.format(time.time() - start_time))\n",
    "if nrow_test < 700000:\n",
    "\tpreds = preds[:nrow_test]\n",
    "\n",
    "model = Ridge(solver=\"saga\", fit_intercept=True, random_state=205)\n",
    "model.fit(X, y)\n",
    "print('[{}] Finished to train ridge'.format(time.time() - start_time))\n",
    "preds1 = model.predict(X=X_test)\n",
    "print('[{}] Finished to predict ridge'.format(time.time() - start_time))\n",
    "# submission['price'] = np.expm1(preds1)\n",
    "# submission.to_csv(\"submission_ridge_nlp2.csv\", index=False)\n",
    "print_memory_usage()\n",
    "if nrow_test < 700000:\n",
    "\tpreds1 = preds1[:nrow_test]\n",
    "\t\n",
    "preds += 0.4*preds1\n",
    "submission['price'] = np.expm1(preds)\n",
    "submission.to_csv(\"submission_lgbm_ridge_nlp2.csv\", index=False)\n",
    "# if __name__ == '__main__':\n",
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
