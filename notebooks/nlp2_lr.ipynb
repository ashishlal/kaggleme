{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from textblob import TextBlob\n",
    "import lightgbm as lgb\n",
    "import os, psutil\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from collections import Counter\n",
    "import re\n",
    "import lzma\n",
    "import Levenshtein\n",
    "from numba import jit\n",
    "\n",
    "import sys\n",
    "sys.stdout = open('/dev/stdout', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "NAME_MIN_DF = 10\n",
    "MAX_FEATURES_ITEM_DESCRIPTION = 2 ** 14\n",
    "NUM_PARTITIONS = 12 #number of partitions to split dataframe\n",
    "NUM_CORES = 8 #number of cores on your machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "import random, copy, struct\n",
    "from hashlib import sha1\n",
    "\n",
    "# The size of a hash value in number of bytes\n",
    "hashvalue_byte_size = len(bytes(np.int64(42).data))\n",
    "\n",
    "# http://en.wikipedia.org/wiki/Mersenne_prime\n",
    "_mersenne_prime = (1 << 61) - 1\n",
    "_max_hash = (1 << 32) - 1\n",
    "_hash_range = (1 << 32)\n",
    "\n",
    "class MinHash(object):\n",
    "    '''MinHash is a probabilistic data structure for computing \n",
    "    `Jaccard similarity`_ between sets.\n",
    " \n",
    "    Args:\n",
    "        num_perm (int, optional): Number of random permutation functions.\n",
    "            It will be ignored if `hashvalues` is not None.\n",
    "        seed (int, optional): The random seed controls the set of random \n",
    "            permutation functions generated for this MinHash.\n",
    "        hashobj (optional): The hash function used by this MinHash. \n",
    "            It must implements\n",
    "            the `digest()` method similar to hashlib_ hash functions, such\n",
    "            as `hashlib.sha1`.\n",
    "        hashvalues (`numpy.array` or `list`, optional): The hash values is \n",
    "            the internal state of the MinHash. It can be specified for faster \n",
    "            initialization using the existing state from another MinHash. \n",
    "        permutations (optional): The permutation function parameters. This argument\n",
    "            can be specified for faster initialization using the existing\n",
    "            state from another MinHash.\n",
    "    \n",
    "    Note:\n",
    "        To save memory usage, consider using :class:`datasketch.LeanMinHash`.\n",
    "        \n",
    "    Note:\n",
    "        Since version 1.1.1, MinHash will only support serialization using \n",
    "        `pickle`_. ``serialize`` and ``deserialize`` methods are removed, \n",
    "        and are supported in :class:`datasketch.LeanMinHash` instead. \n",
    "        MinHash serialized before version 1.1.1 cannot be deserialized properly \n",
    "        in newer versions (`need to migrate? <https://github.com/ekzhu/datasketch/issues/18>`_). \n",
    "    Note:\n",
    "        Since version 1.1.3, MinHash uses Numpy's random number generator \n",
    "        instead of Python's built-in random package. This change makes the \n",
    "        hash values consistent across different Python versions.\n",
    "        The side-effect is that now MinHash created before version 1.1.3 won't\n",
    "        work (i.e., ``jaccard``, ``merge`` and ``union``)\n",
    "        with those created after. \n",
    "    .. _`Jaccard similarity`: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    .. _hashlib: https://docs.python.org/3.5/library/hashlib.html\n",
    "    .. _`pickle`: https://docs.python.org/3/library/pickle.html\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_perm=128, seed=1, hashobj=sha1,\n",
    "            hashvalues=None, permutations=None):\n",
    "        if hashvalues is not None:\n",
    "            num_perm = len(hashvalues)\n",
    "        if num_perm > _hash_range:\n",
    "            # Because 1) we don't want the size to be too large, and\n",
    "            # 2) we are using 4 bytes to store the size value\n",
    "            raise ValueError(\"Cannot have more than %d number of\\\n",
    "                    permutation functions\" % _hash_range)\n",
    "        self.seed = seed\n",
    "        self.hashobj = hashobj\n",
    "        # Initialize hash values\n",
    "        if hashvalues is not None:\n",
    "            self.hashvalues = self._parse_hashvalues(hashvalues)\n",
    "        else:\n",
    "            self.hashvalues = self._init_hashvalues(num_perm)\n",
    "        # Initalize permutation function parameters\n",
    "        if permutations is not None:\n",
    "            self.permutations = permutations\n",
    "        else:\n",
    "            generator = np.random.RandomState(self.seed)\n",
    "            # Create parameters for a random bijective permutation function\n",
    "            # that maps a 32-bit hash value to another 32-bit hash value.\n",
    "            # http://en.wikipedia.org/wiki/Universal_hashing\n",
    "            self.permutations = np.array([(generator.randint(1, _mersenne_prime, dtype=np.uint64),\n",
    "                                           generator.randint(0, _mersenne_prime, dtype=np.uint64))\n",
    "                                          for _ in range(num_perm)], dtype=np.uint64).T\n",
    "        if len(self) != len(self.permutations[0]):\n",
    "            raise ValueError(\"Numbers of hash values and permutations mismatch\")\n",
    "\n",
    "    def _init_hashvalues(self, num_perm):\n",
    "        return np.ones(num_perm, dtype=np.uint64)*_max_hash\n",
    "\n",
    "    def _parse_hashvalues(self, hashvalues):\n",
    "        return np.array(hashvalues, dtype=np.uint64)\n",
    "    @jit\n",
    "    def update(self, b):\n",
    "        '''Update this MinHash with a new value.\n",
    "        \n",
    "        Args:\n",
    "            b (bytes): The value of type `bytes`.\n",
    "            \n",
    "        Example:\n",
    "            To update with a new string value:\n",
    "            \n",
    "            .. code-block:: python\n",
    "                minhash.update(\"new value\".encode('utf-8'))\n",
    "        '''\n",
    "        hv = struct.unpack('<I', self.hashobj(b).digest()[:4])[0]\n",
    "        a, b = self.permutations\n",
    "        phv = np.bitwise_and((a * hv + b) % _mersenne_prime, np.uint64(_max_hash))\n",
    "        self.hashvalues = np.minimum(phv, self.hashvalues)\n",
    "    @jit\n",
    "    def jaccard(self, other):\n",
    "        '''Estimate the `Jaccard similarity`_ (resemblance) between the sets\n",
    "        represented by this MinHash and the other.\n",
    "        \n",
    "        Args:\n",
    "            other (datasketch.MinHash): The other MinHash.\n",
    "            \n",
    "        Returns:\n",
    "            float: The Jaccard similarity, which is between 0.0 and 1.0.\n",
    "        '''\n",
    "        if other.seed != self.seed:\n",
    "            raise ValueError(\"Cannot compute Jaccard given MinHash with\\\n",
    "                    different seeds\")\n",
    "        if len(self) != len(other):\n",
    "            raise ValueError(\"Cannot compute Jaccard given MinHash with\\\n",
    "                    different numbers of permutation functions\")\n",
    "        return np.float(np.count_nonzero(self.hashvalues==other.hashvalues)) /\\\n",
    "                np.float(len(self))\n",
    "    @jit\n",
    "    def count(self):\n",
    "        '''Estimate the cardinality count based on the technique described in\n",
    "        `this paper <http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=365694>`_.\n",
    "        \n",
    "        Returns:\n",
    "            int: The estimated cardinality of the set represented by this MinHash.\n",
    "        '''\n",
    "        k = len(self)\n",
    "        return np.float(k) / np.sum(self.hashvalues / np.float(_max_hash)) - 1.0\n",
    "    @jit\n",
    "    def merge(self, other):\n",
    "        '''Merge the other MinHash with this one, making this one the union\n",
    "        of both.\n",
    "        \n",
    "        Args:\n",
    "            other (datasketch.MinHash): The other MinHash.\n",
    "        '''\n",
    "        if other.seed != self.seed:\n",
    "            raise ValueError(\"Cannot merge MinHash with\\\n",
    "                    different seeds\")\n",
    "        if len(self) != len(other):\n",
    "            raise ValueError(\"Cannot merge MinHash with\\\n",
    "                    different numbers of permutation functions\")\n",
    "        self.hashvalues = np.minimum(other.hashvalues, self.hashvalues)\n",
    "    @jit\n",
    "    def digest(self):\n",
    "        '''Export the hash values, which is the internal state of the\n",
    "        MinHash.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.array: The hash values which is a Numpy array.\n",
    "        '''\n",
    "        return copy.copy(self.hashvalues)\n",
    "    @jit\n",
    "    def is_empty(self):\n",
    "        '''\n",
    "        Returns: \n",
    "            bool: If the current MinHash is empty - at the state of just\n",
    "                initialized.\n",
    "        '''\n",
    "        if np.any(self.hashvalues != _max_hash):\n",
    "            return False\n",
    "        return True\n",
    "    @jit\n",
    "    def clear(self):\n",
    "        '''\n",
    "        Clear the current state of the MinHash.\n",
    "        All hash values are reset.\n",
    "        '''\n",
    "        self.hashvalues = self._init_hashvalues(len(self))\n",
    "\n",
    "    @jit\n",
    "    def copy(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            datasketch.MinHash: A copy of this MinHash by exporting its\n",
    "                state.\n",
    "        '''\n",
    "        return MinHash(seed=self.seed, hashvalues=self.digest(),\n",
    "                permutations=self.permutations)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            int: The number of hash values.\n",
    "        '''\n",
    "        return len(self.hashvalues)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        '''\n",
    "        Returns:\n",
    "            bool: If their seeds and hash values are both equal then two\n",
    "                are equivalent.\n",
    "        '''\n",
    "        return self.seed == other.seed and \\\n",
    "                np.array_equal(self.hashvalues, other.hashvalues)\n",
    "                \n",
    "    @classmethod\n",
    "    @jit\n",
    "    def union(cls, *mhs):\n",
    "        '''Create a MinHash which is the union of the MinHash objects passed as arguments.\n",
    "        Args:\n",
    "            *mhs: The MinHash objects to be united. The argument list length is variable,\n",
    "                but must be at least 2.\n",
    "        \n",
    "        Returns:\n",
    "            datasketch.MinHash: A new union MinHash.\n",
    "        '''\n",
    "        if len(mhs) < 2:\n",
    "            raise ValueError(\"Cannot union less than 2 MinHash\")\n",
    "        num_perm = len(mhs[0])\n",
    "        seed = mhs[0].seed\n",
    "        if any((seed != m.seed or num_perm != len(m)) for m in mhs):\n",
    "            raise ValueError(\"The unioning MinHash must have the\\\n",
    "                    same seed and number of permutation functions\")\n",
    "        hashvalues = np.minimum.reduce([m.hashvalues for m in mhs])\n",
    "        permutations = mhs[0].permutations\n",
    "        return cls(num_perm=num_perm, seed=seed, hashvalues=hashvalues,\n",
    "                permutations=permutations)\n",
    "###################################################################################\n",
    "\n",
    "def rmsle(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n",
    "\n",
    "def split_cat(text):\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['category_name'] = dataset['category_name'].astype('category')\n",
    "    dataset['brand_name'] = dataset['brand_name'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n",
    "\n",
    "def print_memory_usage():\n",
    "    print('cpu: {}'.format(psutil.cpu_percent()))\n",
    "    print('consuming {:.2f}GB RAM'.format(\n",
    "    \t   psutil.Process(os.getpid()).memory_info().rss / 1073741824),\n",
    "    \t  flush=True)\n",
    "\n",
    "\n",
    "def _sigmoid(score):\n",
    "    p = 1. / (1. + np.exp(-score))\n",
    "    return p\n",
    "\n",
    "\n",
    "def _logit(p):\n",
    "    return np.log(p/(1.-p))\n",
    "\n",
    "\n",
    "def _softmax(score):\n",
    "    score = np.asarray(score, dtype=float)\n",
    "    score = np.exp(score - np.max(score))\n",
    "    score /= np.sum(score, axis=1)[:,np.newaxis]\n",
    "    return score\n",
    "\n",
    "\n",
    "def _cast_proba_predict(proba):\n",
    "    N = proba.shape[1]\n",
    "    w = np.arange(1,N+1)\n",
    "    pred = proba * w[np.newaxis,:]\n",
    "    pred = np.sum(pred, axis=1)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def _one_hot_label(label, n_classes):\n",
    "    num = label.shape[0]\n",
    "    tmp = np.zeros((num, n_classes), dtype=int)\n",
    "    tmp[np.arange(num),label.astype(int)] = 1\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def _majority_voting(x, weight=None):\n",
    "    ## apply weight\n",
    "    if weight is not None:\n",
    "    \tassert len(weight) == len(x)\n",
    "    \tx = np.repeat(x, weight)\n",
    "    c = Counter(x)\n",
    "    value, count = c.most_common()[0]\n",
    "    return value\n",
    "\n",
    "\n",
    "def _voter(x, weight=None):\n",
    "    idx = np.isfinite(x)\n",
    "    if sum(idx) == 0:\n",
    "    \tvalue = config.MISSING_VALUE_NUMERIC\n",
    "    else:\n",
    "    \tif weight is not None:\n",
    "    \t\tvalue = _majority_voting(x[idx], weight[idx])\n",
    "    \telse:\n",
    "    \t\tvalue = _majority_voting(x[idx])\n",
    "    return value\n",
    "\n",
    "\n",
    "def _array_majority_voting(X, weight=None):\n",
    "    y = np.apply_along_axis(_voter, axis=1, arr=X, weight=weight)\n",
    "    return y\n",
    "\n",
    "\n",
    "def _mean(x):\n",
    "    idx = np.isfinite(x)\n",
    "    if sum(idx) == 0:\n",
    "    \tvalue = float(config.MISSING_VALUE_NUMERIC) # cast it to float to accommodate the np.mean\n",
    "    else:\n",
    "    \tvalue = np.mean(x[idx]) # this is float!\n",
    "    return value\n",
    "\n",
    "\n",
    "def _array_mean(X):\n",
    "    y = np.apply_along_axis(_mean, axis=1, arr=X)\n",
    "    return y\n",
    "\n",
    "\n",
    "def _corr(x, y_train):\n",
    "    if _dim(x) == 1:\n",
    "    \tcorr = pearsonr(x.flatten(), y_train)[0]\n",
    "    \tif str(corr) == \"nan\":\n",
    "    \t\tcorr = 0.\n",
    "    else:\n",
    "    \tcorr = 1.\n",
    "    return corr\n",
    "\n",
    "\n",
    "def _dim(x):\n",
    "    d = 1 if len(x.shape) == 1 else x.shape[1]\n",
    "    return d\n",
    "\n",
    "@jit\n",
    "def _entropy(proba):\n",
    "    entropy = -np.sum(proba*np.log(proba))\n",
    "    return entropy\n",
    "\n",
    "@jit\n",
    "def _try_divide(x, y, val=0.0):\n",
    "    \"\"\"try to divide two numbers\"\"\"\n",
    "    if y != 0.0:\n",
    "    \tval = float(x) / y\n",
    "    return val\n",
    "\n",
    "@jit\n",
    "def _jaccard_coef(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return _try_divide(float(len(A.intersection(B))), len(A.union(B)))\n",
    "\n",
    "@jit\n",
    "def _dice_dist(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return _try_divide(2.*float(len(A.intersection(B))), (len(A) + len(B)))\n",
    "\n",
    "@jit    \n",
    "def entropy(obs, token_pattern=' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    counter = Counter(obs_tokens)\n",
    "    count = np.asarray(list(counter.values()))\n",
    "    proba = count/np.sum(count)\n",
    "    # del obs_tokens\n",
    "    return _entropy(proba)\n",
    "        \n",
    "def digit_count(obs):\n",
    "    return len(re.findall(r\"\\d\", obs))\n",
    "\n",
    "def digit_ratio(obs, token_pattern = ' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    return _try_divide(len(re.findall(r\"\\d\", obs)), len(obs_tokens))\n",
    "\n",
    "def emoji_count(obs):\n",
    "    return len(re.findall(r'[^\\w\\s,]', obs))\n",
    "\n",
    "def emoji_ratio(obs, token_pattern = ' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    return _try_divide(len(re.findall(r'[^\\w\\s,]', obs)), len(obs_tokens))\n",
    "\n",
    "@jit\n",
    "def _unigrams(words):\n",
    "    \"\"\"\n",
    "    \tInput: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "    \tOutput: a list of unigram\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "\n",
    "@jit\n",
    "def _bigrams(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "       Output: a list of bigram, e.g., [\"I_am\", \"am_Denny\"]\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "    \tlst = []\n",
    "    \tfor i in range(L-1):\n",
    "    \t\tfor k in range(1,skip+2):\n",
    "    \t\t\tif i+k < L:\n",
    "    \t\t\t\tlst.append( join_string.join([words[i], words[i+k]]) )\n",
    "    else:\n",
    "    \t# set it as unigram\n",
    "    \tlst = _unigrams(words)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def _trigrams(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "       Output: a list of trigram, e.g., [\"I_am_Denny\"]\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "    \tlst = []\n",
    "    \tfor i in range(L-2):\n",
    "    \t\tfor k1 in range(1,skip+2):\n",
    "    \t\t\tfor k2 in range(1,skip+2):\n",
    "    \t\t\t\tif i+k1 < L and i+k1+k2 < L:\n",
    "    \t\t\t\t\tlst.append( join_string.join([words[i], words[i+k1], words[i+k1+k2]]) )\n",
    "    else:\n",
    "    \t# set it as bigram\n",
    "    \tlst = _bigrams(words, join_string, skip)\n",
    "    return lst\n",
    "\n",
    "def UniqueCount_Ngram(obs, count, token_pattern=' '):\n",
    "    obs_tokens = obs.lower().split(token_pattern)\n",
    "    obs_ngrams = _ngrams(obs_tokens, count)\n",
    "    l = len(set(obs_ngrams))\n",
    "    del obs_tokens\n",
    "    del obs_ngrams\n",
    "    return l\n",
    "\n",
    "def UniqueRatio_Ngram(obs, count, token_pattern=' '):\n",
    "    obs_tokens = obs.lower().split(token_pattern)\n",
    "    obs_ngrams = _ngrams(obs_tokens, count)\n",
    "    r = _try_divide(len(set(obs_ngrams)), len(obs_ngrams))\n",
    "    del obs_tokens\n",
    "    del obs_ngrams\n",
    "    return r\n",
    "\n",
    "def _ngrams(words, ngram, join_string=\" \"):\n",
    "    \"\"\"wrapper for ngram\"\"\"\n",
    "    if ngram == 1:\n",
    "    \treturn _unigrams(words)\n",
    "    elif ngram == 2:\n",
    "    \treturn _bigrams(words, join_string)\n",
    "    elif ngram == 3:\n",
    "    \treturn _trigrams(words, join_string)\n",
    "    elif ngram == 4:\n",
    "    \treturn _fourgrams(words, join_string)\n",
    "    elif ngram == 12:\n",
    "    \tunigram = _unigrams(words)\n",
    "    \tbigram = [x for x in _bigrams(words, join_string) if len(x.split(join_string)) == 2]\n",
    "    \treturn unigram + bigram\n",
    "    elif ngram == 123:\n",
    "    \tunigram = _unigrams(words)\n",
    "    \tbigram = [x for x in _bigrams(words, join_string) if len(x.split(join_string)) == 2]\n",
    "    \ttrigram = [x for x in _trigrams(words, join_string) if len(x.split(join_string)) == 3]\n",
    "    \treturn unigram + bigram + trigram\n",
    "    \t\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, NUM_PARTITIONS)\n",
    "    pool = Pool(NUM_CORES)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def get_sentiment_score(df):\n",
    "    df['sentiment_score'] = df['item_description'].map(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "start_time = time.time()\n",
    "\n",
    "train = pd.read_table('../input/train.tsv', engine='c')\n",
    "test = pd.read_table('../input/test.tsv', engine='c')\n",
    "print('[{}] Finished to load data'.format(time.time() - start_time))\n",
    "print('Train shape: ', train.shape)\n",
    "print('Test shape: ', test.shape)\n",
    "\n",
    "nrow_test = test.shape[0]\n",
    "\n",
    "test_id = test['test_id'].values\n",
    "submission: pd.DataFrame = test[['test_id']]\n",
    "\n",
    "if nrow_test < 700000:\n",
    "    test = pd.concat([test,test,test,test,test])\n",
    "    print('Test shape ', test.shape)\n",
    "\n",
    "\n",
    "nrow_train = train.shape[0]\n",
    "y = np.log1p(train[\"price\"])\n",
    "del train['price']\n",
    "merge: pd.DataFrame = pd.concat([train, test])\n",
    "\n",
    "train_cols = set(train.columns)\n",
    "del train\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "handle_missing_inplace(merge)\n",
    "print('[{}] Handle missing completed.'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_doclen_name(df):\n",
    "#     df['name_doclen'] = df['name'].map(lambda x: len(str(x).lower().split(' ')))\n",
    "#     return df\n",
    "\n",
    "# def get_doclen_itemdesc(df):\n",
    "#     df['item_description_doclen'] = df['item_description'].map(lambda x: len(str(x).lower().split(' ')))\n",
    "#     return df\n",
    "\n",
    "# def get_doclen_brand_name(df):\n",
    "#     df['brand_name_doclen'] = df['brand_name'].map(lambda x: len(str(x).lower().split(' ')))\n",
    "#     return df\n",
    "\n",
    "# def get_entropy_name(df):\n",
    "#     df['name_entropy'] = df['name'].map(lambda x: entropy(str(x).lower(), ' '))\n",
    "#     return df\n",
    "\n",
    "# def get_entropy_itemdesc(df):\n",
    "#     df['item_description_entropy'] = \\\n",
    "#     \tdf['item_description'].map(lambda x: entropy(str(x).lower(), ' '))\n",
    "#     return df\n",
    "\n",
    "# def get_entropy_brand_name(df):\n",
    "#     df['brand_name_entropy'] = \\\n",
    "#     \tdf['brand_name'].map(lambda x: entropy(str(x).lower(), ' '))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_count_name(df):\n",
    "#     df['name_dc'] = df['name'].map(lambda x: digit_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_count_itemdesc(df):\n",
    "#     df['item_description_dc'] = \\\n",
    "#     \tdf['item_description'].map(lambda x: digit_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_count_brand_name(df):\n",
    "#     df['brand_name_dc'] = \\\n",
    "#     \tdf['brand_name'].map(lambda x: digit_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_ratio_name(df):\n",
    "#     df['name_dr'] = df['name'].map(lambda x: digit_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_ratio_itemdesc(df):\n",
    "#     df['item_description_dr'] = \\\n",
    "#     \tdf['item_description'].map(lambda x: digit_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_ratio_brand_name(df):\n",
    "#     df['brand_name_dr'] = \\\n",
    "#     \tdf['brand_name'].map(lambda x: digit_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_count_name(df):\n",
    "#     df['name_ec'] = df['name'].map(lambda x: emoji_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_count_itemdesc(df):\n",
    "#     df['item_description_ec'] = \\\n",
    "#     \tdf['item_description'].map(lambda x: emoji_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_count_brand_name(df):\n",
    "#     df['brand_name_ec'] = \\\n",
    "#     \tdf['brand_name'].map(lambda x: emoji_count(str(x).lower()))\n",
    "#     return df\n",
    "        \n",
    "# def get_emoji_ratio_name(df):\n",
    "#     df['name_er'] = df['name'].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_ratio_itemdesc(df):\n",
    "#     df['item_description_er'] = \\\n",
    "#     \tdf['item_description'].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_ratio_brand_name(df):\n",
    "#     df['brand_name_er'] = \\\n",
    "#     \tdf['brand_name'].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# cols1 = set(merge.columns)\n",
    "# cols = []\n",
    "# obs_fields = ['name', 'brand_name', 'item_description']\n",
    "# merge = parallelize_dataframe(merge, get_doclen_name)\n",
    "# merge = parallelize_dataframe(merge, get_doclen_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_doclen_brand_name)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_entropy_name)\n",
    "# merge = parallelize_dataframe(merge, get_entropy_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_entropy_brand_name)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_digit_count_name)\n",
    "# merge = parallelize_dataframe(merge, get_digit_count_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_digit_count_brand_name)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_digit_ratio_name)\n",
    "# merge = parallelize_dataframe(merge, get_digit_ratio_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_digit_ratio_brand_name)\n",
    "\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_count_name)\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_count_itemdesc)\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_count_brand_name)\n",
    "\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_ratio_name)\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_ratio_itemdesc)\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_ratio_brand_name)\n",
    "\n",
    "# print('[{}] Finished basic creation for name, bn, item_desc'.format(time.time() - start_time))\n",
    "\n",
    "# for f in obs_fields:\n",
    "#     counter = Counter(merge[f].values)\n",
    "#     merge[f+'_docfreq'] = merge[f].map(lambda x: counter[x])\n",
    "    \n",
    "#     cols.append(f+'_doclen')\n",
    "#     cols.append(f+'_docfreq')\n",
    "#     cols.append(f+'_docEntropy')\n",
    "#     cols.append(f+'_digitCount')\n",
    "#     cols.append(f+'_digitRatio')\n",
    "#     # cols.append(f+'_emojiCount')\n",
    "#     # cols.append(f+'_emojiRatio')\n",
    "\n",
    "# f = 'category_name'\n",
    "# def get_category_name_doclen(df):\n",
    "#     df[f+'_doclen'] = df[f].map(lambda x: len(str(x).lower().split('/')))\n",
    "#     return df\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_category_name_doclen)\n",
    "\n",
    "# counter = Counter(merge[f].values)\n",
    "# merge[f+'_docfreq'] = merge[f].map(lambda x: counter[x])\n",
    "\n",
    "# token_pattern = '/'\n",
    "\n",
    "# def get_category_name_entropy(df):\n",
    "# \tdf[f+'_docEntropy'] = df[f].map(lambda x: entropy(str(x).lower(),token_pattern))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_category_name_entropy)\n",
    "\n",
    "# def get_category_name_dc(df):\n",
    "# \tdf[f+'_dc'] = df[f].map(lambda x: digit_count(str(x).lower()))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_category_name_dc)\n",
    "\n",
    "# def get_category_name_dr(df):\n",
    "# \tdf[f+'_dr'] = df[f].map(lambda x: digit_ratio(str(x).lower(), token_pattern))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_category_name_dr)\n",
    "\n",
    "# def get_category_name_ec(df):\n",
    "# \tdf[f+'_emojiCount'] = df[f].map(lambda x: emoji_count(str(x).lower()))\n",
    "# \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_category_name_ec)\n",
    "\n",
    "# def get_category_name_er(df):\n",
    "# \tdf[f+'_emojiRatio'] = df[f].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "# \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_category_name_er)\n",
    "\n",
    "# cols.append(f+'_doclen')\n",
    "# cols.append(f+'_docfreq')\n",
    "# cols.append(f+'_docEntropy')\n",
    "# cols.append(f+'_digitCount')\n",
    "# cols.append(f+'_digitRatio')\n",
    "# # cols.append(f+'_emojiCount')\n",
    "# # cols.append(f+'_emojiRatio')\n",
    "\n",
    "# print('[{}] Finished basic creation for category_name'.format(time.time() - start_time))\n",
    "\n",
    "# obs_fields = [\"name\", \"item_description\"]\n",
    "\n",
    "# # def get_onegram_uc_name(df):\n",
    "# # \tdf['name_1_uc'] = df['name'].map(lambda x: UniqueCount_Ngram(str(x), 1))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_onegram_uc_name)\n",
    "\n",
    "# # def get_onegram_uc_item_desc(df):\n",
    "# # \tdf['item_desc_1_uc'] = \\\n",
    "# # \t\tdf['item_description'].map(lambda x: UniqueCount_Ngram(str(x), 1))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_onegram_uc_item_desc)\n",
    "\n",
    "# # def get_onegram_ur_name(df):\n",
    "# # \tdf['name_1_ur'] = df['name'].map(lambda x: UniqueRatio_Ngram(str(x), 1))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_onegram_ur_name)\n",
    "\n",
    "# # def get_onegram_ur_item_desc(df):\n",
    "# # \tdf['item_desc_1_ur'] = \\\n",
    "# # \t\tdf['item_description'].map(lambda x: UniqueRatio_Ngram(str(x), 1))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_onegram_ur_item_desc)\n",
    "\n",
    "# def get_bigram_uc_name(df):\n",
    "# \tdf['name_2_uc'] = df['name'].map(lambda x: UniqueCount_Ngram(str(x), 2))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_bigram_uc_name)\n",
    "\n",
    "# def get_bigram_uc_item_desc(df):\n",
    "# \tdf['item_desc_2_uc'] = \\\n",
    "# \t\tdf['item_description'].map(lambda x: UniqueCount_Ngram(str(x), 2))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_bigram_uc_item_desc)\n",
    "\n",
    "# def get_bigram_ur_name(df):\n",
    "# \tdf['name_2_ur'] = df['name'].map(lambda x: UniqueRatio_Ngram(str(x), 2))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_bigram_ur_name)\n",
    "\n",
    "# def get_bigram_ur_item_desc(df):\n",
    "# \tdf['item_desc_2_ur'] = \\\n",
    "# \t\tdf['item_description'].map(lambda x: UniqueRatio_Ngram(str(x), 2))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_bigram_ur_item_desc)\n",
    "\n",
    "# # def get_trigram_uc_name(df):\n",
    "# # \tdf['name_3_uc'] = df['name'].map(lambda x: UniqueCount_Ngram(str(x), 3))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_trigram_uc_name)\n",
    "\n",
    "# # def get_trigram_uc_item_desc(df):\n",
    "# # \tdf['item_desc_3_uc'] = \\\n",
    "# # \t\tdf['item_description'].map(lambda x: UniqueCount_Ngram(str(x), 3))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_trigram_uc_item_desc)\n",
    "\n",
    "# # def get_trigram_ur_name(df):\n",
    "# # \tdf['name_3_ur'] = df['name'].map(lambda x: UniqueRatio_Ngram(str(x), 3))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_trigram_ur_name)\n",
    "\n",
    "# # def get_trigram_ur_item_desc(df):\n",
    "# # \tdf['item_desc_3_ur'] = \\\n",
    "# # \t\tdf['item_description'].map(lambda x: UniqueRatio_Ngram(str(x), 3))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_trigram_ur_item_desc)\n",
    "\n",
    "# # print('[{}] Finished ngram count for name, item_desc'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# # ngrams = [1,2,3]\n",
    "# # token_pattern =' '\n",
    "# # for f in obs_fields:\n",
    "# # \tfor n in ngrams:\n",
    "# # \t\tcols.append(f+'_{}_uc'.format(n))\n",
    "# # \t\tcols.append(f+'_{}_ur'.format(n))\n",
    "\n",
    "# # f = 'category_name'\n",
    "# # merge[f+'_{}_uc'.format(n)] = merge[f].map(lambda x: UniqueCount_Ngram(str(x), n, '/'))\n",
    "# # merge[f+'_{}_ur'.format(n)] = merge[f].map(lambda x: UniqueRatio_Ngram(str(x), n, '/'))\n",
    "# # cols.append(f+'_{}_uc'.format(n))\n",
    "# # cols.append(f+'_{}_ur'.format(n))\n",
    "\t\t\n",
    "# # remove constatnt cols\n",
    "# merge =  merge.loc[:, (merge != merge.iloc[0]).any()]\n",
    "# print(len(cols))\n",
    "# del cols\n",
    "# cols = list(set(merge.columns) - cols1)\n",
    "# print(len(cols))\n",
    "\n",
    "# X_b = merge[cols]\n",
    "\n",
    "# print('[{}] Finished X_basic1'.format(time.time() - start_time))\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# X_b = scaler.fit_transform(X_b)\n",
    "# X_basic = csr_matrix(X_b)\n",
    "# print('basic: ', X_basic.data.nbytes)\n",
    "# print('[{}] Finished X_basic2'.format(time.time() - start_time))\n",
    "# del X_b\n",
    "# for c in cols:\n",
    "#     merge = merge.drop(c, axis=1)\n",
    "# print_memory_usage()\n",
    "\n",
    "# # jaccard and dice\n",
    "# merge['n_id'] = merge['name'].astype('str') + '___' + merge['item_description'].astype('str')\n",
    "\n",
    "# from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# def jaccard_skl(text):\n",
    "#     obs, target = text.split('___')\n",
    "#     obs_tokens = obs.split(' ')\n",
    "#     target_tokens = target.split(' ')\n",
    "#     j1 = 1 - pairwise_distances(obs_tokens, target_tokens, metric = \"hamming\")\n",
    "#     del obs, target, target_tokens, obs_tokens\n",
    "#     return j1\n",
    "\n",
    "# @jit    \n",
    "# def jaccard_1(text):\n",
    "#     obs, target = text.split('___')\n",
    "#     obs_tokens = obs.split(' ')\n",
    "#     target_tokens = target.split(' ')\n",
    "#     obs_ngrams = _ngrams(obs_tokens, 1)\n",
    "#     target_ngrams = _ngrams(target_tokens, 1)\n",
    "#     j1 = _jaccard_coef(obs_ngrams, target_ngrams)\n",
    "#     # del obs, target, target_tokens, obs_tokens, obs_ngrams, target_ngrams\n",
    "#     return j1\n",
    "    \n",
    "# def jaccard_2(text):\n",
    "#     obs, target = text.split('___')\n",
    "#     obs_tokens = obs.split(' ')\n",
    "#     target_tokens = target.split(' ')\n",
    "#     obs_ngrams = _ngrams(obs_tokens, 2)\n",
    "#     target_ngrams = _ngrams(target_tokens, 2)\n",
    "#     j2 = _jaccard_coef(obs_ngrams, target_ngrams)\n",
    "#     del obs, target, target_tokens, obs_tokens, obs_ngrams, target_ngrams\n",
    "#     return j2\n",
    "\n",
    "# def jaccard_3(text):\n",
    "#     obs, target = text.split('___')\n",
    "#     obs_tokens = obs.split(' ')\n",
    "#     target_tokens = target.split(' ')\n",
    "#     obs_ngrams = _ngrams(obs_tokens, 3)\n",
    "#     target_ngrams = _ngrams(target_tokens, 3)\n",
    "#     j3 = _jaccard_coef(obs_ngrams, target_ngrams)\n",
    "#     del obs, target, target_tokens, obs_tokens, obs_ngrams, target_ngrams\n",
    "#     return j3\n",
    "\n",
    "# @jit\n",
    "# def jaccard_minhash(text):\n",
    "#     obs, target = text.split('___')\n",
    "#     obs_tokens = obs.split(' ')\n",
    "#     target_tokens = target.split(' ')\n",
    "#     m1, m2 = MinHash(), MinHash()\n",
    "#     for d in obs_tokens:\n",
    "#         m1.update(d.encode('utf8'))\n",
    "#     for d in target_tokens:\n",
    "#         m2.update(d.encode('utf8'))\n",
    "#     j = m1.jaccard(m2)\n",
    "#     return j\n",
    "\n",
    "# # def get_j2(df):\n",
    "#     # merge['j2'] = merge['n_id'].map(lambda x: jaccard_2(x))\n",
    "#     # return df\n",
    "# # merge = parallelize_dataframe(merge, get_j2)\n",
    "\n",
    "# def get_j1(df):\n",
    "#     merge['j1'] = merge['n_id'].map(lambda x: jaccard_1(x))\n",
    "#     return df\n",
    "# merge = parallelize_dataframe(merge, get_j1)\n",
    "\n",
    "# X_j = merge[['j1']]\n",
    "# del merge['n_id']\n",
    "# np.min(X_j)\n",
    "# np.max(X_j)\n",
    "# print('[{}] Finished X_j'.format(time.time() - start_time))\n",
    "# print_memory_usage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abbr = {}\n",
    "abbr['BNWT'] = ['bnwt', 'brand new with tags']\n",
    "abbr['NWT'] = ['nwt', 'new with tags']\n",
    "abbr['BNWOT'] = ['bnwot', 'brand new with out tags', 'brand new without tags']\n",
    "abbr['NWOT'] = ['nwot', 'new with out tags', 'new without tags']\n",
    "abbr['BNIP'] = ['bnip', 'brand new in packet', 'brand new in packet']\n",
    "abbr['NIP'] = ['nip', 'new in packet', 'new in packet']\n",
    "abbr['BNIB'] = ['bnib', 'brand new in box']\n",
    "abbr['NIB'] = ['nib', 'new in box']\n",
    "abbr['MIB'] = ['mib', 'mint in box']\n",
    "abbr['MWOB'] = ['mwob', 'mint with out box', 'mint without box']\n",
    "abbr['MIP'] = ['mip', 'mint in packet']\n",
    "abbr['MWOP'] = ['mwop', 'mint with out packet', 'mint without packet']\n",
    "\n",
    "merge['tag'] = merge['item_description'].map(lambda a: 'BNWT' if any(x in a.lower() for x in abbr['BNWT'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'NWT' if any(x in a.lower() for x in abbr['NWT'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'BNWOT' if any(x in a.lower() for x in abbr['BNWOT'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'NWOT' if any(x in a.lower() for x in abbr['NWOT'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'BNIP' if any(x in a.lower() for x in abbr['BNIP'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'NIP' if any(x in a.lower() for x in abbr['NIP'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'BNIB' if any(x in a.lower() for x in abbr['BNIB'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'NIB' if any(x in a.lower() for x in abbr['NIB'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'MIB' if any(x in a.lower() for x in abbr['MIB'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'MWOB' if any(x in a.lower() for x in abbr['MWOB'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'MIP' if any(x in a.lower() for x in abbr['MIP'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'MWOP' if any(x in a.lower() for x in abbr['MWOP'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'None')\n",
    "print('[{}] Finished tag'.format(time.time() - start_time))\n",
    "del abbr\n",
    "print_memory_usage()\n",
    "\n",
    "merge['bci'] = merge['brand_name'].astype('str') + ' ' + merge['category_name'].astype('str') + ' ' + \\\n",
    "\t\t\tmerge['item_condition_id'].astype('str')\n",
    "\n",
    "merge['bc'] = merge['brand_name'].astype('str') + ' ' + merge['category_name'].astype('str')\n",
    "\n",
    "merge['bcis'] = merge['brand_name'].astype('str') + ' ' \\\n",
    "\t\t\t\t+ merge['category_name'].astype('str') + ' ' + \\\n",
    "\t\t\t\tmerge['item_condition_id'].astype('str') + ' ' + \\\n",
    "\t\t\t\tmerge['shipping'].astype('str')\n",
    "\n",
    "merge['bcs'] = merge['brand_name'].astype('str') + ' ' + \\\n",
    "\t\t\t\tmerge['category_name'].astype('str') + ' ' + \\\n",
    "\t\t\t\tmerge['shipping'].astype('str')\n",
    "\n",
    "# merge['bi'] = merge['brand_name'].astype('str') + '_' +   merge['item_condition_id'].astype('str')\n",
    "\t\n",
    "# merge['ci'] = merge['category_name'].astype('str') + '_' + merge['item_condition_id'].astype('str')\n",
    "\n",
    "print('[{}] Finished creating bci bc bi ci bcs bcis'.format(time.time() - start_time))\n",
    "print_memory_usage()\n",
    "\n",
    "\n",
    "# merge.drop(['bci', 'bc'], axis=1, inplace=True)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_sentiment_score)\n",
    "# merge['sentiment_score'] = merge['item_description'].map(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# print('[{}] Finished sentiment score'.format(time.time() - start_time))\n",
    "# a = merge['sentiment_score'].values\n",
    "# print(np.min(a))\n",
    "# print(np.max(a))\n",
    "\n",
    "# print_memory_usage()\n",
    "# merge['sentiment'] = merge['sentiment_score'].map(lambda x: 'VPos' if x > 0.5 \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# else 'Pos' if (x <= 0.5) and (x > 0)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# else 'Neu' if  x == 0 \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# else 'Neg' if (x < 0) and (x >= -0.5)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# else 'VNeg')\n",
    "\n",
    "# print('[{}] Finished sentiment'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutting(merge)\n",
    "print('[{}] Finished to cut'.format(time.time() - start_time))\n",
    "\n",
    "to_categorical(merge)\n",
    "print('[{}] Finished to convert categorical'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # we need a custom pre-processor to extract correct field,\n",
    "# # but want to also use default scikit-learn preprocessing (e.g. lowercasing)\n",
    "# default_preprocessor = CountVectorizer().build_preprocessor()\n",
    "# def build_preprocessor(field):\n",
    "#     field_idx = list(merge.columns).index(field)\n",
    "#     return lambda x: default_preprocessor(x[field_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(merge.columns).index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tv = TfidfVectorizer(max_features=2 ** 14,\n",
    "#                      min_df=NAME_MIN_DF,\n",
    "# \t\t\t\t\t ngram_range=(1, 3),\n",
    "# \t\t\t\t\t stop_words='english')\n",
    "# X_name1 = tv.fit_transform(merge['name'])\n",
    "# print('[{}] Finished TFIDF vectorize `name`'.format(time.time() - start_time))\n",
    "# print(X_name1.shape)\n",
    "# print(np.min(X_name1))\n",
    "# print(np.max(X_name1))\n",
    "# # del merge['item_description']\n",
    "# print_memory_usage()\n",
    "\n",
    "cv = CountVectorizer(min_df=NAME_MIN_DF, stop_words='english')\n",
    "X_name = cv.fit_transform(merge['name'])\n",
    "norm = Normalizer()\n",
    "X_name = norm.fit_transform(X_name)\n",
    "print('[{}] Finished count vectorize `name`'.format(time.time() - start_time))\n",
    "print(X_name.shape)\n",
    "print(np.min(X_name))\n",
    "print(np.max(X_name))\n",
    "del merge['name']\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_category = cv.fit_transform(merge['category_name'])\n",
    "norm = Normalizer()\n",
    "X_category = norm.fit_transform(X_category)\n",
    "print('[{}] Finished count vectorize `category_name`'.format(time.time() - start_time))\n",
    "print(X_category.shape)\n",
    "print(np.min(X_category))\n",
    "print(np.max(X_category))\n",
    "del merge['category_name']\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "# cv = CountVectorizer()\n",
    "# X_bci_cv = cv.fit_transform(merge['bci'])\n",
    "# norm = Normalizer()\n",
    "# X_bci_cv = norm.fit_transform(X_bci_cv)\n",
    "# print('[{}] Finished count vectorize `X_bci_cv`'.format(time.time() - start_time))\n",
    "# print(X_bci_cv.shape)\n",
    "# print(np.min(X_bci_cv))\n",
    "# print(np.max(X_bci_cv))\n",
    "# del merge['bci']\n",
    "# gc.collect()\n",
    "# print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION,\n",
    "\t\t\t\t\t ngram_range=(1, 3),\n",
    "\t\t\t\t\t stop_words='english')\n",
    "X_description = tv.fit_transform(merge['item_description'])\n",
    "print('[{}] Finished TFIDF vectorize `item_description`'.format(time.time() - start_time))\n",
    "print(X_description.shape)\n",
    "print(np.min(X_description))\n",
    "print(np.max(X_description))\n",
    "del merge['item_description']\n",
    "print_memory_usage()\n",
    "\n",
    "# X_cos = cosine_similarity(X_description, dense_output=False)\n",
    "# X_cos = squareform(pdist(np.asarray(X_description.toarray()), 'cosine'))\n",
    "# print(X_cos.shape)\n",
    "# print('[{}] Finished cosine similarity'.format(time.time() - start_time))\n",
    "# print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb.fit_transform(merge['brand_name'])\n",
    "print('[{}] Finished label binarize `brand_name`'.format(time.time() - start_time))\n",
    "print(X_brand.shape)\n",
    "del merge['brand_name']\n",
    "print_memory_usage()\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_bci = lb.fit_transform(merge['bci'])\n",
    "print('[{}] Finished label binarize `bci`'.format(time.time() - start_time))\n",
    "print(X_bci.shape)\n",
    "del merge['bci']\n",
    "print_memory_usage()\n",
    "\n",
    "# lb = LabelBinarizer(sparse_output=True)\n",
    "# X_bc = lb.fit_transform(merge['bc'])\n",
    "# print('[{}] Finished label binarize `bc`'.format(time.time() - start_time))\n",
    "# print(X_bc.shape)\n",
    "# del merge['bc']\n",
    "# print_memory_usage()\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_bcis = lb.fit_transform(merge['bcis'])\n",
    "print('[{}] Finished label binarize `bcis`'.format(time.time() - start_time))\n",
    "print(X_bcis.shape)\n",
    "del merge['bcis']\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_bcs = lb.fit_transform(merge['bcs'])\n",
    "print('[{}] Finished label binarize `bcs`'.format(time.time() - start_time))\n",
    "print(X_bcs.shape)\n",
    "del merge['bcs']\n",
    "gc.collect()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping', \n",
    "\t\t\t\t\t\t\t\t\t\t\t'tag']], sparse=True).values)\n",
    "print('[{}] Finished to get dummies on `item_condition_id` and `shipping`'.format(time.time() - start_time))\n",
    "print(X_dummies.shape)\n",
    "print_memory_usage()\n",
    "\n",
    "del merge\n",
    "gc.collect()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('basic: ', X_basic.data.nbytes)\n",
    "print('bcis: ', X_bcis.data.nbytes)\n",
    "print('bci: ', X_bci.data.nbytes)\n",
    "print('dummies: ', X_dummies.data.nbytes)\n",
    "print('description: ', X_description.data.nbytes)\n",
    "print('brand: ', X_brand.data.nbytes)\n",
    "print('category: ', X_category.data.nbytes)\n",
    "print('name: ', X_name.data.nbytes)\n",
    "# print('name1: ', X_name1.data.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_merge = hstack((X_bci, X_bcis, X_dummies, X_description, X_brand, X_category, X_name)).tocsr()\n",
    "print('[{}] Finished to create sparse merge'.format(time.time() - start_time))\n",
    "\n",
    "del X_bcis, X_bci, X_bcs, X_dummies, X_description, X_brand, X_category, X_name\n",
    "gc.collect()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = sparse_merge[:nrow_train]\n",
    "X_test = sparse_merge[nrow_train:]\n",
    "\n",
    "print(X.shape)\n",
    "print_memory_usage()\n",
    "\n",
    "del sparse_merge\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "np.random.seed(0)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.01, random_state = 0) \n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def batch_generator(X, y, batch_size):\n",
    "#     number_of_batches = samples_per_epoch/batch_size\n",
    "#     counter=0\n",
    "#     shuffle_index = np.arange(np.shape(y)[0])\n",
    "#     np.random.shuffle(shuffle_index)\n",
    "#     X =  X[shuffle_index, :]\n",
    "#     y =  y[shuffle_index]\n",
    "#     while 1:\n",
    "#         index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "#         X_batch = X[index_batch,:].todense()\n",
    "#         y_batch = y[index_batch]\n",
    "#         counter += 1\n",
    "#         yield(np.array(X_batch),y_batch)\n",
    "#         if (counter < number_of_batches):\n",
    "#             np.random.shuffle(shuffle_index)\n",
    "#             counter=0\n",
    "\n",
    "# from keras.datasets import reuters\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Activation\n",
    "# from keras.utils import np_utils\n",
    "\n",
    "# epochs=3\n",
    "# BATCH_SIZE = 512 * 4\n",
    "# def rmsle_cust(y_true, y_pred):\n",
    "#     first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "#     second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "#     return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n",
    "\n",
    "# def eval_model(model):\n",
    "#     val_preds = model.predict_generator(generator=batch_generator_x(valid_X, BATCH_SIZE),\n",
    "#                                steps=valid_X['name'].shape[0]//BATCH_SIZE + 1,\n",
    "#                               )\n",
    "                              \n",
    "#     val_preds = np.expm1(val_preds)\n",
    "    \n",
    "#     y_true = np.array(valid_y)\n",
    "#     y_pred = val_preds[:, 0]\n",
    "#     v_rmsle = rmsle(y_true, y_pred)\n",
    "#     print(\" RMSLE error on dev test: \"+str(v_rmsle))\n",
    "#     return v_rmsle\n",
    "    \n",
    "# print('Building model...')\n",
    "# model = Sequential()\n",
    "# model.add(Dense(512))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(64))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error', optimizer=\"adam\")\n",
    "\n",
    "# model.fit_generator(generator=batch_generator(train_X, train_y, BATCH_SIZE),\n",
    "#                     nb_epoch=epochs, \n",
    "#                     samples_per_epoch=train_X['name'].shape[0]//BATCH_SIZE + 1)\n",
    "                    \n",
    "# rmsle = eval_model(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# d_train = lgb.Dataset(X, label=y, max_bin=8192)\n",
    "d_train = lgb.Dataset(train_X, label=train_y, max_bin=8192)\n",
    "d_valid = lgb.Dataset(valid_X, label=valid_y, max_bin=8192)\n",
    "watchlist = [d_train, d_valid]\n",
    "print_memory_usage()\n",
    "\n",
    "params = {\n",
    "\t'learning_rate': 0.75,\n",
    "\t'application': 'regression',\n",
    "\t'max_depth': 3,\n",
    "\t'num_leaves': 100,\n",
    "\t'verbosity': -1,\n",
    "\t'metric': 'RMSE',\n",
    "\t'num_threads': 4\n",
    "}\n",
    "\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, valid_sets=watchlist,\n",
    "\t\t\t\t\tnum_boost_round=5000,early_stopping_rounds=100,verbose_eval=500) \n",
    "print('[{}] Finished to train lgbm'.format(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "show_weights() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8e4f739a6653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meli5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: show_weights() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import eli5\n",
    "eli5.show_weights(model, d_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "print('[{}] Finished to train predict lgbm'.format(time.time() - start_time))\n",
    "del model, d_train, d_valid\n",
    "print_memory_usage()\n",
    "\n",
    "# submission=pd.DataFrame()\n",
    "# submission['test_id'] = test_id\n",
    "# submission['price'] = np.expm1(preds)\n",
    "# submission.to_csv(\"submission_lgbm_nlp2.csv\", index=False)\n",
    "preds *= 0.6\n",
    "# print('[{}] Finished submission lgbm'.format(time.time() - start_time))\n",
    "if nrow_test < 700000:\n",
    "\tpreds = preds[:nrow_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Ridge(solver=\"saga\", fit_intercept=True, random_state=205)\n",
    "model.fit(X, y)\n",
    "print('[{}] Finished to train ridge'.format(time.time() - start_time))\n",
    "preds1 = model.predict(X=X_test)\n",
    "print('[{}] Finished to predict ridge'.format(time.time() - start_time))\n",
    "# submission['price'] = np.expm1(preds1)\n",
    "# submission.to_csv(\"submission_ridge_nlp2.csv\", index=False)\n",
    "print_memory_usage()\n",
    "if nrow_test < 700000:\n",
    "\tpreds1 = preds1[:nrow_test]\n",
    "\t\n",
    "preds += 0.4*preds1\n",
    "submission['price'] = np.expm1(preds)\n",
    "submission.to_csv(\"../cache/submission_lgbm_ridge_nlp2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
