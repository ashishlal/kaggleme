{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1481661, 9)\n",
      "5 folds scaling the test_df\n",
      "(693359, 7)\n",
      "new shape  (3493359, 7)\n",
      "[27.481009244918823] Finished scaling test set...\n",
      "Handling missing values...\n",
      "(1481661, 9)\n",
      "(3493359, 7)\n",
      "[28.545302152633667] Finished handling missing data...\n",
      "Handling categorical variables...\n",
      "[66.51027536392212] Finished PROCESSING CATEGORICAL DATA...\n",
      "Text to seq process...\n",
      "   Fitting tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Transforming text to seq...\n",
      "[339.4072787761688] Finished PROCESSING TEXT DATA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda2/envs/kme-cpu/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1466844, 13)\n",
      "(14817, 13)\n",
      "[344.94825983047485] Finished EMBEDDINGS MAX VALUE...\n",
      "[438.05800676345825] Finished DATA PREPARARTION...\n",
      "[438.0589849948883] Finished DEFINEING MODEL...\n",
      "Train on 1452175 samples, validate on 14669 samples\n",
      "Epoch 1/3\n",
      "Epoch 2/3\n",
      "Epoch 3/3\n",
      "[2123.6134021282196] Finished FITTING MODEL...\n",
      " RMSLE error on dev test: 0.43528846581148545\n",
      "[2126.3701252937317] Finished predicting valid set...\n",
      "[2348.2904143333435] Finished predicting test set...\n",
      "[2355.4364523887634] Finished submission...\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# mainly forking from notebook\n",
    "# https://www.kaggle.com/johnfarrell/simple-rnn-with-keras-script\n",
    "\n",
    "# ADDED\n",
    "# 5x scaled test set\n",
    "# category name embedding\n",
    "# some small changes like lr, decay, batch_size~\n",
    "\n",
    "# In[ ]:\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "test = pd.read_csv('../input/test.tsv', sep='\\t')\n",
    "\n",
    "train = train[train.price != 0]\n",
    "train['target'] = np.log1p(train['price'])\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print('5 folds scaling the test_df')\n",
    "print(test.shape)\n",
    "test_len = test.shape[0]\n",
    "def simulate_test(test):\n",
    "    if test.shape[0] < 800000:\n",
    "        indices = np.random.choice(test.index.values, 2800000)\n",
    "        test_ = pd.concat([test, test.iloc[indices]], axis=0)\n",
    "        return test_.copy()\n",
    "    else:\n",
    "        return test\n",
    "test = simulate_test(test)\n",
    "print('new shape ', test.shape)\n",
    "print('[{}] Finished scaling test set...'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#HANDLE MISSING VALUES\n",
    "print(\"Handling missing values...\")\n",
    "def handle_missing(dataset):\n",
    "    dataset.category_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.item_description.fillna(value=\"missing\", inplace=True)\n",
    "    return (dataset)\n",
    "\n",
    "train = handle_missing(train)\n",
    "test = handle_missing(test)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "print('[{}] Finished handling missing data...'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#PROCESS CATEGORICAL DATA\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "print(\"Handling categorical variables...\")\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(np.hstack([train.category_name, test.category_name]))\n",
    "train['category'] = le.transform(train.category_name)\n",
    "test['category'] = le.transform(test.category_name)\n",
    "\n",
    "le.fit(np.hstack([train.brand_name, test.brand_name]))\n",
    "train['brand'] = le.transform(train.brand_name)\n",
    "test['brand'] = le.transform(test.brand_name)\n",
    "del le, train['brand_name'], test['brand_name']\n",
    "\n",
    "print('[{}] Finished PROCESSING CATEGORICAL DATA...'.format(time.time() - start_time))\n",
    "train.head(3)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#PROCESS TEXT: RAW\n",
    "print(\"Text to seq process...\")\n",
    "print(\"   Fitting tokenizer...\")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "raw_text = np.hstack([train.category_name.str.lower(), \n",
    "                      train.item_description.str.lower(), \n",
    "                      train.name.str.lower()])\n",
    "\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "print(\"   Transforming text to seq...\")\n",
    "train[\"seq_category_name\"] = tok_raw.texts_to_sequences(train.category_name.str.lower())\n",
    "test[\"seq_category_name\"] = tok_raw.texts_to_sequences(test.category_name.str.lower())\n",
    "train[\"seq_item_description\"] = tok_raw.texts_to_sequences(train.item_description.str.lower())\n",
    "test[\"seq_item_description\"] = tok_raw.texts_to_sequences(test.item_description.str.lower())\n",
    "train[\"seq_name\"] = tok_raw.texts_to_sequences(train.name.str.lower())\n",
    "test[\"seq_name\"] = tok_raw.texts_to_sequences(test.name.str.lower())\n",
    "train.head(3)\n",
    "\n",
    "print('[{}] Finished PROCESSING TEXT DATA...'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#EXTRACT DEVELOPTMENT TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "dtrain, dvalid = train_test_split(train, random_state=233, train_size=0.99)\n",
    "print(dtrain.shape)\n",
    "print(dvalid.shape)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#EMBEDDINGS MAX VALUE\n",
    "#Base on the histograms, we select the next lengths\n",
    "MAX_NAME_SEQ = 20 #17\n",
    "MAX_ITEM_DESC_SEQ = 60 #269\n",
    "MAX_CATEGORY_NAME_SEQ = 20 #8\n",
    "MAX_TEXT = np.max([np.max(train.seq_name.max())\n",
    "                   , np.max(test.seq_name.max())\n",
    "                   , np.max(train.seq_category_name.max())\n",
    "                   , np.max(test.seq_category_name.max())\n",
    "                   , np.max(train.seq_item_description.max())\n",
    "                   , np.max(test.seq_item_description.max())])+2\n",
    "MAX_CATEGORY = np.max([train.category.max(), test.category.max()])+1\n",
    "MAX_BRAND = np.max([train.brand.max(), test.brand.max()])+1\n",
    "MAX_CONDITION = np.max([train.item_condition_id.max(), \n",
    "                        test.item_condition_id.max()])+1\n",
    "\n",
    "print('[{}] Finished EMBEDDINGS MAX VALUE...'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#KERAS DATA DEFINITION\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_keras_data(dataset):\n",
    "    X = {\n",
    "        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n",
    "        ,'item_desc': pad_sequences(dataset.seq_item_description\n",
    "                                    , maxlen=MAX_ITEM_DESC_SEQ)\n",
    "        ,'brand': np.array(dataset.brand)\n",
    "        ,'category': np.array(dataset.category)\n",
    "        ,'category_name': pad_sequences(dataset.seq_category_name\n",
    "                                        , maxlen=MAX_CATEGORY_NAME_SEQ)\n",
    "        ,'item_condition': np.array(dataset.item_condition_id)\n",
    "        ,'num_vars': np.array(dataset[[\"shipping\"]])\n",
    "    }\n",
    "    return X\n",
    "\n",
    "X_train = get_keras_data(dtrain)\n",
    "X_valid = get_keras_data(dvalid)\n",
    "X_test = get_keras_data(test)\n",
    "\n",
    "print('[{}] Finished DATA PREPARARTION...'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#KERAS MODEL DEFINITION\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, \\\n",
    "    Activation, concatenate, GRU, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    import math\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 \\\n",
    "              for i, pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n",
    "\n",
    "dr = 0.25\n",
    "\n",
    "def get_model():\n",
    "    #params\n",
    "    dr_r = dr\n",
    "    \n",
    "    #Inputs\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n",
    "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand = Input(shape=[1], name=\"brand\")\n",
    "    category = Input(shape=[1], name=\"category\")\n",
    "    category_name = Input(shape=[X_train[\"category_name\"].shape[1]], \n",
    "                          name=\"category_name\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    \n",
    "    #Embeddings layers\n",
    "    emb_size = 60\n",
    "    \n",
    "    emb_name = Embedding(MAX_TEXT, emb_size//3)(name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, emb_size)(item_desc)\n",
    "    emb_category_name = Embedding(MAX_TEXT, emb_size//3)(category_name)\n",
    "    emb_brand = Embedding(MAX_BRAND, 10)(brand)\n",
    "    emb_category = Embedding(MAX_CATEGORY, 10)(category)\n",
    "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "    \n",
    "    rnn_layer1 = GRU(24) (emb_item_desc)\n",
    "    rnn_layer2 = GRU(8) (emb_category_name)\n",
    "    rnn_layer3 = GRU(20) (emb_name)\n",
    "   \n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_brand)\n",
    "        , Flatten() (emb_category)\n",
    "        , Flatten() (emb_item_condition)\n",
    "        , rnn_layer1\n",
    "        , rnn_layer2\n",
    "        , rnn_layer3\n",
    "        , num_vars\n",
    "    ])\n",
    "    main_l = Dropout(0.1)(Dense(512,activation='relu') (main_l))\n",
    "    main_l = Dropout(0.1)(Dense(64,activation='relu') (main_l))\n",
    "    \n",
    "    # main_l = Dropout(0.1)(Dense(512,activation='relu') (main_l))\n",
    "    # main_l = Dropout(0.25)(Dense(256,activation='relu') (main_l))\n",
    "    # main_l = Dropout(0.5)(Dense(32,activation='relu') (main_l))\n",
    "    #output\n",
    "    output = Dense(1,activation=\"linear\") (main_l)\n",
    "    \n",
    "    #model\n",
    "    model = Model([name, item_desc, brand\n",
    "                   , category, category_name\n",
    "                   , item_condition, num_vars], output)\n",
    "    #optimizer = optimizers.RMSprop()\n",
    "    optimizer = optimizers.Adam()\n",
    "    model.compile(loss=\"mse\", \n",
    "                  optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def eval_model(model):\n",
    "    val_preds = model.predict(X_valid)\n",
    "    val_preds = np.expm1(val_preds)\n",
    "    \n",
    "    y_true = np.array(dvalid.price.values)\n",
    "    y_pred = val_preds[:, 0]\n",
    "    v_rmsle = rmsle(y_true, y_pred)\n",
    "    print(\" RMSLE error on dev test: \"+str(v_rmsle))\n",
    "    return v_rmsle\n",
    "#fin_lr=init_lr * (1/(1+decay))**(steps-1)\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "\n",
    "print('[{}] Finished DEFINEING MODEL...'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "gc.collect()\n",
    "#FITTING THE MODEL\n",
    "epochs = 3\n",
    "BATCH_SIZE = 512 * 5\n",
    "steps = int(len(X_train['name'])//BATCH_SIZE) * epochs\n",
    "lr_init, lr_fin = 0.013, 0.009\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "log_subdir = '_'.join(['ep', str(epochs),\n",
    "                    'bs', str(BATCH_SIZE),\n",
    "                    'lrI', str(lr_init),\n",
    "                    'lrF', str(lr_fin),\n",
    "                    'dr', str(dr)])\n",
    "\n",
    "model = get_model()\n",
    "K.set_value(model.optimizer.lr, lr_init)\n",
    "K.set_value(model.optimizer.decay, lr_decay)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=1, \\\n",
    "                          verbose=1, mode='auto')\n",
    "                          \n",
    "history = model.fit(X_train, dtrain.target\n",
    "                    , epochs=epochs\n",
    "                    , batch_size=BATCH_SIZE\n",
    "                    , validation_split=0.01\n",
    "                    , callbacks=[earlystop]\n",
    "                    , verbose=10\n",
    "                    )\n",
    "print('[{}] Finished FITTING MODEL...'.format(time.time() - start_time))\n",
    "#EVLUEATE THE MODEL ON DEV TEST\n",
    "v_rmsle = eval_model(model)\n",
    "print('[{}] Finished predicting valid set...'.format(time.time() - start_time))\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#CREATE PREDICTIONS\n",
    "preds = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "preds = np.expm1(preds)\n",
    "print('[{}] Finished predicting test set...'.format(time.time() - start_time))\n",
    "submission = test[[\"test_id\"]][:test_len]\n",
    "submission[\"price\"] = preds[:test_len]\n",
    "submission.to_csv(\"../cache/myNN\"+log_subdir+\"_{:.6}.csv\".format(v_rmsle), index=False)\n",
    "print('[{}] Finished submission...'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# org: \n",
    "# [2123.6134021282196] Finished FITTING MODEL...\n",
    "# RMSLE error on dev test: 0.43528846581148545\n",
    "# [2355.4364523887634] Finished submission..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
