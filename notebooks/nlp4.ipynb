{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Based on Bojan's -> https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-44944\n",
    "#\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "# from textblob import TextBlob\n",
    "import lightgbm as lgb\n",
    "import os, psutil\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from collections import Counter\n",
    "import re\n",
    "import lzma\n",
    "# import Levenshtein\n",
    "from numba import jit\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    import lzma\n",
    "    import Levenshtein\n",
    "except:\n",
    "    pass\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "NAME_MIN_DF = 10\n",
    "MAX_FEATURES_ITEM_DESCRIPTION = 2 ** 14\n",
    "NUM_PARTITIONS = 12 #number of partitions to split dataframe\n",
    "NUM_CORES = 4 #number of cores on your machine\n",
    "\n",
    "###################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n",
    "\n",
    "def split_cat(text):\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['category_name'] = dataset['category_name'].astype('category')\n",
    "    dataset['brand_name'] = dataset['brand_name'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n",
    "\n",
    "def print_memory_usage():\n",
    "    print('cpu: {}'.format(psutil.cpu_percent()))\n",
    "    print('consuming {:.2f}GB RAM'.format(\n",
    "           psutil.Process(os.getpid()).memory_info().rss / 1073741824),\n",
    "          flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _edit_dist(str1, str2):\n",
    "    try:\n",
    "        # very fast\n",
    "        # http://stackoverflow.com/questions/14260126/how-python-levenshtein-ratio-is-computed\n",
    "        # d = Levenshtein.ratio(str1, str2)\n",
    "        d = Levenshtein.distance(str1, str2)/float(max(len(str1),len(str2)))\n",
    "    except:\n",
    "        # https://docs.python.org/2/library/difflib.html\n",
    "        d = 1. - SequenceMatcher(lambda x: x==\" \", str1, str2).ratio()\n",
    "    return d\n",
    "def _entropy(proba):\n",
    "    entropy = -np.sum(proba*np.log(proba))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def _try_divide(x, y, val=0.0):\n",
    "    \"\"\"try to divide two numbers\"\"\"\n",
    "    if y != 0.0:\n",
    "        val = float(x) / y\n",
    "    return val\n",
    "\n",
    "def _jaccard_coef(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return _try_divide(float(len(A.intersection(B))), len(A.union(B)))\n",
    "\n",
    "\n",
    "def _dice_dist(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return _try_divide(2.*float(len(A.intersection(B))), (len(A) + len(B)))\n",
    "\n",
    "   \n",
    "def entropy(obs, token_pattern=' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    counter = Counter(obs_tokens)\n",
    "    count = np.asarray(list(counter.values()))\n",
    "    proba = count/np.sum(count)\n",
    "    del obs_tokens\n",
    "    return _entropy(proba)\n",
    "\n",
    "def digit_count(obs):\n",
    "    return len(re.findall(r\"\\d\", obs))\n",
    "\n",
    "def digit_ratio(obs, token_pattern = ' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    dr = _try_divide(len(re.findall(r\"\\d\", obs)), len(obs_tokens))\n",
    "    del obs_tokens\n",
    "    return dr\n",
    "\n",
    "def _entropy(proba):\n",
    "    entropy = -np.sum(proba*np.log(proba))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def _try_divide(x, y, val=0.0):\n",
    "    \"\"\"try to divide two numbers\"\"\"\n",
    "    if y != 0.0:\n",
    "        val = float(x) / y\n",
    "    return val\n",
    "\n",
    "\n",
    "def _jaccard_coef(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return _try_divide(float(len(A.intersection(B))), len(A.union(B)))\n",
    "\n",
    "\n",
    "def _dice_dist(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return _try_divide(2.*float(len(A.intersection(B))), (len(A) + len(B)))\n",
    "\n",
    " \n",
    "def entropy(obs, token_pattern=' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    counter = Counter(obs_tokens)\n",
    "    count = np.asarray(list(counter.values()))\n",
    "    proba = count/np.sum(count)\n",
    "    del obs_tokens\n",
    "    return _entropy(proba)\n",
    "\n",
    "def digit_count(obs):\n",
    "    return len(re.findall(r\"\\d\", obs))\n",
    "\n",
    "def digit_ratio(obs, token_pattern = ' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    return _try_divide(len(re.findall(r\"\\d\", obs)), len(obs_tokens))\n",
    "\n",
    "\n",
    "def _unigrams(words):\n",
    "    \"\"\"\n",
    "        Input: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "        Output: a list of unigram\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "\n",
    "def _bigrams(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "       Output: a list of bigram, e.g., [\"I_am\", \"am_Denny\"]\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L-1):\n",
    "                for k in range(1,skip+2):\n",
    "                        if i+k < L:\n",
    "                                lst.append( join_string.join([words[i], words[i+k]]) )\n",
    "    else:\n",
    "        # set it as unigram\n",
    "        lst = _unigrams(words)\n",
    "    return lst\n",
    "\n",
    "def _trigrams(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "       Output: a list of trigram, e.g., [\"I_am_Denny\"]\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in range(L-2):\n",
    "                for k1 in range(1,skip+2):\n",
    "                        for k2 in range(1,skip+2):\n",
    "                                if i+k1 < L and i+k1+k2 < L:\n",
    "                                        lst.append( join_string.join([words[i], words[i+k1], words[i+k1+k2]]) )\n",
    "    else:\n",
    "        # set it as bigram\n",
    "        lst = _bigrams(words, join_string, skip)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def _ngrams(words, ngram, join_string=\" \"):\n",
    "    \"\"\"wrapper for ngram\"\"\"\n",
    "    if ngram == 1:\n",
    "        return _unigrams(words)\n",
    "    elif ngram == 2:\n",
    "        return _bigrams(words, join_string)\n",
    "    elif ngram == 3:\n",
    "        return _trigrams(words, join_string)\n",
    "    elif ngram == 4:\n",
    "        return _fourgrams(words, join_string)\n",
    "    elif ngram == 12:\n",
    "        unigram = _unigrams(words)\n",
    "        bigram = [x for x in _bigrams(words, join_string) if len(x.split(join_string)) == 2]\n",
    "        return unigram + bigram\n",
    "    elif ngram == 123:\n",
    "        unigram = _unigrams(words)\n",
    "        bigram = [x for x in _bigrams(words, join_string) if len(x.split(join_string)) == 2]\n",
    "        trigram = [x for x in _trigrams(words, join_string) if len(x.split(join_string)) == 3]\n",
    "        return unigram + bigram + trigram\n",
    "\n",
    "def _ngrams(words, ngram, join_string=\" \"):\n",
    "    \"\"\"wrapper for ngram\"\"\"\n",
    "    if ngram == 1:\n",
    "        return _unigrams(words)\n",
    "    elif ngram == 2:\n",
    "        return _bigrams(words, join_string)\n",
    "    elif ngram == 3:\n",
    "        return _trigrams(words, join_string)\n",
    "    elif ngram == 4:\n",
    "        return _fourgrams(words, join_string)\n",
    "    elif ngram == 12:\n",
    "        unigram = _unigrams(words)\n",
    "        bigram = [x for x in _bigrams(words, join_string) if len(x.split(join_string)) == 2]\n",
    "        return unigram + bigram\n",
    "    elif ngram == 123:\n",
    "        unigram = _unigrams(words)\n",
    "        bigram = [x for x in _bigrams(words, join_string) if len(x.split(join_string)) == 2]\n",
    "        trigram = [x for x in _trigrams(words, join_string) if len(x.split(join_string)) == 3]\n",
    "        return unigram + bigram + trigram\n",
    "\n",
    "def UniqueCount_Ngram(obs, count, token_pattern=' '):\n",
    "    obs_tokens = obs.lower().split(token_pattern)\n",
    "    obs_ngrams = _ngrams(obs_tokens, count)\n",
    "    l = len(set(obs_ngrams))\n",
    "    del obs_tokens\n",
    "    del obs_ngrams\n",
    "    return l\n",
    "\n",
    "def UniqueRatio_Ngram(obs, count, token_pattern=' '):\n",
    "    obs_tokens = obs.lower().split(token_pattern)\n",
    "    obs_ngrams = _ngrams(obs_tokens, count)\n",
    "    r = _try_divide(len(set(obs_ngrams)), len(obs_ngrams))\n",
    "    del obs_tokens\n",
    "    del obs_ngrams\n",
    "    return r\n",
    "\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, NUM_PARTITIONS)\n",
    "    pool = Pool(NUM_CORES)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def get_sentiment_score(df):\n",
    "    df['sentiment_score'] = df['item_description'].map(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[333.3775749206543] Finished to load data\n",
      "Train shape:  (1481661, 8)\n",
      "Test shape:  (693359, 7)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_table('../input/train.tsv', engine='c')\n",
    "test = pd.read_table('../input/test.tsv', engine='c')\n",
    "train = train[train.price != 0] \n",
    "print('[{}] Finished to load data'.format(time.time() - start_time))\n",
    "print('Train shape: ', train.shape)\n",
    "print('Test shape: ', test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_id             1481661\n",
       "name                 1224596\n",
       "item_condition_id          5\n",
       "category_name           1287\n",
       "brand_name              4807\n",
       "price                    827\n",
       "shipping                   2\n",
       "item_description     1280671\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['IDL'] = train['item_description'].map(lambda x: len(str(x).lower().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(train['IDL'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrow_test = test.shape[0]\n",
    "\n",
    "test_id = test['test_id'].values\n",
    "submission = pd.DataFrame(test[['test_id']])\n",
    "\n",
    "if nrow_test < 700000:\n",
    "    test = pd.concat([test,test,test,test,test])\n",
    "    print('Test shape ', test.shape)\n",
    "\n",
    "\n",
    "nrow_train = train.shape[0]\n",
    "y = np.log1p(train[\"price\"])\n",
    "del train['price']\n",
    "merge= pd.concat([train, test])\n",
    "\n",
    "train_cols = set(train.columns)\n",
    "# del train\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "handle_missing_inplace(merge)\n",
    "print('[{}] Handle missing completed.'.format(time.time() - start_time))\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4948456"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ab2443890fed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand_name              5288\n",
       "category_name           1311\n",
       "item_condition_id          5\n",
       "item_description     1861289\n",
       "name                 1749956\n",
       "shipping                   2\n",
       "test_id               693359\n",
       "train_id             1481661\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70.99395394325256] Finished to cut\n",
      "[72.0034282207489] Finished to convert categorical\n"
     ]
    }
   ],
   "source": [
    "cutting(merge)\n",
    "print('[{}] Finished to cut'.format(time.time() - start_time))\n",
    "\n",
    "to_categorical(merge)\n",
    "print('[{}] Finished to convert categorical'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>shipping</th>\n",
       "      <th>test_id</th>\n",
       "      <th>train_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>missing</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>3</td>\n",
       "      <td>No description yet</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Razer</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>3</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Target</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>missing</td>\n",
       "      <td>Home/Home Décor/Home Décor Accents</td>\n",
       "      <td>1</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>missing</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>1</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>missing</td>\n",
       "      <td>Women/Other/Other</td>\n",
       "      <td>3</td>\n",
       "      <td>Banana republic bottoms, Candies skirt with ma...</td>\n",
       "      <td>Bundled items requested for Ruie</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Acacia Swimwear</td>\n",
       "      <td>Women/Swimwear/Two-Piece</td>\n",
       "      <td>3</td>\n",
       "      <td>Size small but straps slightly shortened to fi...</td>\n",
       "      <td>Acacia pacific tides santorini top</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soffe</td>\n",
       "      <td>Sports &amp; Outdoors/Apparel/Girls</td>\n",
       "      <td>3</td>\n",
       "      <td>You get three pairs of Sophie cheer shorts siz...</td>\n",
       "      <td>Girls cheer and tumbling bundle of 7</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Sports &amp; Outdoors/Apparel/Girls</td>\n",
       "      <td>3</td>\n",
       "      <td>Girls Size small Plus green. Three shorts total.</td>\n",
       "      <td>Girls Nike Pro shorts</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>missing</td>\n",
       "      <td>Vintage &amp; Collectibles/Collectibles/Doll</td>\n",
       "      <td>3</td>\n",
       "      <td>I realized his pants are on backwards after th...</td>\n",
       "      <td>Porcelain clown doll checker pants VTG</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Smashbox</td>\n",
       "      <td>Beauty/Makeup/Face</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25 oz Full size is 1oz for [rm] in Sephora</td>\n",
       "      <td>Smashbox primer</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Victoria's Secret</td>\n",
       "      <td>Beauty/Fragrance/Women</td>\n",
       "      <td>1</td>\n",
       "      <td>(5) new vs pink body mists (2.5 oz each) Fresh...</td>\n",
       "      <td>New vs pi k body mists</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rue</td>\n",
       "      <td>Women/Dresses/Above Knee, Mini</td>\n",
       "      <td>2</td>\n",
       "      <td>Xl, great condition</td>\n",
       "      <td>Black Skater dress</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scholastic</td>\n",
       "      <td>Other/Office supplies/School Supplies</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "      <td>Sharpener and eraser</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UGG Australia</td>\n",
       "      <td>Women/Shoes/Boots</td>\n",
       "      <td>3</td>\n",
       "      <td>Authentic. Suede fringe boots. Great condition...</td>\n",
       "      <td>HOLD for Dogs2016 Minnetonka boots</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Tarte</td>\n",
       "      <td>Beauty/Makeup/Makeup Sets</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand new. Deluxe travel size products. Contai...</td>\n",
       "      <td>Sephora tarte birthday gift</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wet n Wild</td>\n",
       "      <td>Beauty/Makeup/Eyes</td>\n",
       "      <td>1</td>\n",
       "      <td>2 glitter eyeshadows; one in Brass and one in ...</td>\n",
       "      <td>Glitter Eyeshadow</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>missing</td>\n",
       "      <td>Kids/Gear/Backpacks &amp; Carriers</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand new in box Size: Medium Color: Coral Ret...</td>\n",
       "      <td>New: Baby K'tan active baby carrier</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Too Faced</td>\n",
       "      <td>Beauty/Makeup/Makeup Palettes</td>\n",
       "      <td>1</td>\n",
       "      <td>This AUTHENTIC pallete by Too Faced is brand n...</td>\n",
       "      <td>Too Faced Limited \"Merry Macaroons\"</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Anthropologie</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>2</td>\n",
       "      <td>Fancy, dressy or casual! Dress it up or down 1...</td>\n",
       "      <td>Cream/ Beige Front Cross Shirt</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Torrid</td>\n",
       "      <td>Women/Tops &amp; Blouses/Tank, Cami</td>\n",
       "      <td>3</td>\n",
       "      <td>Size 1. Worn once. Excellent condition</td>\n",
       "      <td>Torrid Nautical Peplum Tube Top</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Victoria's Secret</td>\n",
       "      <td>Women/Athletic Apparel/Sports Bras</td>\n",
       "      <td>1</td>\n",
       "      <td>NWT Victoria's Secret ULTIMATE SPORT BRA -MAXI...</td>\n",
       "      <td>NWT VS ULTIMATE SPORTS BRA 34ddd</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Samsung</td>\n",
       "      <td>Electronics/Cell Phones &amp; Accessories/Cell Pho...</td>\n",
       "      <td>3</td>\n",
       "      <td>Reasonable offers welcomed. But if you ask \"lo...</td>\n",
       "      <td>Galaxy S7 Edge (Unlocked) 32GB</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>missing</td>\n",
       "      <td>Electronics/Cell Phones &amp; Accessories/Chargers...</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand new never used All colors are available ...</td>\n",
       "      <td>Triple car charger</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>FOREVER 21</td>\n",
       "      <td>Women/Tops &amp; Blouses/T-Shirts</td>\n",
       "      <td>2</td>\n",
       "      <td>lanascloset ~~~ description: never worn! ✨ i d...</td>\n",
       "      <td>Black and Red Baseball Tee</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Air Jordan</td>\n",
       "      <td>Men/Shoes/Athletic</td>\n",
       "      <td>3</td>\n",
       "      <td>They are 100 percent authentic. They are beate...</td>\n",
       "      <td>Air Jordan carmine 6s</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Electronics/Cell Phones &amp; Accessories/Cases, C...</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand new Otterbox Defender iPhone 6 Plus/6s Plus</td>\n",
       "      <td>Otterbox Defender iPhone 6 Plus/6s Plus</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LuLaRoe</td>\n",
       "      <td>Women/Athletic Apparel/Pants, Tights, Leggings</td>\n",
       "      <td>3</td>\n",
       "      <td>Worn one time. Excellent condition</td>\n",
       "      <td>LuLaRoe OS Black With White Polka Dots</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FOREVER 21</td>\n",
       "      <td>Women/Swimwear/One-Piece</td>\n",
       "      <td>2</td>\n",
       "      <td>Beautiful Excellent condition Zips and ties in...</td>\n",
       "      <td>Forever21 floral romper strapless</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Hollister</td>\n",
       "      <td>Women/Jeans/Boot Cut</td>\n",
       "      <td>3</td>\n",
       "      <td>29w. X 33l. Social stretch hollister jeans ***...</td>\n",
       "      <td>Kendra bundle</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Adidas</td>\n",
       "      <td>Women/Shoes/Athletic</td>\n",
       "      <td>3</td>\n",
       "      <td>Overall good condition. A few signs of wear</td>\n",
       "      <td>Adidas Ultraboost Shoes</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>missing</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Fine or Fashion: Fashion Item Type: Necklace ...</td>\n",
       "      <td>Partners In Crime Necklace ShipfromChina</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Nostalgia Electrics</td>\n",
       "      <td>Home/Home Décor/Home Fragrance</td>\n",
       "      <td>1</td>\n",
       "      <td>WARMERS PICTURED &amp; 6 SAMPLE PACKAGES OF HIGHLY...</td>\n",
       "      <td>Listing for Aknuckles1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>missing</td>\n",
       "      <td>Kids/Boys 0-24 Mos/Accessories</td>\n",
       "      <td>3</td>\n",
       "      <td>Probably best for up to a 2 year old.</td>\n",
       "      <td>Toddler Vineyard Vines Hat</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Rae Dunn</td>\n",
       "      <td>Home/Kitchen &amp; Dining/Coffee &amp; Tea Accessories</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand new! No cracks or chips! I package every...</td>\n",
       "      <td>Rae Dunn Beauty and the Beast</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Littlest Pet Shop</td>\n",
       "      <td>Kids/Toys/Dolls &amp; Accessories</td>\n",
       "      <td>3</td>\n",
       "      <td>these are on hold do not buy :)</td>\n",
       "      <td>Littlest pet shop accessories for Bon</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>missing</td>\n",
       "      <td>Women/Swimwear/Two-Piece</td>\n",
       "      <td>1</td>\n",
       "      <td>Both are a Chinese size medium which fits a US...</td>\n",
       "      <td>Black &amp; Burgundy Bikini Bottoms</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>missing</td>\n",
       "      <td>Home/Home Décor/Other</td>\n",
       "      <td>1</td>\n",
       "      <td>Aura Fluorite: ⋅ The \"protection stone\" ⋅ Grou...</td>\n",
       "      <td>Aura Fluorite Double Terminated Wand</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>missing</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>3</td>\n",
       "      <td>Worn once; will be washed before sent</td>\n",
       "      <td>Trump Shirt</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>missing</td>\n",
       "      <td>Kids/Girls 0-24 Mos/Shoes</td>\n",
       "      <td>3</td>\n",
       "      <td>Great condition sea wees size 0 brown</td>\n",
       "      <td>Sea wees size 0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Motherhood Maternity</td>\n",
       "      <td>Women/Maternity/Tops &amp; Blouses</td>\n",
       "      <td>3</td>\n",
       "      <td>Sheer black flowy top with cute flower design....</td>\n",
       "      <td>Maternity top bundle</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Sephora</td>\n",
       "      <td>Beauty/Makeup/Face</td>\n",
       "      <td>1</td>\n",
       "      <td>New without original Packaging. SHADE IS MEDIU...</td>\n",
       "      <td>*Hold*FREE TODAY-IT COSMETICS CC HOLD</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>missing</td>\n",
       "      <td>Beauty/Makeup/Eyes</td>\n",
       "      <td>2</td>\n",
       "      <td>5 Mascaras Loreal Covergirl Rimmel Maybelline</td>\n",
       "      <td>Mascara Bundle</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>missing</td>\n",
       "      <td>Beauty/Makeup/Eyes</td>\n",
       "      <td>1</td>\n",
       "      <td>Eyebrows Essential Kit   Everything you need t...</td>\n",
       "      <td>Eyebrows Essential Kit MEDIUM; Brown</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>missing</td>\n",
       "      <td>Women/Tops &amp; Blouses/T-Shirts</td>\n",
       "      <td>3</td>\n",
       "      <td>Extra large shirt with purple beaded flower de...</td>\n",
       "      <td>Women's Espresso Pink Coral shirt XL</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>missing</td>\n",
       "      <td>Home/Bedding/Sheets &amp; Pillowcases</td>\n",
       "      <td>3</td>\n",
       "      <td>Hot pink/reddish sheet set from Victoria's Sec...</td>\n",
       "      <td>VS PINK sheet Set</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>missing</td>\n",
       "      <td>Sports &amp; Outdoors/Exercise/Fitness technology</td>\n",
       "      <td>1</td>\n",
       "      <td>This boxed set of 12 DVD’s includes the Fitnes...</td>\n",
       "      <td>Jillian Michaels bodyshred Fitness DVD</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Melissa &amp; Doug</td>\n",
       "      <td>Kids/Toys/Dress Up &amp; Pretend Play</td>\n",
       "      <td>3</td>\n",
       "      <td>Used but still a great play item.</td>\n",
       "      <td>Melissa and Doug folding go kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Pokemon</td>\n",
       "      <td>Vintage &amp; Collectibles/Trading Cards/Animation</td>\n",
       "      <td>3</td>\n",
       "      <td>I have 2 available</td>\n",
       "      <td>Misty tentacruel foil pokemon card</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Kids/Boys (4+)/Shoes</td>\n",
       "      <td>3</td>\n",
       "      <td>Size 6Y. High tops. Super light. In good condi...</td>\n",
       "      <td>Nike boy sneakers</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Target</td>\n",
       "      <td>Home/Storage &amp; Organization/Jewelry Boxes &amp; Or...</td>\n",
       "      <td>2</td>\n",
       "      <td>No description yet</td>\n",
       "      <td>Jewel holder</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Nintendo</td>\n",
       "      <td>Electronics/Video Games &amp; Consoles/Games</td>\n",
       "      <td>3</td>\n",
       "      <td>Good used condition. Ask about bundling to save.</td>\n",
       "      <td>Goosebumps HorrorLand DS Game</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Silver Jeans Co.</td>\n",
       "      <td>Women/Jeans/Boot Cut</td>\n",
       "      <td>3</td>\n",
       "      <td>Distressed with holes. Great shape well taken ...</td>\n",
       "      <td>Silver jeans size 11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>missing</td>\n",
       "      <td>Sports &amp; Outdoors/Fan Shop/NCAA</td>\n",
       "      <td>3</td>\n",
       "      <td>Size M (8), cuffs show wear and letters are no...</td>\n",
       "      <td>Crimson Tide Alabama T-shirt</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Spin Master</td>\n",
       "      <td>Kids/Toys/Electronics for Kids</td>\n",
       "      <td>1</td>\n",
       "      <td>BNIP VHTF! HOTTEST toy of the season!</td>\n",
       "      <td>Hatchimals Draggle</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Gap</td>\n",
       "      <td>Kids/Girls 2T-5T/Shoes</td>\n",
       "      <td>3</td>\n",
       "      <td>BabyGap canvas army green Gently loved, lots o...</td>\n",
       "      <td>Gap canvas shoes {size 8}</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>missing</td>\n",
       "      <td>Beauty/Makeup/Face</td>\n",
       "      <td>1</td>\n",
       "      <td>Bought off Groupon and they charged me for 2, ...</td>\n",
       "      <td>Luminess Air Airbrush Makeup System</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>missing</td>\n",
       "      <td>Electronics/Cell Phones &amp; Accessories/Cases, C...</td>\n",
       "      <td>2</td>\n",
       "      <td>Bling Mickey ear case!! Just used one for a Di...</td>\n",
       "      <td>iPhone 5S Mickey ear case</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>missing</td>\n",
       "      <td>Men/Men's Accessories/Watches</td>\n",
       "      <td>3</td>\n",
       "      <td>Rose gold bezel and crown Engraved logo Red an...</td>\n",
       "      <td>Gucci</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Lululemon</td>\n",
       "      <td>Women/Athletic Apparel/Pants, Tights, Leggings</td>\n",
       "      <td>3</td>\n",
       "      <td>Watercolor Inspire crop. No pilling or stickin...</td>\n",
       "      <td>Size 6 Watercolor Inspire Crop</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              brand_name                                      category_name  \\\n",
       "0                missing                                  Men/Tops/T-shirts   \n",
       "1                  Razer  Electronics/Computers & Tablets/Components & P...   \n",
       "2                 Target                        Women/Tops & Blouses/Blouse   \n",
       "3                missing                 Home/Home Décor/Home Décor Accents   \n",
       "4                missing                            Women/Jewelry/Necklaces   \n",
       "5                missing                                  Women/Other/Other   \n",
       "6        Acacia Swimwear                           Women/Swimwear/Two-Piece   \n",
       "7                  Soffe                    Sports & Outdoors/Apparel/Girls   \n",
       "8                   Nike                    Sports & Outdoors/Apparel/Girls   \n",
       "9                missing           Vintage & Collectibles/Collectibles/Doll   \n",
       "10              Smashbox                                 Beauty/Makeup/Face   \n",
       "11     Victoria's Secret                             Beauty/Fragrance/Women   \n",
       "12                   rue                     Women/Dresses/Above Knee, Mini   \n",
       "13            Scholastic              Other/Office supplies/School Supplies   \n",
       "14         UGG Australia                                  Women/Shoes/Boots   \n",
       "15                 Tarte                          Beauty/Makeup/Makeup Sets   \n",
       "16            Wet n Wild                                 Beauty/Makeup/Eyes   \n",
       "17               missing                     Kids/Gear/Backpacks & Carriers   \n",
       "18             Too Faced                      Beauty/Makeup/Makeup Palettes   \n",
       "19         Anthropologie                        Women/Tops & Blouses/Blouse   \n",
       "20                Torrid                    Women/Tops & Blouses/Tank, Cami   \n",
       "21     Victoria's Secret                 Women/Athletic Apparel/Sports Bras   \n",
       "22               Samsung  Electronics/Cell Phones & Accessories/Cell Pho...   \n",
       "23               missing  Electronics/Cell Phones & Accessories/Chargers...   \n",
       "24            FOREVER 21                      Women/Tops & Blouses/T-Shirts   \n",
       "25            Air Jordan                                 Men/Shoes/Athletic   \n",
       "26                 Apple  Electronics/Cell Phones & Accessories/Cases, C...   \n",
       "27               LuLaRoe     Women/Athletic Apparel/Pants, Tights, Leggings   \n",
       "28            FOREVER 21                           Women/Swimwear/One-Piece   \n",
       "29             Hollister                               Women/Jeans/Boot Cut   \n",
       "..                   ...                                                ...   \n",
       "70                Adidas                               Women/Shoes/Athletic   \n",
       "71               missing                            Women/Jewelry/Necklaces   \n",
       "72   Nostalgia Electrics                     Home/Home Décor/Home Fragrance   \n",
       "73               missing                     Kids/Boys 0-24 Mos/Accessories   \n",
       "74              Rae Dunn     Home/Kitchen & Dining/Coffee & Tea Accessories   \n",
       "75     Littlest Pet Shop                      Kids/Toys/Dolls & Accessories   \n",
       "76               missing                           Women/Swimwear/Two-Piece   \n",
       "77               missing                              Home/Home Décor/Other   \n",
       "78               missing                                  Men/Tops/T-shirts   \n",
       "79               missing                          Kids/Girls 0-24 Mos/Shoes   \n",
       "80  Motherhood Maternity                     Women/Maternity/Tops & Blouses   \n",
       "81               Sephora                                 Beauty/Makeup/Face   \n",
       "82               missing                                 Beauty/Makeup/Eyes   \n",
       "83               missing                                 Beauty/Makeup/Eyes   \n",
       "84               missing                      Women/Tops & Blouses/T-Shirts   \n",
       "85               missing                  Home/Bedding/Sheets & Pillowcases   \n",
       "86               missing      Sports & Outdoors/Exercise/Fitness technology   \n",
       "87        Melissa & Doug                  Kids/Toys/Dress Up & Pretend Play   \n",
       "88               Pokemon     Vintage & Collectibles/Trading Cards/Animation   \n",
       "89                  Nike                               Kids/Boys (4+)/Shoes   \n",
       "90                Target  Home/Storage & Organization/Jewelry Boxes & Or...   \n",
       "91              Nintendo           Electronics/Video Games & Consoles/Games   \n",
       "92      Silver Jeans Co.                               Women/Jeans/Boot Cut   \n",
       "93               missing                    Sports & Outdoors/Fan Shop/NCAA   \n",
       "94           Spin Master                     Kids/Toys/Electronics for Kids   \n",
       "95                   Gap                             Kids/Girls 2T-5T/Shoes   \n",
       "96               missing                                 Beauty/Makeup/Face   \n",
       "97               missing  Electronics/Cell Phones & Accessories/Cases, C...   \n",
       "98               missing                      Men/Men's Accessories/Watches   \n",
       "99             Lululemon     Women/Athletic Apparel/Pants, Tights, Leggings   \n",
       "\n",
       "   item_condition_id                                   item_description  \\\n",
       "0                  3                                 No description yet   \n",
       "1                  3  This keyboard is in great condition and works ...   \n",
       "2                  1  Adorable top with a hint of lace and a key hol...   \n",
       "3                  1  New with tags. Leather horses. Retail for [rm]...   \n",
       "4                  1          Complete with certificate of authenticity   \n",
       "5                  3  Banana republic bottoms, Candies skirt with ma...   \n",
       "6                  3  Size small but straps slightly shortened to fi...   \n",
       "7                  3  You get three pairs of Sophie cheer shorts siz...   \n",
       "8                  3   Girls Size small Plus green. Three shorts total.   \n",
       "9                  3  I realized his pants are on backwards after th...   \n",
       "10                 2       0.25 oz Full size is 1oz for [rm] in Sephora   \n",
       "11                 1  (5) new vs pink body mists (2.5 oz each) Fresh...   \n",
       "12                 2                                Xl, great condition   \n",
       "13                 1                                 No description yet   \n",
       "14                 3  Authentic. Suede fringe boots. Great condition...   \n",
       "15                 1  Brand new. Deluxe travel size products. Contai...   \n",
       "16                 1  2 glitter eyeshadows; one in Brass and one in ...   \n",
       "17                 1  Brand new in box Size: Medium Color: Coral Ret...   \n",
       "18                 1  This AUTHENTIC pallete by Too Faced is brand n...   \n",
       "19                 2  Fancy, dressy or casual! Dress it up or down 1...   \n",
       "20                 3             Size 1. Worn once. Excellent condition   \n",
       "21                 1  NWT Victoria's Secret ULTIMATE SPORT BRA -MAXI...   \n",
       "22                 3  Reasonable offers welcomed. But if you ask \"lo...   \n",
       "23                 1  Brand new never used All colors are available ...   \n",
       "24                 2  lanascloset ~~~ description: never worn! ✨ i d...   \n",
       "25                 3  They are 100 percent authentic. They are beate...   \n",
       "26                 1  Brand new Otterbox Defender iPhone 6 Plus/6s Plus   \n",
       "27                 3                 Worn one time. Excellent condition   \n",
       "28                 2  Beautiful Excellent condition Zips and ties in...   \n",
       "29                 3  29w. X 33l. Social stretch hollister jeans ***...   \n",
       "..               ...                                                ...   \n",
       "70                 3        Overall good condition. A few signs of wear   \n",
       "71                 1  \"Fine or Fashion: Fashion Item Type: Necklace ...   \n",
       "72                 1  WARMERS PICTURED & 6 SAMPLE PACKAGES OF HIGHLY...   \n",
       "73                 3              Probably best for up to a 2 year old.   \n",
       "74                 1  Brand new! No cracks or chips! I package every...   \n",
       "75                 3                    these are on hold do not buy :)   \n",
       "76                 1  Both are a Chinese size medium which fits a US...   \n",
       "77                 1  Aura Fluorite: ⋅ The \"protection stone\" ⋅ Grou...   \n",
       "78                 3              Worn once; will be washed before sent   \n",
       "79                 3              Great condition sea wees size 0 brown   \n",
       "80                 3  Sheer black flowy top with cute flower design....   \n",
       "81                 1  New without original Packaging. SHADE IS MEDIU...   \n",
       "82                 2      5 Mascaras Loreal Covergirl Rimmel Maybelline   \n",
       "83                 1  Eyebrows Essential Kit   Everything you need t...   \n",
       "84                 3  Extra large shirt with purple beaded flower de...   \n",
       "85                 3  Hot pink/reddish sheet set from Victoria's Sec...   \n",
       "86                 1  This boxed set of 12 DVD’s includes the Fitnes...   \n",
       "87                 3                  Used but still a great play item.   \n",
       "88                 3                                 I have 2 available   \n",
       "89                 3  Size 6Y. High tops. Super light. In good condi...   \n",
       "90                 2                                 No description yet   \n",
       "91                 3   Good used condition. Ask about bundling to save.   \n",
       "92                 3  Distressed with holes. Great shape well taken ...   \n",
       "93                 3  Size M (8), cuffs show wear and letters are no...   \n",
       "94                 1              BNIP VHTF! HOTTEST toy of the season!   \n",
       "95                 3  BabyGap canvas army green Gently loved, lots o...   \n",
       "96                 1  Bought off Groupon and they charged me for 2, ...   \n",
       "97                 2  Bling Mickey ear case!! Just used one for a Di...   \n",
       "98                 3  Rose gold bezel and crown Engraved logo Red an...   \n",
       "99                 3  Watercolor Inspire crop. No pilling or stickin...   \n",
       "\n",
       "                                        name  shipping  test_id  train_id  \n",
       "0        MLB Cincinnati Reds T Shirt Size XL         1      NaN       0.0  \n",
       "1           Razer BlackWidow Chroma Keyboard         0      NaN       1.0  \n",
       "2                             AVA-VIV Blouse         1      NaN       2.0  \n",
       "3                      Leather Horse Statues         1      NaN       3.0  \n",
       "4                       24K GOLD plated rose         0      NaN       4.0  \n",
       "5           Bundled items requested for Ruie         0      NaN       5.0  \n",
       "6         Acacia pacific tides santorini top         0      NaN       6.0  \n",
       "7       Girls cheer and tumbling bundle of 7         1      NaN       7.0  \n",
       "8                      Girls Nike Pro shorts         0      NaN       8.0  \n",
       "9     Porcelain clown doll checker pants VTG         0      NaN       9.0  \n",
       "10                           Smashbox primer         1      NaN      10.0  \n",
       "11                    New vs pi k body mists         0      NaN      11.0  \n",
       "12                        Black Skater dress         0      NaN      12.0  \n",
       "13                      Sharpener and eraser         1      NaN      13.0  \n",
       "14        HOLD for Dogs2016 Minnetonka boots         0      NaN      14.0  \n",
       "15               Sephora tarte birthday gift         1      NaN      15.0  \n",
       "16                         Glitter Eyeshadow         1      NaN      16.0  \n",
       "17       New: Baby K'tan active baby carrier         1      NaN      17.0  \n",
       "18       Too Faced Limited \"Merry Macaroons\"         1      NaN      18.0  \n",
       "19            Cream/ Beige Front Cross Shirt         0      NaN      19.0  \n",
       "20           Torrid Nautical Peplum Tube Top         1      NaN      20.0  \n",
       "21          NWT VS ULTIMATE SPORTS BRA 34ddd         0      NaN      21.0  \n",
       "22            Galaxy S7 Edge (Unlocked) 32GB         0      NaN      22.0  \n",
       "23                        Triple car charger         1      NaN      23.0  \n",
       "24                Black and Red Baseball Tee         0      NaN      24.0  \n",
       "25                     Air Jordan carmine 6s         0      NaN      25.0  \n",
       "26   Otterbox Defender iPhone 6 Plus/6s Plus         1      NaN      26.0  \n",
       "27    LuLaRoe OS Black With White Polka Dots         0      NaN      27.0  \n",
       "28         Forever21 floral romper strapless         1      NaN      28.0  \n",
       "29                             Kendra bundle         1      NaN      29.0  \n",
       "..                                       ...       ...      ...       ...  \n",
       "70                   Adidas Ultraboost Shoes         0      NaN      70.0  \n",
       "71  Partners In Crime Necklace ShipfromChina         1      NaN      71.0  \n",
       "72                    Listing for Aknuckles1         0      NaN      72.0  \n",
       "73                Toddler Vineyard Vines Hat         0      NaN      73.0  \n",
       "74             Rae Dunn Beauty and the Beast         0      NaN      74.0  \n",
       "75     Littlest pet shop accessories for Bon         1      NaN      75.0  \n",
       "76           Black & Burgundy Bikini Bottoms         0      NaN      76.0  \n",
       "77      Aura Fluorite Double Terminated Wand         1      NaN      77.0  \n",
       "78                               Trump Shirt         0      NaN      78.0  \n",
       "79                           Sea wees size 0         0      NaN      79.0  \n",
       "80                      Maternity top bundle         0      NaN      80.0  \n",
       "81     *Hold*FREE TODAY-IT COSMETICS CC HOLD         1      NaN      81.0  \n",
       "82                            Mascara Bundle         1      NaN      82.0  \n",
       "83      Eyebrows Essential Kit MEDIUM; Brown         1      NaN      83.0  \n",
       "84      Women's Espresso Pink Coral shirt XL         0      NaN      84.0  \n",
       "85                         VS PINK sheet Set         0      NaN      85.0  \n",
       "86    Jillian Michaels bodyshred Fitness DVD         0      NaN      86.0  \n",
       "87       Melissa and Doug folding go kitchen         0      NaN      87.0  \n",
       "88        Misty tentacruel foil pokemon card         1      NaN      88.0  \n",
       "89                         Nike boy sneakers         0      NaN      89.0  \n",
       "90                              Jewel holder         1      NaN      90.0  \n",
       "91             Goosebumps HorrorLand DS Game         0      NaN      91.0  \n",
       "92                      Silver jeans size 11         0      NaN      92.0  \n",
       "93              Crimson Tide Alabama T-shirt         0      NaN      93.0  \n",
       "94                        Hatchimals Draggle         1      NaN      94.0  \n",
       "95                 Gap canvas shoes {size 8}         1      NaN      95.0  \n",
       "96       Luminess Air Airbrush Makeup System         1      NaN      96.0  \n",
       "97                 iPhone 5S Mickey ear case         1      NaN      97.0  \n",
       "98                                     Gucci         0      NaN      98.0  \n",
       "99            Size 6 Watercolor Inspire Crop         0      NaN      99.0  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(x):\n",
    "    doc = nlp(x)\n",
    "    tokens = [i.lemma_ for i in doc]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokens_name(df):\n",
    "    df['name1'] = df['name'].map(lambda x: get_tokens(x))\n",
    "    return df\n",
    "\n",
    "def get_tokens_item_desc(df):\n",
    "    df['item_description1'] = df['item_description'].map(lambda x: get_tokens(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 8.71 s, total: 1min 15s\n",
      "Wall time: 9min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merge = parallelize_dataframe(merge, get_tokens_name)\n",
    "merge = parallelize_dataframe(merge, get_tokens_item_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge = merge.drop('name', axis=1)\n",
    "merge = merge.drop('item_description', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge.rename(columns={'item_description1': 'item_description', 'name1': 'name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>shipping</th>\n",
       "      <th>test_id</th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>missing</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[MLB, Cincinnati, Reds, T, Shirt, Size, XL]</td>\n",
       "      <td>[No, description, yet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Razer</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Razer, BlackWidow, Chroma, Keyboard]</td>\n",
       "      <td>[This, keyboard, be, in, great, condition, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Target</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[AVA, -, VIV, Blouse]</td>\n",
       "      <td>[Adorable, top, with, a, hint, of, lace, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>missing</td>\n",
       "      <td>Home/Home Décor/Home Décor Accents</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[Leather, Horse, Statues]</td>\n",
       "      <td>[New, with, tag, ., Leather, horse, ., Retail,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>missing</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[24, K, GOLD, plate, rise]</td>\n",
       "      <td>[Complete, with, certificate, of, authenticity]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  brand_name                                      category_name  \\\n",
       "0    missing                                  Men/Tops/T-shirts   \n",
       "1      Razer  Electronics/Computers & Tablets/Components & P...   \n",
       "2     Target                        Women/Tops & Blouses/Blouse   \n",
       "3    missing                 Home/Home Décor/Home Décor Accents   \n",
       "4    missing                            Women/Jewelry/Necklaces   \n",
       "\n",
       "   item_condition_id  shipping  test_id  train_id  \\\n",
       "0                  3         1      NaN       0.0   \n",
       "1                  3         0      NaN       1.0   \n",
       "2                  1         1      NaN       2.0   \n",
       "3                  1         1      NaN       3.0   \n",
       "4                  1         0      NaN       4.0   \n",
       "\n",
       "                                          name  \\\n",
       "0  [MLB, Cincinnati, Reds, T, Shirt, Size, XL]   \n",
       "1        [Razer, BlackWidow, Chroma, Keyboard]   \n",
       "2                        [AVA, -, VIV, Blouse]   \n",
       "3                    [Leather, Horse, Statues]   \n",
       "4                   [24, K, GOLD, plate, rise]   \n",
       "\n",
       "                                    item_description  \n",
       "0                             [No, description, yet]  \n",
       "1  [This, keyboard, be, in, great, condition, and...  \n",
       "2  [Adorable, top, with, a, hint, of, lace, and, ...  \n",
       "3  [New, with, tag, ., Leather, horse, ., Retail,...  \n",
       "4    [Complete, with, certificate, of, authenticity]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_doclen_name(df):\n",
    "#     df['name_doclen'] = df['name'].map(lambda x: len(str(x).lower().split(' ')))\n",
    "#     return df\n",
    "\n",
    "# def get_doclen_itemdesc(df):\n",
    "#     df['item_description_doclen'] = df['item_description'].map(lambda x: len(str(x).lower().split(' ')))\n",
    "#     return df\n",
    "\n",
    "# def get_doclen_brand_name(df):\n",
    "#     df['brand_name_doclen'] = df['brand_name'].map(lambda x: len(str(x).lower().split(' ')))\n",
    "#     return df\n",
    "\n",
    "# def get_entropy_name(df):\n",
    "#     df['name_entropy'] = df['name'].map(lambda x: entropy(str(x).lower(), ' '))\n",
    "#     return df\n",
    "\n",
    "# def get_entropy_itemdesc(df):\n",
    "#     df['item_description_entropy'] = \\\n",
    "#         df['item_description'].map(lambda x: entropy(str(x).lower(), ' '))\n",
    "#     return df\n",
    "\n",
    "# def get_entropy_brand_name(df):\n",
    "#     df['brand_name_entropy'] = \\\n",
    "#         df['brand_name'].map(lambda x: entropy(str(x).lower(), ' '))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_count_name(df):\n",
    "#     df['name_dc'] = df['name'].map(lambda x: digit_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_count_itemdesc(df):\n",
    "#     df['item_description_dc'] = \\\n",
    "#         df['item_description'].map(lambda x: digit_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_count_brand_name(df):\n",
    "#     df['brand_name_dc'] = \\\n",
    "#         df['brand_name'].map(lambda x: digit_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_ratio_name(df):\n",
    "#     df['name_dr'] = df['name'].map(lambda x: digit_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_ratio_itemdesc(df):\n",
    "#     df['item_description_dr'] = \\\n",
    "#         df['item_description'].map(lambda x: digit_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_ratio_brand_name(df):\n",
    "#     df['brand_name_dr'] = \\\n",
    "#         df['brand_name'].map(lambda x: digit_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_count_name(df):\n",
    "#     df['name_ec'] = df['name'].map(lambda x: emoji_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_count_itemdesc(df):\n",
    "#     df['item_description_ec'] = \\\n",
    "#         df['item_description'].map(lambda x: emoji_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_count_brand_name(df):\n",
    "#     df['brand_name_ec'] = \\\n",
    "#         df['brand_name'].map(lambda x: emoji_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_ratio_name(df):\n",
    "#     df['name_er'] = df['name'].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_ratio_itemdesc(df):\n",
    "#     df['item_description_er'] = \\\n",
    "#         df['item_description'].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_ratio_brand_name(df):\n",
    "#     df['brand_name_er'] = \\\n",
    "#         df['brand_name'].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cols1 = set(merge.columns)\n",
    "# cols = []\n",
    "# obs_fields = ['name', 'brand_name', 'item_description']\n",
    "# merge = parallelize_dataframe(merge, get_doclen_name)\n",
    "# merge = parallelize_dataframe(merge, get_doclen_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_doclen_brand_name)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_entropy_name)\n",
    "# merge = parallelize_dataframe(merge, get_entropy_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_entropy_brand_name)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_digit_count_name)\n",
    "# merge = parallelize_dataframe(merge, get_digit_count_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_digit_count_brand_name)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_digit_ratio_name)\n",
    "# merge = parallelize_dataframe(merge, get_digit_ratio_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_digit_ratio_brand_name)\n",
    "\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_count_name)\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_count_itemdesc)\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_count_brand_name)\n",
    "\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_ratio_name)\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_ratio_itemdesc)\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_ratio_brand_name)\n",
    "\n",
    "# print('[{}] Finished basic creation for name, bn, item_desc'.format(time.time() - start_time))\n",
    "# print_memory_usage()\n",
    "\n",
    "# for f in obs_fields:\n",
    "#     counter = Counter(merge[f].values)\n",
    "#     merge[f+'_docfreq'] = merge[f].map(lambda x: counter[x])\n",
    "\n",
    "#     cols.append(f+'_doclen')\n",
    "#     cols.append(f+'_docfreq')\n",
    "#     cols.append(f+'_docEntropy')\n",
    "#     cols.append(f+'_digitCount')\n",
    "#     cols.append(f+'_digitRatio')\n",
    "#     # cols.append(f+'_emojiCount')\n",
    "#     # cols.append(f+'_emojiRatio')\n",
    "\n",
    "# f = 'category_name'\n",
    "# def get_category_name_doclen(df):\n",
    "#     df[f+'_doclen'] = df[f].map(lambda x: len(str(x).lower().split('/')))\n",
    "#     return df\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_category_name_doclen)\n",
    "\n",
    "# counter = Counter(merge[f].values)\n",
    "# merge[f+'_docfreq'] = merge[f].map(lambda x: counter[x])\n",
    "\n",
    "# token_pattern = '/'\n",
    "\n",
    "# def get_category_name_entropy(df):\n",
    "#         df[f+'_docEntropy'] = df[f].map(lambda x: entropy(str(x).lower(),token_pattern))\n",
    "#         return df\n",
    "# merge = parallelize_dataframe(merge, get_category_name_entropy)\n",
    "\n",
    "# def get_category_name_dc(df):\n",
    "#         df[f+'_dc'] = df[f].map(lambda x: digit_count(str(x).lower()))\n",
    "#         return df\n",
    "# merge = parallelize_dataframe(merge, get_category_name_dc)\n",
    "\n",
    "# def get_category_name_dr(df):\n",
    "#         df[f+'_dr'] = df[f].map(lambda x: digit_ratio(str(x).lower(), token_pattern))\n",
    "#         return df\n",
    "# merge = parallelize_dataframe(merge, get_category_name_dr)\n",
    "\n",
    "# def get_category_name_ec(df):\n",
    "#         df[f+'_emojiCount'] = df[f].map(lambda x: emoji_count(str(x).lower()))\n",
    "#         return df\n",
    "# # merge = parallelize_dataframe(merge, get_category_name_ec)\n",
    "\n",
    "# def get_category_name_er(df):\n",
    "#         df[f+'_emojiRatio'] = df[f].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "#         return df\n",
    "# # merge = parallelize_dataframe(merge, get_category_name_er)\n",
    "\n",
    "# cols.append(f+'_doclen')\n",
    "# cols.append(f+'_docfreq')\n",
    "# cols.append(f+'_docEntropy')\n",
    "# cols.append(f+'_digitCount')\n",
    "# cols.append(f+'_digitRatio')\n",
    "# # cols.append(f+'_emojiCount')\n",
    "# # cols.append(f+'_emojiRatio')\n",
    "\n",
    "# print('[{}] Finished basic creation for category_name'.format(time.time() - start_time))\n",
    "\n",
    "# obs_fields = [\"name\", \"item_description\"]\n",
    "\n",
    "# print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_bigram_uc_name(df):\n",
    "#     df['name_2_uc'] = df['name'].map(lambda x: UniqueCount_Ngram(str(x), 2))\n",
    "#     return df\n",
    "# merge = parallelize_dataframe(merge, get_bigram_uc_name)\n",
    "\n",
    "# def get_bigram_uc_item_desc(df):\n",
    "#     df['item_desc_2_uc'] = \\\n",
    "#             df['item_description'].map(lambda x: UniqueCount_Ngram(str(x), 2))\n",
    "#     return df\n",
    "# merge = parallelize_dataframe(merge, get_bigram_uc_item_desc)\n",
    "\n",
    "# def get_bigram_ur_name(df):\n",
    "#     df['name_2_ur'] = df['name'].map(lambda x: UniqueRatio_Ngram(str(x), 2))\n",
    "#     return df\n",
    "# merge = parallelize_dataframe(merge, get_bigram_ur_name)\n",
    "\n",
    "# def get_bigram_ur_item_desc(df):\n",
    "#     df['item_desc_2_ur'] = \\\n",
    "#             df['item_description'].map(lambda x: UniqueRatio_Ngram(str(x), 2))\n",
    "#     return df\n",
    "# merge = parallelize_dataframe(merge, get_bigram_ur_item_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge =  merge.loc[:, (merge != merge.iloc[0]).any()]\n",
    "# print(len(cols))\n",
    "# del cols\n",
    "# cols = list(set(merge.columns) - cols1)\n",
    "# print(len(cols))\n",
    "\n",
    "# X_b = merge[cols]\n",
    "\n",
    "# print('[{}] Finished X_basic1'.format(time.time() - start_time))\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# X_b = scaler.fit_transform(X_b)\n",
    "# X_basic = csr_matrix(X_b)\n",
    "# print('basic: ', X_basic.data.nbytes)\n",
    "# print('[{}] Finished X_basic2'.format(time.time() - start_time))\n",
    "# del X_b\n",
    "# for c in cols:\n",
    "#     merge = merge.drop(c, axis=1)\n",
    "# print_memory_usage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# jaccard and dice\n",
    "merge['n_id'] = merge['name'].astype('str') + '+++++_____+++++' + merge['item_description'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _jaccard_coef(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return _try_divide(float(len(A.intersection(B))), len(A.union(B)))\n",
    "\n",
    "\n",
    "def _dice_dist(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return _try_divide(2.*float(len(A.intersection(B))), (len(A) + len(B)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "def cyjaccard1(seq1, seq2):\n",
    "    cdef set set1 = set(seq1)\n",
    "    cdef set set2 = set()\n",
    "\n",
    "    cdef Py_ssize_t length_intersect = 0\n",
    "\n",
    "    for char in seq2:\n",
    "        if char not in set2:\n",
    "            if char in set1:\n",
    "                length_intersect += 1\n",
    "            set2.add(char)\n",
    "\n",
    "    return 1 - (length_intersect / float(len(set1) + len(set2) - length_intersect))\n",
    "\n",
    "def cydice1(seq1, seq2):\n",
    "    cdef set set1 = set(seq1)\n",
    "    cdef set set2 = set()\n",
    "\n",
    "    cdef Py_ssize_t length_intersect = 0\n",
    "\n",
    "    for char in seq2:\n",
    "        if char not in set2:\n",
    "            if char in set1:\n",
    "                length_intersect += 1\n",
    "            set2.add(char)\n",
    "\n",
    "    return ((2. * length_intersect) / float(len(set1) + len(set2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard_c(x):\n",
    "#     print('---------------------------------')\n",
    "#     print(x)\n",
    "    x1, x2 = x.split('+++++_____+++++')\n",
    "    x1_tokens = list(x1)\n",
    "    x2_tokens = list(x2)\n",
    "    j = 1-cyjaccard1(x1_tokens, x2_tokens)\n",
    "    return j\n",
    "\n",
    "def dice_c(x):\n",
    "#     print('---------------------------------')\n",
    "#     print(x)\n",
    "    x1, x2 = x.split('+++++_____+++++')\n",
    "    x1_tokens = list(x1)\n",
    "    x2_tokens = list(x2)\n",
    "    j = cydice1(x1_tokens, x2_tokens)\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_jaccard(df):\n",
    "    df['j_n_id'] = df['n_id'].map(lambda x: jaccard_c(str(x)))\n",
    "    return df\n",
    "\n",
    "def get_dice(df):\n",
    "    df['d_n_id'] = df['n_id'].map(lambda x: dice_c(str(x)))\n",
    "    return df\n",
    "\n",
    "def _is_str_match(str1, str2, threshold=1.0):\n",
    "    assert threshold >= 0.0 and threshold <= 1.0, \"Wrong threshold.\"\n",
    "    if float(threshold) == 1.0:\n",
    "        return str1 == str2\n",
    "    else:\n",
    "        return (1. - _edit_dist(str1, str2)) >= threshold\n",
    "\n",
    "    \n",
    "def _get_match_count(obs, target, idx):\n",
    "    cnt = 0\n",
    "    if (len(obs) != 0) and (len(target) != 0):\n",
    "        for word in target:\n",
    "            if _is_str_match(word, obs[idx], 0.85):\n",
    "                cnt += 1\n",
    "    return cnt\n",
    "\n",
    "def firstIntersectionNGram(obs_tokens, target_tokens, count):\n",
    "    \n",
    "#     obs_tokens = _tokenize(obs, token_pattern)\n",
    "#     target_tokens = _tokenize(target, token_pattern)\n",
    "    obs_ngrams = _ngrams(obs_tokens, count)\n",
    "    target_ngrams = _ngrams(target_tokens, count)\n",
    "    return _get_match_count(obs_ngrams, target_ngrams, 0)\n",
    "\n",
    "def lastIntersectionNGram(obs_tokens, target_tokens, count):\n",
    "#     obs_tokens = _tokenize(obs, token_pattern)\n",
    "#     target_tokens = _tokenize(target, token_pattern)\n",
    "    obs_ngrams = _ngrams(obs_tokens, count)\n",
    "    target_ngrams = _ngrams(target_tokens, count)\n",
    "    return _get_match_count(obs_ngrams, target_ngrams, -1)\n",
    "\n",
    "def get_first_i_n(x):\n",
    "    x1, x2 = x.split('+++++_____+++++')\n",
    "    x1_tokens = list(x1)\n",
    "    x2_tokens = list(x2)\n",
    "    return firstIntersectionNGram(x1_tokens, x2_tokens, 1)\n",
    "\n",
    "def get_last_i_n(x):\n",
    "    x1, x2 = x.split('+++++_____+++++')\n",
    "    x1_tokens = list(x1)\n",
    "    x2_tokens = list(x2)\n",
    "    return lastIntersectionNGram(x1_tokens, x2_tokens, 1)\n",
    "\n",
    "def get_first_int_ngram(df):\n",
    "    df['f_i_n'] = df['n_id'].map(lambda x: get_first_i_n(str(x)))\n",
    "    return df\n",
    "\n",
    "def get_last_int_ngram(df):\n",
    "    df['l_i_n'] = df['n_id'].map(lambda x: get_last_i_n(str(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 11.3 s, total: 1min 32s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merge = parallelize_dataframe(merge, get_jaccard)\n",
    "# merge = parallelize_dataframe(merge, get_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-118fabc7b356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmerge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'merge' is not defined"
     ]
    }
   ],
   "source": [
    "merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 56s, sys: 31.2 s, total: 3min 27s\n",
      "Wall time: 1h 56min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merge = parallelize_dataframe(merge, get_first_int_ngram)\n",
    "merge = parallelize_dataframe(merge, get_last_int_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j_n_id    0.056338\n",
      "f_i_n     1.000000\n",
      "l_i_n     1.000000\n",
      "dtype: float64\n",
      "j_n_id     1.0\n",
      "f_i_n     43.0\n",
      "l_i_n     43.0\n",
      "dtype: float64\n",
      "[7690.919023275375] Finished X_j\n",
      "cpu: 34.0\n",
      "consuming 27.67GB RAM\n"
     ]
    }
   ],
   "source": [
    "X_j = merge[['j_n_id', 'f_i_n', 'l_i_n']]\n",
    "del merge['n_id']\n",
    "print(np.min(X_j))\n",
    "print(np.max(X_j))\n",
    "print('[{}] Finished X_j'.format(time.time() - start_time))\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7691.370595693588] Finished X_j csr\n",
      "cpu: 9.8\n",
      "consuming 27.83GB RAM\n"
     ]
    }
   ],
   "source": [
    "X_j = csr_matrix(X_j)\n",
    "print('[{}] Finished X_j csr'.format(time.time() - start_time))\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge['name2'] = merge['name'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge['item_description2'] = merge['item_description'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8235.72233915329] Finished tag\n",
      "cpu: 2.7\n",
      "consuming 29.00GB RAM\n"
     ]
    }
   ],
   "source": [
    "abbr = {}\n",
    "abbr['BNWT'] = ['bnwt', 'brand new with tags']\n",
    "abbr['NWT'] = ['nwt', 'new with tags']\n",
    "abbr['BNWOT'] = ['bnwot', 'brand new with out tags', 'brand new without tags']\n",
    "abbr['NWOT'] = ['nwot', 'new with out tags', 'new without tags']\n",
    "abbr['BNIP'] = ['bnip', 'brand new in packet', 'brand new in packet']\n",
    "abbr['NIP'] = ['nip', 'new in packet', 'new in packet']\n",
    "abbr['BNIB'] = ['bnib', 'brand new in box']\n",
    "abbr['NIB'] = ['nib', 'new in box']\n",
    "abbr['MIB'] = ['mib', 'mint in box']\n",
    "abbr['MWOB'] = ['mwob', 'mint with out box', 'mint without box']\n",
    "abbr['MIP'] = ['mip', 'mint in packet']\n",
    "abbr['MWOP'] = ['mwop', 'mint with out packet', 'mint without packet']\n",
    "\n",
    "merge['tag'] = merge['item_description2'].map(lambda a: 'BNWT' if any(x in a.lower() for x in abbr['BNWT'])\n",
    "                                                                                   else 'NWT' if any(x in a.lower() for x in abbr['NWT'])\n",
    "                                                                                   else 'BNWOT' if any(x in a.lower() for x in abbr['BNWOT'])\n",
    "                                                                                   else 'NWOT' if any(x in a.lower() for x in abbr['NWOT'])\n",
    "                                                                                   else 'BNIP' if any(x in a.lower() for x in abbr['BNIP'])\n",
    "                                                                                   else 'NIP' if any(x in a.lower() for x in abbr['NIP'])\n",
    "                                                                                   else 'BNIB' if any(x in a.lower() for x in abbr['BNIB'])\n",
    "                                                                                   else 'NIB' if any(x in a.lower() for x in abbr['NIB'])\n",
    "                                                                                   else 'MIB' if any(x in a.lower() for x in abbr['MIB'])\n",
    "                                                                                   else 'MWOB' if any(x in a.lower() for x in abbr['MWOB'])\n",
    "                                                                                   else 'MIP' if any(x in a.lower() for x in abbr['MIP'])\n",
    "                                                                                   else 'MWOP' if any(x in a.lower() for x in abbr['MWOP'])\n",
    "                                                                                   else 'None')\n",
    "print('[{}] Finished tag'.format(time.time() - start_time))\n",
    "del abbr\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8284.722241163254] Finished creating bci bc bi ci bcs bcis\n",
      "cpu: 8.7\n",
      "consuming 30.94GB RAM\n",
      "[8305.616286754608] Finished to cut\n",
      "[8307.865171909332] Finished to convert categorical\n"
     ]
    }
   ],
   "source": [
    "merge['bci'] = merge['brand_name'].astype('str') + ' ' + merge['category_name'].astype('str') + ' ' + \\\n",
    "                        merge['item_condition_id'].astype('str')\n",
    "\n",
    "merge['bc'] = merge['brand_name'].astype('str') + ' ' + merge['category_name'].astype('str')\n",
    "\n",
    "merge['bcis'] = merge['brand_name'].astype('str') + ' ' \\\n",
    "                                + merge['category_name'].astype('str') + ' ' + \\\n",
    "                                merge['item_condition_id'].astype('str') + ' ' + \\\n",
    "                                merge['shipping'].astype('str')\n",
    "\n",
    "merge['bcs'] = merge['brand_name'].astype('str') + ' ' + \\\n",
    "                                merge['category_name'].astype('str') + ' ' + \\\n",
    "                                merge['shipping'].astype('str')\n",
    "\n",
    "# merge['bi'] = merge['brand_name'].astype('str') + '_' +   merge['item_condition_id'].astype('str')\n",
    "\n",
    "# merge['ci'] = merge['category_name'].astype('str') + '_' + merge['item_condition_id'].astype('str')\n",
    "\n",
    "print('[{}] Finished creating bci bc bi ci bcs bcis'.format(time.time() - start_time))\n",
    "print_memory_usage()\n",
    "\n",
    "\n",
    "# merge.drop(['bci', 'bc'], axis=1, inplace=True)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_sentiment_score)\n",
    "# merge['sentiment_score'] = merge['item_description'].map(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# print('[{}] Finished sentiment score'.format(time.time() - start_time))\n",
    "# a = merge['sentiment_score'].values\n",
    "# print(np.min(a))\n",
    "# print(np.max(a))\n",
    "\n",
    "# print_memory_usage()\n",
    "# merge['sentiment'] = merge['sentiment_score'].map(lambda x: 'VPos' if x > 0.5\n",
    "                                                                                                        # else 'Pos' if (x <= 0.5) and (x > 0)\n",
    "                                                                                                        # else 'Neu' if  x == 0\n",
    "                                                                                                        # else 'Neg' if (x < 0) and (x >= -0.5)\n",
    "                                                                                                        # else 'VNeg')\n",
    "\n",
    "# print('[{}] Finished sentiment'.format(time.time() - start_time))\n",
    "\n",
    "cutting(merge)\n",
    "print('[{}] Finished to cut'.format(time.time() - start_time))\n",
    "\n",
    "to_categorical(merge)\n",
    "print('[{}] Finished to convert categorical'.format(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8341.990030288696] Finished count vectorize `name`\n",
      "(4949330, 32989)\n",
      "0.0\n",
      "1.0\n",
      "cpu: 8.7\n",
      "consuming 31.47GB RAM\n",
      "[8371.144346475601] Finished count vectorize `category_name`\n",
      "(4949330, 1022)\n",
      "0.0\n",
      "1.0\n",
      "cpu: 8.7\n",
      "consuming 31.70GB RAM\n",
      "[8764.98168349266] Finished TFIDF vectorize `item_description`\n",
      "(4949330, 16384)\n",
      "0.0\n",
      "1.0\n",
      "cpu: 8.9\n",
      "consuming 36.67GB RAM\n",
      "[8795.687213897705] Finished label binarize `brand_name`\n",
      "(4949330, 4001)\n",
      "cpu: 8.9\n",
      "consuming 36.83GB RAM\n",
      "[8888.251164674759] Finished label binarize `bci`\n",
      "(4949330, 92981)\n",
      "cpu: 9.0\n",
      "consuming 36.90GB RAM\n",
      "[8982.92055773735] Finished label binarize `bcis`\n",
      "(4949330, 127794)\n",
      "cpu: 9.0\n",
      "consuming 36.95GB RAM\n",
      "[9081.005019426346] Finished label binarize `bcs`\n",
      "(4949330, 71722)\n",
      "cpu: 9.0\n",
      "consuming 37.03GB RAM\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=NAME_MIN_DF, stop_words='english')\n",
    "X_name = cv.fit_transform(merge['name2'])\n",
    "norm = Normalizer()\n",
    "X_name = norm.fit_transform(X_name)\n",
    "print('[{}] Finished count vectorize `name`'.format(time.time() - start_time))\n",
    "print(X_name.shape)\n",
    "print(np.min(X_name))\n",
    "print(np.max(X_name))\n",
    "del merge['name']\n",
    "print_memory_usage()\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_category = cv.fit_transform(merge['category_name'])\n",
    "norm = Normalizer()\n",
    "X_category = norm.fit_transform(X_category)\n",
    "print('[{}] Finished count vectorize `category_name`'.format(time.time() - start_time))\n",
    "print(X_category.shape)\n",
    "print(np.min(X_category))\n",
    "print(np.max(X_category))\n",
    "del merge['category_name']\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION,\n",
    "                                         ngram_range=(1, 3),\n",
    "                                         stop_words='english')\n",
    "X_description = tv.fit_transform(merge['item_description2'])\n",
    "print('[{}] Finished TFIDF vectorize `item_description`'.format(time.time() - start_time))\n",
    "print(X_description.shape)\n",
    "print(np.min(X_description))\n",
    "print(np.max(X_description))\n",
    "del merge['item_description']\n",
    "print_memory_usage()\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb.fit_transform(merge['brand_name'])\n",
    "print('[{}] Finished label binarize `brand_name`'.format(time.time() - start_time))\n",
    "print(X_brand.shape)\n",
    "del merge['brand_name']\n",
    "print_memory_usage()\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_bci = lb.fit_transform(merge['bci'])\n",
    "print('[{}] Finished label binarize `bci`'.format(time.time() - start_time))\n",
    "print(X_bci.shape)\n",
    "del merge['bci']\n",
    "print_memory_usage()\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_bcis = lb.fit_transform(merge['bcis'])\n",
    "print('[{}] Finished label binarize `bcis`'.format(time.time() - start_time))\n",
    "print(X_bcis.shape)\n",
    "del merge['bcis']\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_bcs = lb.fit_transform(merge['bcs'])\n",
    "print('[{}] Finished label binarize `bcs`'.format(time.time() - start_time))\n",
    "print(X_bcs.shape)\n",
    "del merge['bcs']\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9107.689462423325] Finished to get dummies on `item_condition_id` and `shipping`\n",
      "(4949330, 17)\n",
      "cpu: 8.7\n",
      "consuming 37.24GB RAM\n",
      "cpu: 8.9\n",
      "consuming 26.07GB RAM\n"
     ]
    }
   ],
   "source": [
    "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping',\n",
    "                                                                                        'tag']], sparse=True).values)\n",
    "print('[{}] Finished to get dummies on `item_condition_id` and `shipping`'.format(time.time() - start_time))\n",
    "print(X_dummies.shape)\n",
    "print_memory_usage()\n",
    "\n",
    "del merge\n",
    "gc.collect()\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j:  118783920\n",
      "bcis:  39594640\n",
      "bci:  39594640\n",
      "dummies:  96911280\n",
      "description:  763011728\n",
      "brand:  39594640\n",
      "category:  158521288\n",
      "name:  153527288\n",
      "[9135.312714576721] Finished to create sparse merge\n",
      "cpu: 8.9\n",
      "consuming 26.01GB RAM\n",
      "(1482535, 346913)\n",
      "cpu: 8.7\n",
      "consuming 28.04GB RAM\n",
      "cpu: 8.9\n",
      "consuming 26.01GB RAM\n"
     ]
    }
   ],
   "source": [
    "print('j: ', X_j.data.nbytes)\n",
    "# print('basic: ', X_basic.data.nbytes)\n",
    "print('bcis: ', X_bcis.data.nbytes)\n",
    "print('bci: ', X_bci.data.nbytes)\n",
    "print('dummies: ', X_dummies.data.nbytes)\n",
    "print('description: ', X_description.data.nbytes)\n",
    "print('brand: ', X_brand.data.nbytes)\n",
    "print('category: ', X_category.data.nbytes)\n",
    "print('name: ', X_name.data.nbytes)\n",
    "# print('name1: ', X_name1.data.nbytes)\n",
    "\n",
    "sparse_merge = hstack((X_j, X_bci, X_bcis, X_bcs, X_dummies, X_description, X_brand, X_category, X_name)).tocsr()\n",
    "print('[{}] Finished to create sparse merge'.format(time.time() - start_time))\n",
    "\n",
    "del X_j, X_bcis, X_bci, X_bcs, X_dummies, X_description, X_brand, X_category, X_name\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "X = sparse_merge[:nrow_train]\n",
    "X_test = sparse_merge[nrow_train:]\n",
    "\n",
    "print(X.shape)\n",
    "print_memory_usage()\n",
    "\n",
    "del sparse_merge\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu: 9.1\n",
      "consuming 26.41GB RAM\n",
      "cpu: 0.0\n",
      "consuming 26.41GB RAM\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's rmse: 0.489008\tvalid_1's rmse: 0.494383\n",
      "[1000]\ttraining's rmse: 0.46791\tvalid_1's rmse: 0.476431\n",
      "[1500]\ttraining's rmse: 0.456376\tvalid_1's rmse: 0.467865\n",
      "[2000]\ttraining's rmse: 0.448011\tvalid_1's rmse: 0.464196\n",
      "[2500]\ttraining's rmse: 0.441391\tvalid_1's rmse: 0.461019\n",
      "[3000]\ttraining's rmse: 0.436142\tvalid_1's rmse: 0.458279\n",
      "Early stopping, best iteration is:\n",
      "[3182]\ttraining's rmse: 0.434581\tvalid_1's rmse: 0.457517\n",
      "[9594.305319786072] Finished to train lgbm\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.01, random_state = 0)\n",
    "print_memory_usage()\n",
    "# d_train = lgb.Dataset(X, label=y, max_bin=8192)\n",
    "d_train = lgb.Dataset(train_X, label=train_y, max_bin=8192)\n",
    "d_valid = lgb.Dataset(valid_X, label=valid_y, max_bin=8192)\n",
    "watchlist = [d_train, d_valid]\n",
    "print_memory_usage()\n",
    "\n",
    "params = {\n",
    "        'learning_rate': 0.75,\n",
    "        'application': 'regression',\n",
    "        'max_depth': 3,\n",
    "        'num_leaves': 100,\n",
    "        'verbosity': -1,\n",
    "        'metric': 'RMSE',\n",
    "        'num_threads': 4\n",
    "}\n",
    "\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, valid_sets=watchlist,\n",
    "                                        num_boost_round=5000,early_stopping_rounds=100,verbose_eval=500)\n",
    "print('[{}] Finished to train lgbm'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('predicting..')\n",
    "preds = model.predict(X_test)\n",
    "print('[{}] Finished to train predict lgbm'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print('Deleting mem..')\n",
    "#del model, d_train, d_valid\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# submission=pd.DataFrame()\n",
    "# submission['test_id'] = test_id\n",
    "# submission['price'] = np.expm1(preds)\n",
    "# submission.to_csv(\"submission_lgbm_nlp2.csv\", index=False)\n",
    "preds *= 0.6\n",
    "# print('[{}] Finished submission lgbm'.format(time.time() - start_time))\n",
    "if nrow_test < 700000:\n",
    "        preds = preds[:nrow_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Ridge(solver=\"saga\", fit_intercept=True, random_state=205)\n",
    "model.fit(X, y)\n",
    "print('[{}] Finished to train ridge'.format(time.time() - start_time))\n",
    "preds1 = model.predict(X=X_test)\n",
    "print('[{}] Finished to predict ridge'.format(time.time() - start_time))\n",
    "# submission['price'] = np.expm1(preds1)\n",
    "# submission.to_csv(\"submission_ridge_nlp2.csv\", index=False)\n",
    "print_memory_usage()\n",
    "if nrow_test < 700000:\n",
    "        preds1 = preds1[:nrow_test]\n",
    "\n",
    "preds += 0.4*preds1\n",
    "submission['price'] = np.expm1(preds)\n",
    "# submission.to_csv(\"submission_lgbm_ridge_nlp2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kme",
   "language": "python",
   "name": "kme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
