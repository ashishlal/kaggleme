{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from textblob import TextBlob\n",
    "import lightgbm as lgb\n",
    "import os, psutil\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from collections import Counter\n",
    "import re\n",
    "import lzma\n",
    "import Levenshtein\n",
    "from numba import jit\n",
    "\n",
    "import sys\n",
    "sys.stdout = open('/dev/stdout', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "NAME_MIN_DF = 10\n",
    "MAX_FEATURES_ITEM_DESCRIPTION = 2 ** 14\n",
    "NUM_PARTITIONS = 12 #number of partitions to split dataframe\n",
    "NUM_CORES = 8 #number of cores on your machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "import random, copy, struct\n",
    "from hashlib import sha1\n",
    "\n",
    "# The size of a hash value in number of bytes\n",
    "hashvalue_byte_size = len(bytes(np.int64(42).data))\n",
    "\n",
    "# http://en.wikipedia.org/wiki/Mersenne_prime\n",
    "_mersenne_prime = (1 << 61) - 1\n",
    "_max_hash = (1 << 32) - 1\n",
    "_hash_range = (1 << 32)\n",
    "\n",
    "class MinHash(object):\n",
    "    '''MinHash is a probabilistic data structure for computing \n",
    "    `Jaccard similarity`_ between sets.\n",
    " \n",
    "    Args:\n",
    "        num_perm (int, optional): Number of random permutation functions.\n",
    "            It will be ignored if `hashvalues` is not None.\n",
    "        seed (int, optional): The random seed controls the set of random \n",
    "            permutation functions generated for this MinHash.\n",
    "        hashobj (optional): The hash function used by this MinHash. \n",
    "            It must implements\n",
    "            the `digest()` method similar to hashlib_ hash functions, such\n",
    "            as `hashlib.sha1`.\n",
    "        hashvalues (`numpy.array` or `list`, optional): The hash values is \n",
    "            the internal state of the MinHash. It can be specified for faster \n",
    "            initialization using the existing state from another MinHash. \n",
    "        permutations (optional): The permutation function parameters. This argument\n",
    "            can be specified for faster initialization using the existing\n",
    "            state from another MinHash.\n",
    "    \n",
    "    Note:\n",
    "        To save memory usage, consider using :class:`datasketch.LeanMinHash`.\n",
    "        \n",
    "    Note:\n",
    "        Since version 1.1.1, MinHash will only support serialization using \n",
    "        `pickle`_. ``serialize`` and ``deserialize`` methods are removed, \n",
    "        and are supported in :class:`datasketch.LeanMinHash` instead. \n",
    "        MinHash serialized before version 1.1.1 cannot be deserialized properly \n",
    "        in newer versions (`need to migrate? <https://github.com/ekzhu/datasketch/issues/18>`_). \n",
    "    Note:\n",
    "        Since version 1.1.3, MinHash uses Numpy's random number generator \n",
    "        instead of Python's built-in random package. This change makes the \n",
    "        hash values consistent across different Python versions.\n",
    "        The side-effect is that now MinHash created before version 1.1.3 won't\n",
    "        work (i.e., ``jaccard``, ``merge`` and ``union``)\n",
    "        with those created after. \n",
    "    .. _`Jaccard similarity`: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    .. _hashlib: https://docs.python.org/3.5/library/hashlib.html\n",
    "    .. _`pickle`: https://docs.python.org/3/library/pickle.html\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_perm=128, seed=1, hashobj=sha1,\n",
    "            hashvalues=None, permutations=None):\n",
    "        if hashvalues is not None:\n",
    "            num_perm = len(hashvalues)\n",
    "        if num_perm > _hash_range:\n",
    "            # Because 1) we don't want the size to be too large, and\n",
    "            # 2) we are using 4 bytes to store the size value\n",
    "            raise ValueError(\"Cannot have more than %d number of\\\n",
    "                    permutation functions\" % _hash_range)\n",
    "        self.seed = seed\n",
    "        self.hashobj = hashobj\n",
    "        # Initialize hash values\n",
    "        if hashvalues is not None:\n",
    "            self.hashvalues = self._parse_hashvalues(hashvalues)\n",
    "        else:\n",
    "            self.hashvalues = self._init_hashvalues(num_perm)\n",
    "        # Initalize permutation function parameters\n",
    "        if permutations is not None:\n",
    "            self.permutations = permutations\n",
    "        else:\n",
    "            generator = np.random.RandomState(self.seed)\n",
    "            # Create parameters for a random bijective permutation function\n",
    "            # that maps a 32-bit hash value to another 32-bit hash value.\n",
    "            # http://en.wikipedia.org/wiki/Universal_hashing\n",
    "            self.permutations = np.array([(generator.randint(1, _mersenne_prime, dtype=np.uint64),\n",
    "                                           generator.randint(0, _mersenne_prime, dtype=np.uint64))\n",
    "                                          for _ in range(num_perm)], dtype=np.uint64).T\n",
    "        if len(self) != len(self.permutations[0]):\n",
    "            raise ValueError(\"Numbers of hash values and permutations mismatch\")\n",
    "\n",
    "    def _init_hashvalues(self, num_perm):\n",
    "        return np.ones(num_perm, dtype=np.uint64)*_max_hash\n",
    "\n",
    "    def _parse_hashvalues(self, hashvalues):\n",
    "        return np.array(hashvalues, dtype=np.uint64)\n",
    "    @jit\n",
    "    def update(self, b):\n",
    "        '''Update this MinHash with a new value.\n",
    "        \n",
    "        Args:\n",
    "            b (bytes): The value of type `bytes`.\n",
    "            \n",
    "        Example:\n",
    "            To update with a new string value:\n",
    "            \n",
    "            .. code-block:: python\n",
    "                minhash.update(\"new value\".encode('utf-8'))\n",
    "        '''\n",
    "        hv = struct.unpack('<I', self.hashobj(b).digest()[:4])[0]\n",
    "        a, b = self.permutations\n",
    "        phv = np.bitwise_and((a * hv + b) % _mersenne_prime, np.uint64(_max_hash))\n",
    "        self.hashvalues = np.minimum(phv, self.hashvalues)\n",
    "    @jit\n",
    "    def jaccard(self, other):\n",
    "        '''Estimate the `Jaccard similarity`_ (resemblance) between the sets\n",
    "        represented by this MinHash and the other.\n",
    "        \n",
    "        Args:\n",
    "            other (datasketch.MinHash): The other MinHash.\n",
    "            \n",
    "        Returns:\n",
    "            float: The Jaccard similarity, which is between 0.0 and 1.0.\n",
    "        '''\n",
    "        if other.seed != self.seed:\n",
    "            raise ValueError(\"Cannot compute Jaccard given MinHash with\\\n",
    "                    different seeds\")\n",
    "        if len(self) != len(other):\n",
    "            raise ValueError(\"Cannot compute Jaccard given MinHash with\\\n",
    "                    different numbers of permutation functions\")\n",
    "        return np.float(np.count_nonzero(self.hashvalues==other.hashvalues)) /\\\n",
    "                np.float(len(self))\n",
    "    @jit\n",
    "    def count(self):\n",
    "        '''Estimate the cardinality count based on the technique described in\n",
    "        `this paper <http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=365694>`_.\n",
    "        \n",
    "        Returns:\n",
    "            int: The estimated cardinality of the set represented by this MinHash.\n",
    "        '''\n",
    "        k = len(self)\n",
    "        return np.float(k) / np.sum(self.hashvalues / np.float(_max_hash)) - 1.0\n",
    "    @jit\n",
    "    def merge(self, other):\n",
    "        '''Merge the other MinHash with this one, making this one the union\n",
    "        of both.\n",
    "        \n",
    "        Args:\n",
    "            other (datasketch.MinHash): The other MinHash.\n",
    "        '''\n",
    "        if other.seed != self.seed:\n",
    "            raise ValueError(\"Cannot merge MinHash with\\\n",
    "                    different seeds\")\n",
    "        if len(self) != len(other):\n",
    "            raise ValueError(\"Cannot merge MinHash with\\\n",
    "                    different numbers of permutation functions\")\n",
    "        self.hashvalues = np.minimum(other.hashvalues, self.hashvalues)\n",
    "    @jit\n",
    "    def digest(self):\n",
    "        '''Export the hash values, which is the internal state of the\n",
    "        MinHash.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.array: The hash values which is a Numpy array.\n",
    "        '''\n",
    "        return copy.copy(self.hashvalues)\n",
    "    @jit\n",
    "    def is_empty(self):\n",
    "        '''\n",
    "        Returns: \n",
    "            bool: If the current MinHash is empty - at the state of just\n",
    "                initialized.\n",
    "        '''\n",
    "        if np.any(self.hashvalues != _max_hash):\n",
    "            return False\n",
    "        return True\n",
    "    @jit\n",
    "    def clear(self):\n",
    "        '''\n",
    "        Clear the current state of the MinHash.\n",
    "        All hash values are reset.\n",
    "        '''\n",
    "        self.hashvalues = self._init_hashvalues(len(self))\n",
    "\n",
    "    @jit\n",
    "    def copy(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            datasketch.MinHash: A copy of this MinHash by exporting its\n",
    "                state.\n",
    "        '''\n",
    "        return MinHash(seed=self.seed, hashvalues=self.digest(),\n",
    "                permutations=self.permutations)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            int: The number of hash values.\n",
    "        '''\n",
    "        return len(self.hashvalues)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        '''\n",
    "        Returns:\n",
    "            bool: If their seeds and hash values are both equal then two\n",
    "                are equivalent.\n",
    "        '''\n",
    "        return self.seed == other.seed and \\\n",
    "                np.array_equal(self.hashvalues, other.hashvalues)\n",
    "                \n",
    "    @classmethod\n",
    "    @jit\n",
    "    def union(cls, *mhs):\n",
    "        '''Create a MinHash which is the union of the MinHash objects passed as arguments.\n",
    "        Args:\n",
    "            *mhs: The MinHash objects to be united. The argument list length is variable,\n",
    "                but must be at least 2.\n",
    "        \n",
    "        Returns:\n",
    "            datasketch.MinHash: A new union MinHash.\n",
    "        '''\n",
    "        if len(mhs) < 2:\n",
    "            raise ValueError(\"Cannot union less than 2 MinHash\")\n",
    "        num_perm = len(mhs[0])\n",
    "        seed = mhs[0].seed\n",
    "        if any((seed != m.seed or num_perm != len(m)) for m in mhs):\n",
    "            raise ValueError(\"The unioning MinHash must have the\\\n",
    "                    same seed and number of permutation functions\")\n",
    "        hashvalues = np.minimum.reduce([m.hashvalues for m in mhs])\n",
    "        permutations = mhs[0].permutations\n",
    "        return cls(num_perm=num_perm, seed=seed, hashvalues=hashvalues,\n",
    "                permutations=permutations)\n",
    "###################################################################################\n",
    "\n",
    "def rmsle(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n",
    "\n",
    "def split_cat(text):\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['category_name'] = dataset['category_name'].astype('category')\n",
    "    dataset['brand_name'] = dataset['brand_name'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n",
    "\n",
    "def print_memory_usage():\n",
    "    print('cpu: {}'.format(psutil.cpu_percent()))\n",
    "    print('consuming {:.2f}GB RAM'.format(\n",
    "    \t   psutil.Process(os.getpid()).memory_info().rss / 1073741824),\n",
    "    \t  flush=True)\n",
    "\n",
    "\n",
    "def _sigmoid(score):\n",
    "    p = 1. / (1. + np.exp(-score))\n",
    "    return p\n",
    "\n",
    "\n",
    "def _logit(p):\n",
    "    return np.log(p/(1.-p))\n",
    "\n",
    "\n",
    "def _softmax(score):\n",
    "    score = np.asarray(score, dtype=float)\n",
    "    score = np.exp(score - np.max(score))\n",
    "    score /= np.sum(score, axis=1)[:,np.newaxis]\n",
    "    return score\n",
    "\n",
    "\n",
    "def _cast_proba_predict(proba):\n",
    "    N = proba.shape[1]\n",
    "    w = np.arange(1,N+1)\n",
    "    pred = proba * w[np.newaxis,:]\n",
    "    pred = np.sum(pred, axis=1)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def _one_hot_label(label, n_classes):\n",
    "    num = label.shape[0]\n",
    "    tmp = np.zeros((num, n_classes), dtype=int)\n",
    "    tmp[np.arange(num),label.astype(int)] = 1\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def _majority_voting(x, weight=None):\n",
    "    ## apply weight\n",
    "    if weight is not None:\n",
    "    \tassert len(weight) == len(x)\n",
    "    \tx = np.repeat(x, weight)\n",
    "    c = Counter(x)\n",
    "    value, count = c.most_common()[0]\n",
    "    return value\n",
    "\n",
    "\n",
    "def _voter(x, weight=None):\n",
    "    idx = np.isfinite(x)\n",
    "    if sum(idx) == 0:\n",
    "    \tvalue = config.MISSING_VALUE_NUMERIC\n",
    "    else:\n",
    "    \tif weight is not None:\n",
    "    \t\tvalue = _majority_voting(x[idx], weight[idx])\n",
    "    \telse:\n",
    "    \t\tvalue = _majority_voting(x[idx])\n",
    "    return value\n",
    "\n",
    "\n",
    "def _array_majority_voting(X, weight=None):\n",
    "    y = np.apply_along_axis(_voter, axis=1, arr=X, weight=weight)\n",
    "    return y\n",
    "\n",
    "\n",
    "def _mean(x):\n",
    "    idx = np.isfinite(x)\n",
    "    if sum(idx) == 0:\n",
    "    \tvalue = float(config.MISSING_VALUE_NUMERIC) # cast it to float to accommodate the np.mean\n",
    "    else:\n",
    "    \tvalue = np.mean(x[idx]) # this is float!\n",
    "    return value\n",
    "\n",
    "\n",
    "def _array_mean(X):\n",
    "    y = np.apply_along_axis(_mean, axis=1, arr=X)\n",
    "    return y\n",
    "\n",
    "\n",
    "def _corr(x, y_train):\n",
    "    if _dim(x) == 1:\n",
    "    \tcorr = pearsonr(x.flatten(), y_train)[0]\n",
    "    \tif str(corr) == \"nan\":\n",
    "    \t\tcorr = 0.\n",
    "    else:\n",
    "    \tcorr = 1.\n",
    "    return corr\n",
    "\n",
    "\n",
    "def _dim(x):\n",
    "    d = 1 if len(x.shape) == 1 else x.shape[1]\n",
    "    return d\n",
    "\n",
    "@jit\n",
    "def _entropy(proba):\n",
    "    entropy = -np.sum(proba*np.log(proba))\n",
    "    return entropy\n",
    "\n",
    "@jit\n",
    "def _try_divide(x, y, val=0.0):\n",
    "    \"\"\"try to divide two numbers\"\"\"\n",
    "    if y != 0.0:\n",
    "    \tval = float(x) / y\n",
    "    return val\n",
    "\n",
    "@jit\n",
    "def _jaccard_coef(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return _try_divide(float(len(A.intersection(B))), len(A.union(B)))\n",
    "\n",
    "@jit\n",
    "def _dice_dist(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return _try_divide(2.*float(len(A.intersection(B))), (len(A) + len(B)))\n",
    "\n",
    "@jit    \n",
    "def entropy(obs, token_pattern=' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    counter = Counter(obs_tokens)\n",
    "    count = np.asarray(list(counter.values()))\n",
    "    proba = count/np.sum(count)\n",
    "    # del obs_tokens\n",
    "    return _entropy(proba)\n",
    "        \n",
    "def digit_count(obs):\n",
    "    return len(re.findall(r\"\\d\", obs))\n",
    "\n",
    "def digit_ratio(obs, token_pattern = ' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    return _try_divide(len(re.findall(r\"\\d\", obs)), len(obs_tokens))\n",
    "\n",
    "def emoji_count(obs):\n",
    "    return len(re.findall(r'[^\\w\\s,]', obs))\n",
    "\n",
    "def emoji_ratio(obs, token_pattern = ' '):\n",
    "    obs_tokens = obs.split(token_pattern)\n",
    "    return _try_divide(len(re.findall(r'[^\\w\\s,]', obs)), len(obs_tokens))\n",
    "\n",
    "@jit\n",
    "def _unigrams(words):\n",
    "    \"\"\"\n",
    "    \tInput: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "    \tOutput: a list of unigram\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "\n",
    "@jit\n",
    "def _bigrams(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "       Output: a list of bigram, e.g., [\"I_am\", \"am_Denny\"]\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "    \tlst = []\n",
    "    \tfor i in range(L-1):\n",
    "    \t\tfor k in range(1,skip+2):\n",
    "    \t\t\tif i+k < L:\n",
    "    \t\t\t\tlst.append( join_string.join([words[i], words[i+k]]) )\n",
    "    else:\n",
    "    \t# set it as unigram\n",
    "    \tlst = _unigrams(words)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def _trigrams(words, join_string, skip=0):\n",
    "    \"\"\"\n",
    "       Input: a list of words, e.g., [\"I\", \"am\", \"Denny\"]\n",
    "       Output: a list of trigram, e.g., [\"I_am_Denny\"]\n",
    "       I use _ as join_string for this example.\n",
    "    \"\"\"\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "    \tlst = []\n",
    "    \tfor i in range(L-2):\n",
    "    \t\tfor k1 in range(1,skip+2):\n",
    "    \t\t\tfor k2 in range(1,skip+2):\n",
    "    \t\t\t\tif i+k1 < L and i+k1+k2 < L:\n",
    "    \t\t\t\t\tlst.append( join_string.join([words[i], words[i+k1], words[i+k1+k2]]) )\n",
    "    else:\n",
    "    \t# set it as bigram\n",
    "    \tlst = _bigrams(words, join_string, skip)\n",
    "    return lst\n",
    "\n",
    "def UniqueCount_Ngram(obs, count, token_pattern=' '):\n",
    "    obs_tokens = obs.lower().split(token_pattern)\n",
    "    obs_ngrams = _ngrams(obs_tokens, count)\n",
    "    l = len(set(obs_ngrams))\n",
    "    del obs_tokens\n",
    "    del obs_ngrams\n",
    "    return l\n",
    "\n",
    "def UniqueRatio_Ngram(obs, count, token_pattern=' '):\n",
    "    obs_tokens = obs.lower().split(token_pattern)\n",
    "    obs_ngrams = _ngrams(obs_tokens, count)\n",
    "    r = _try_divide(len(set(obs_ngrams)), len(obs_ngrams))\n",
    "    del obs_tokens\n",
    "    del obs_ngrams\n",
    "    return r\n",
    "\n",
    "def _ngrams(words, ngram, join_string=\" \"):\n",
    "    \"\"\"wrapper for ngram\"\"\"\n",
    "    if ngram == 1:\n",
    "    \treturn _unigrams(words)\n",
    "    elif ngram == 2:\n",
    "    \treturn _bigrams(words, join_string)\n",
    "    elif ngram == 3:\n",
    "    \treturn _trigrams(words, join_string)\n",
    "    elif ngram == 4:\n",
    "    \treturn _fourgrams(words, join_string)\n",
    "    elif ngram == 12:\n",
    "    \tunigram = _unigrams(words)\n",
    "    \tbigram = [x for x in _bigrams(words, join_string) if len(x.split(join_string)) == 2]\n",
    "    \treturn unigram + bigram\n",
    "    elif ngram == 123:\n",
    "    \tunigram = _unigrams(words)\n",
    "    \tbigram = [x for x in _bigrams(words, join_string) if len(x.split(join_string)) == 2]\n",
    "    \ttrigram = [x for x in _trigrams(words, join_string) if len(x.split(join_string)) == 3]\n",
    "    \treturn unigram + bigram + trigram\n",
    "    \t\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, NUM_PARTITIONS)\n",
    "    pool = Pool(NUM_CORES)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def get_sentiment_score(df):\n",
    "    df['sentiment_score'] = df['item_description'].map(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "start_time = time.time()\n",
    "\n",
    "train = pd.read_table('../input/train.tsv', engine='c')\n",
    "test = pd.read_table('../input/test.tsv', engine='c')\n",
    "print('[{}] Finished to load data'.format(time.time() - start_time))\n",
    "print('Train shape: ', train.shape)\n",
    "print('Test shape: ', test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../cache/n_name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_name = list(df.col_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_name_rep = ['' for i in range(len(n_name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_name_dict = dict(zip(n_name, n_name_rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train['name'] = train['name'].replace(n_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['name'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for item in n_name:\n",
    "#     test['name'] = test['name'].str.replace(item, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrow_test = test.shape[0]\n",
    "\n",
    "test_id = test['test_id'].values\n",
    "submission: pd.DataFrame = test[['test_id']]\n",
    "\n",
    "if nrow_test < 700000:\n",
    "    test = pd.concat([test,test,test,test,test])\n",
    "    print('Test shape ', test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrow_train = train.shape[0]\n",
    "y = np.log1p(train[\"price\"])\n",
    "del train['price']\n",
    "merge: pd.DataFrame = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_cols = set(train.columns)\n",
    "del train\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "handle_missing_inplace(merge)\n",
    "print('[{}] Handle missing completed.'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_doclen_name(df):\n",
    "#     df['name_doclen'] = df['name'].map(lambda x: len(str(x).lower().split(' ')))\n",
    "#     return df\n",
    "\n",
    "# def get_doclen_itemdesc(df):\n",
    "#     df['item_description_doclen'] = df['item_description'].map(lambda x: len(str(x).lower().split(' ')))\n",
    "#     return df\n",
    "\n",
    "# def get_doclen_brand_name(df):\n",
    "#     df['brand_name_doclen'] = df['brand_name'].map(lambda x: len(str(x).lower().split(' ')))\n",
    "#     return df\n",
    "\n",
    "# def get_entropy_name(df):\n",
    "#     df['name_entropy'] = df['name'].map(lambda x: entropy(str(x).lower(), ' '))\n",
    "#     return df\n",
    "\n",
    "# def get_entropy_itemdesc(df):\n",
    "#     df['item_description_entropy'] = \\\n",
    "#     \tdf['item_description'].map(lambda x: entropy(str(x).lower(), ' '))\n",
    "#     return df\n",
    "\n",
    "# def get_entropy_brand_name(df):\n",
    "#     df['brand_name_entropy'] = \\\n",
    "#     \tdf['brand_name'].map(lambda x: entropy(str(x).lower(), ' '))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_count_name(df):\n",
    "#     df['name_dc'] = df['name'].map(lambda x: digit_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_count_itemdesc(df):\n",
    "#     df['item_description_dc'] = \\\n",
    "#     \tdf['item_description'].map(lambda x: digit_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_count_brand_name(df):\n",
    "#     df['brand_name_dc'] = \\\n",
    "#     \tdf['brand_name'].map(lambda x: digit_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_ratio_name(df):\n",
    "#     df['name_dr'] = df['name'].map(lambda x: digit_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_ratio_itemdesc(df):\n",
    "#     df['item_description_dr'] = \\\n",
    "#     \tdf['item_description'].map(lambda x: digit_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_digit_ratio_brand_name(df):\n",
    "#     df['brand_name_dr'] = \\\n",
    "#     \tdf['brand_name'].map(lambda x: digit_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_count_name(df):\n",
    "#     df['name_ec'] = df['name'].map(lambda x: emoji_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_count_itemdesc(df):\n",
    "#     df['item_description_ec'] = \\\n",
    "#     \tdf['item_description'].map(lambda x: emoji_count(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_count_brand_name(df):\n",
    "#     df['brand_name_ec'] = \\\n",
    "#     \tdf['brand_name'].map(lambda x: emoji_count(str(x).lower()))\n",
    "#     return df\n",
    "        \n",
    "# def get_emoji_ratio_name(df):\n",
    "#     df['name_er'] = df['name'].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_ratio_itemdesc(df):\n",
    "#     df['item_description_er'] = \\\n",
    "#     \tdf['item_description'].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# def get_emoji_ratio_brand_name(df):\n",
    "#     df['brand_name_er'] = \\\n",
    "#     \tdf['brand_name'].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "#     return df\n",
    "\n",
    "# cols1 = set(merge.columns)\n",
    "# cols = []\n",
    "# obs_fields = ['name', 'brand_name', 'item_description']\n",
    "# merge = parallelize_dataframe(merge, get_doclen_name)\n",
    "# merge = parallelize_dataframe(merge, get_doclen_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_doclen_brand_name)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_entropy_name)\n",
    "# merge = parallelize_dataframe(merge, get_entropy_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_entropy_brand_name)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_digit_count_name)\n",
    "# merge = parallelize_dataframe(merge, get_digit_count_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_digit_count_brand_name)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_digit_ratio_name)\n",
    "# merge = parallelize_dataframe(merge, get_digit_ratio_itemdesc)\n",
    "# merge = parallelize_dataframe(merge, get_digit_ratio_brand_name)\n",
    "\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_count_name)\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_count_itemdesc)\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_count_brand_name)\n",
    "\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_ratio_name)\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_ratio_itemdesc)\n",
    "# # merge = parallelize_dataframe(merge, get_emoji_ratio_brand_name)\n",
    "\n",
    "# print('[{}] Finished basic creation for name, bn, item_desc'.format(time.time() - start_time))\n",
    "\n",
    "# for f in obs_fields:\n",
    "#     counter = Counter(merge[f].values)\n",
    "#     merge[f+'_docfreq'] = merge[f].map(lambda x: counter[x])\n",
    "    \n",
    "#     cols.append(f+'_doclen')\n",
    "#     cols.append(f+'_docfreq')\n",
    "#     cols.append(f+'_docEntropy')\n",
    "#     cols.append(f+'_digitCount')\n",
    "#     cols.append(f+'_digitRatio')\n",
    "#     # cols.append(f+'_emojiCount')\n",
    "#     # cols.append(f+'_emojiRatio')\n",
    "\n",
    "# f = 'category_name'\n",
    "# def get_category_name_doclen(df):\n",
    "#     df[f+'_doclen'] = df[f].map(lambda x: len(str(x).lower().split('/')))\n",
    "#     return df\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_category_name_doclen)\n",
    "\n",
    "# counter = Counter(merge[f].values)\n",
    "# merge[f+'_docfreq'] = merge[f].map(lambda x: counter[x])\n",
    "\n",
    "# token_pattern = '/'\n",
    "\n",
    "# def get_category_name_entropy(df):\n",
    "# \tdf[f+'_docEntropy'] = df[f].map(lambda x: entropy(str(x).lower(),token_pattern))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_category_name_entropy)\n",
    "\n",
    "# def get_category_name_dc(df):\n",
    "# \tdf[f+'_dc'] = df[f].map(lambda x: digit_count(str(x).lower()))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_category_name_dc)\n",
    "\n",
    "# def get_category_name_dr(df):\n",
    "# \tdf[f+'_dr'] = df[f].map(lambda x: digit_ratio(str(x).lower(), token_pattern))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_category_name_dr)\n",
    "\n",
    "# def get_category_name_ec(df):\n",
    "# \tdf[f+'_emojiCount'] = df[f].map(lambda x: emoji_count(str(x).lower()))\n",
    "# \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_category_name_ec)\n",
    "\n",
    "# def get_category_name_er(df):\n",
    "# \tdf[f+'_emojiRatio'] = df[f].map(lambda x: emoji_ratio(str(x).lower()))\n",
    "# \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_category_name_er)\n",
    "\n",
    "# cols.append(f+'_doclen')\n",
    "# cols.append(f+'_docfreq')\n",
    "# cols.append(f+'_docEntropy')\n",
    "# cols.append(f+'_digitCount')\n",
    "# cols.append(f+'_digitRatio')\n",
    "# # cols.append(f+'_emojiCount')\n",
    "# # cols.append(f+'_emojiRatio')\n",
    "\n",
    "# print('[{}] Finished basic creation for category_name'.format(time.time() - start_time))\n",
    "\n",
    "# obs_fields = [\"name\", \"item_description\"]\n",
    "\n",
    "# # def get_onegram_uc_name(df):\n",
    "# # \tdf['name_1_uc'] = df['name'].map(lambda x: UniqueCount_Ngram(str(x), 1))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_onegram_uc_name)\n",
    "\n",
    "# # def get_onegram_uc_item_desc(df):\n",
    "# # \tdf['item_desc_1_uc'] = \\\n",
    "# # \t\tdf['item_description'].map(lambda x: UniqueCount_Ngram(str(x), 1))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_onegram_uc_item_desc)\n",
    "\n",
    "# # def get_onegram_ur_name(df):\n",
    "# # \tdf['name_1_ur'] = df['name'].map(lambda x: UniqueRatio_Ngram(str(x), 1))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_onegram_ur_name)\n",
    "\n",
    "# # def get_onegram_ur_item_desc(df):\n",
    "# # \tdf['item_desc_1_ur'] = \\\n",
    "# # \t\tdf['item_description'].map(lambda x: UniqueRatio_Ngram(str(x), 1))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_onegram_ur_item_desc)\n",
    "\n",
    "# def get_bigram_uc_name(df):\n",
    "# \tdf['name_2_uc'] = df['name'].map(lambda x: UniqueCount_Ngram(str(x), 2))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_bigram_uc_name)\n",
    "\n",
    "# def get_bigram_uc_item_desc(df):\n",
    "# \tdf['item_desc_2_uc'] = \\\n",
    "# \t\tdf['item_description'].map(lambda x: UniqueCount_Ngram(str(x), 2))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_bigram_uc_item_desc)\n",
    "\n",
    "# def get_bigram_ur_name(df):\n",
    "# \tdf['name_2_ur'] = df['name'].map(lambda x: UniqueRatio_Ngram(str(x), 2))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_bigram_ur_name)\n",
    "\n",
    "# def get_bigram_ur_item_desc(df):\n",
    "# \tdf['item_desc_2_ur'] = \\\n",
    "# \t\tdf['item_description'].map(lambda x: UniqueRatio_Ngram(str(x), 2))\n",
    "# \treturn df\n",
    "# merge = parallelize_dataframe(merge, get_bigram_ur_item_desc)\n",
    "\n",
    "# # def get_trigram_uc_name(df):\n",
    "# # \tdf['name_3_uc'] = df['name'].map(lambda x: UniqueCount_Ngram(str(x), 3))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_trigram_uc_name)\n",
    "\n",
    "# # def get_trigram_uc_item_desc(df):\n",
    "# # \tdf['item_desc_3_uc'] = \\\n",
    "# # \t\tdf['item_description'].map(lambda x: UniqueCount_Ngram(str(x), 3))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_trigram_uc_item_desc)\n",
    "\n",
    "# # def get_trigram_ur_name(df):\n",
    "# # \tdf['name_3_ur'] = df['name'].map(lambda x: UniqueRatio_Ngram(str(x), 3))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_trigram_ur_name)\n",
    "\n",
    "# # def get_trigram_ur_item_desc(df):\n",
    "# # \tdf['item_desc_3_ur'] = \\\n",
    "# # \t\tdf['item_description'].map(lambda x: UniqueRatio_Ngram(str(x), 3))\n",
    "# # \treturn df\n",
    "# # merge = parallelize_dataframe(merge, get_trigram_ur_item_desc)\n",
    "\n",
    "# # print('[{}] Finished ngram count for name, item_desc'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# # ngrams = [1,2,3]\n",
    "# # token_pattern =' '\n",
    "# # for f in obs_fields:\n",
    "# # \tfor n in ngrams:\n",
    "# # \t\tcols.append(f+'_{}_uc'.format(n))\n",
    "# # \t\tcols.append(f+'_{}_ur'.format(n))\n",
    "\n",
    "# # f = 'category_name'\n",
    "# # merge[f+'_{}_uc'.format(n)] = merge[f].map(lambda x: UniqueCount_Ngram(str(x), n, '/'))\n",
    "# # merge[f+'_{}_ur'.format(n)] = merge[f].map(lambda x: UniqueRatio_Ngram(str(x), n, '/'))\n",
    "# # cols.append(f+'_{}_uc'.format(n))\n",
    "# # cols.append(f+'_{}_ur'.format(n))\n",
    "\t\t\n",
    "# # remove constatnt cols\n",
    "# merge =  merge.loc[:, (merge != merge.iloc[0]).any()]\n",
    "# print(len(cols))\n",
    "# del cols\n",
    "# cols = list(set(merge.columns) - cols1)\n",
    "# print(len(cols))\n",
    "\n",
    "# X_b = merge[cols]\n",
    "\n",
    "# print('[{}] Finished X_basic1'.format(time.time() - start_time))\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# X_b = scaler.fit_transform(X_b)\n",
    "# X_basic = csr_matrix(X_b)\n",
    "# print('basic: ', X_basic.data.nbytes)\n",
    "# print('[{}] Finished X_basic2'.format(time.time() - start_time))\n",
    "# del X_b\n",
    "# for c in cols:\n",
    "#     merge = merge.drop(c, axis=1)\n",
    "# print_memory_usage()\n",
    "\n",
    "# # jaccard and dice\n",
    "# merge['n_id'] = merge['name'].astype('str') + '___' + merge['item_description'].astype('str')\n",
    "\n",
    "# from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# def jaccard_skl(text):\n",
    "#     obs, target = text.split('___')\n",
    "#     obs_tokens = obs.split(' ')\n",
    "#     target_tokens = target.split(' ')\n",
    "#     j1 = 1 - pairwise_distances(obs_tokens, target_tokens, metric = \"hamming\")\n",
    "#     del obs, target, target_tokens, obs_tokens\n",
    "#     return j1\n",
    "\n",
    "# @jit    \n",
    "# def jaccard_1(text):\n",
    "#     obs, target = text.split('___')\n",
    "#     obs_tokens = obs.split(' ')\n",
    "#     target_tokens = target.split(' ')\n",
    "#     obs_ngrams = _ngrams(obs_tokens, 1)\n",
    "#     target_ngrams = _ngrams(target_tokens, 1)\n",
    "#     j1 = _jaccard_coef(obs_ngrams, target_ngrams)\n",
    "#     # del obs, target, target_tokens, obs_tokens, obs_ngrams, target_ngrams\n",
    "#     return j1\n",
    "    \n",
    "# def jaccard_2(text):\n",
    "#     obs, target = text.split('___')\n",
    "#     obs_tokens = obs.split(' ')\n",
    "#     target_tokens = target.split(' ')\n",
    "#     obs_ngrams = _ngrams(obs_tokens, 2)\n",
    "#     target_ngrams = _ngrams(target_tokens, 2)\n",
    "#     j2 = _jaccard_coef(obs_ngrams, target_ngrams)\n",
    "#     del obs, target, target_tokens, obs_tokens, obs_ngrams, target_ngrams\n",
    "#     return j2\n",
    "\n",
    "# def jaccard_3(text):\n",
    "#     obs, target = text.split('___')\n",
    "#     obs_tokens = obs.split(' ')\n",
    "#     target_tokens = target.split(' ')\n",
    "#     obs_ngrams = _ngrams(obs_tokens, 3)\n",
    "#     target_ngrams = _ngrams(target_tokens, 3)\n",
    "#     j3 = _jaccard_coef(obs_ngrams, target_ngrams)\n",
    "#     del obs, target, target_tokens, obs_tokens, obs_ngrams, target_ngrams\n",
    "#     return j3\n",
    "\n",
    "# @jit\n",
    "# def jaccard_minhash(text):\n",
    "#     obs, target = text.split('___')\n",
    "#     obs_tokens = obs.split(' ')\n",
    "#     target_tokens = target.split(' ')\n",
    "#     m1, m2 = MinHash(), MinHash()\n",
    "#     for d in obs_tokens:\n",
    "#         m1.update(d.encode('utf8'))\n",
    "#     for d in target_tokens:\n",
    "#         m2.update(d.encode('utf8'))\n",
    "#     j = m1.jaccard(m2)\n",
    "#     return j\n",
    "\n",
    "# # def get_j2(df):\n",
    "#     # merge['j2'] = merge['n_id'].map(lambda x: jaccard_2(x))\n",
    "#     # return df\n",
    "# # merge = parallelize_dataframe(merge, get_j2)\n",
    "\n",
    "# def get_j1(df):\n",
    "#     merge['j1'] = merge['n_id'].map(lambda x: jaccard_1(x))\n",
    "#     return df\n",
    "# merge = parallelize_dataframe(merge, get_j1)\n",
    "\n",
    "# X_j = merge[['j1']]\n",
    "# del merge['n_id']\n",
    "# np.min(X_j)\n",
    "# np.max(X_j)\n",
    "# print('[{}] Finished X_j'.format(time.time() - start_time))\n",
    "# print_memory_usage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abbr = {}\n",
    "abbr['BNWT'] = ['bnwt', 'brand new with tags']\n",
    "abbr['NWT'] = ['nwt', 'new with tags']\n",
    "abbr['BNWOT'] = ['bnwot', 'brand new with out tags', 'brand new without tags']\n",
    "abbr['NWOT'] = ['nwot', 'new with out tags', 'new without tags']\n",
    "abbr['BNIP'] = ['bnip', 'brand new in packet', 'brand new in packet']\n",
    "abbr['NIP'] = ['nip', 'new in packet', 'new in packet']\n",
    "abbr['BNIB'] = ['bnib', 'brand new in box']\n",
    "abbr['NIB'] = ['nib', 'new in box']\n",
    "abbr['MIB'] = ['mib', 'mint in box']\n",
    "abbr['MWOB'] = ['mwob', 'mint with out box', 'mint without box']\n",
    "abbr['MIP'] = ['mip', 'mint in packet']\n",
    "abbr['MWOP'] = ['mwop', 'mint with out packet', 'mint without packet']\n",
    "\n",
    "merge['tag'] = merge['item_description'].map(lambda a: 'BNWT' if any(x in a.lower() for x in abbr['BNWT'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'NWT' if any(x in a.lower() for x in abbr['NWT'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'BNWOT' if any(x in a.lower() for x in abbr['BNWOT'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'NWOT' if any(x in a.lower() for x in abbr['NWOT'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'BNIP' if any(x in a.lower() for x in abbr['BNIP'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'NIP' if any(x in a.lower() for x in abbr['NIP'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'BNIB' if any(x in a.lower() for x in abbr['BNIB'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'NIB' if any(x in a.lower() for x in abbr['NIB'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'MIB' if any(x in a.lower() for x in abbr['MIB'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'MWOB' if any(x in a.lower() for x in abbr['MWOB'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'MIP' if any(x in a.lower() for x in abbr['MIP'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'MWOP' if any(x in a.lower() for x in abbr['MWOP'])\n",
    "\t\t\t\t\t\t\t\t\t\t   else 'None')\n",
    "print('[{}] Finished tag'.format(time.time() - start_time))\n",
    "del abbr\n",
    "print_memory_usage()\n",
    "\n",
    "merge['bci'] = merge['brand_name'].astype('str') + ' ' + merge['category_name'].astype('str') + ' ' + \\\n",
    "\t\t\tmerge['item_condition_id'].astype('str')\n",
    "\n",
    "merge['bc'] = merge['brand_name'].astype('str') + ' ' + merge['category_name'].astype('str')\n",
    "\n",
    "merge['bcis'] = merge['brand_name'].astype('str') + ' ' \\\n",
    "\t\t\t\t+ merge['category_name'].astype('str') + ' ' + \\\n",
    "\t\t\t\tmerge['item_condition_id'].astype('str') + ' ' + \\\n",
    "\t\t\t\tmerge['shipping'].astype('str')\n",
    "\n",
    "merge['bcs'] = merge['brand_name'].astype('str') + ' ' + \\\n",
    "\t\t\t\tmerge['category_name'].astype('str') + ' ' + \\\n",
    "\t\t\t\tmerge['shipping'].astype('str')\n",
    "\n",
    "# merge['bi'] = merge['brand_name'].astype('str') + '_' +   merge['item_condition_id'].astype('str')\n",
    "\t\n",
    "# merge['ci'] = merge['category_name'].astype('str') + '_' + merge['item_condition_id'].astype('str')\n",
    "\n",
    "print('[{}] Finished creating bci bc bi ci bcs bcis'.format(time.time() - start_time))\n",
    "print_memory_usage()\n",
    "\n",
    "\n",
    "# merge.drop(['bci', 'bc'], axis=1, inplace=True)\n",
    "\n",
    "# merge = parallelize_dataframe(merge, get_sentiment_score)\n",
    "# merge['sentiment_score'] = merge['item_description'].map(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# print('[{}] Finished sentiment score'.format(time.time() - start_time))\n",
    "# a = merge['sentiment_score'].values\n",
    "# print(np.min(a))\n",
    "# print(np.max(a))\n",
    "\n",
    "# print_memory_usage()\n",
    "# merge['sentiment'] = merge['sentiment_score'].map(lambda x: 'VPos' if x > 0.5 \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# else 'Pos' if (x <= 0.5) and (x > 0)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# else 'Neu' if  x == 0 \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# else 'Neg' if (x < 0) and (x >= -0.5)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# else 'VNeg')\n",
    "\n",
    "# print('[{}] Finished sentiment'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutting(merge)\n",
    "print('[{}] Finished to cut'.format(time.time() - start_time))\n",
    "\n",
    "to_categorical(merge)\n",
    "print('[{}] Finished to convert categorical'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tv = TfidfVectorizer(max_features=2 ** 14,\n",
    "#                      min_df=NAME_MIN_DF,\n",
    "# \t\t\t\t\t ngram_range=(1, 3),\n",
    "# \t\t\t\t\t stop_words='english')\n",
    "# X_name1 = tv.fit_transform(merge['name'])\n",
    "# print('[{}] Finished TFIDF vectorize `name`'.format(time.time() - start_time))\n",
    "# print(X_name1.shape)\n",
    "# print(np.min(X_name1))\n",
    "# print(np.max(X_name1))\n",
    "# # del merge['item_description']\n",
    "# print_memory_usage()\n",
    "\n",
    "# cv = CountVectorizer(min_df=NAME_MIN_DF, stop_words='english',  analyzer='char', ngram_range=(4,5)) # trans 1\n",
    "# cv = CountVectorizer(min_df=NAME_MIN_DF, analyzer='char', ngram_range=(3,10)) # trans 2\n",
    "# cv = CountVectorizer(analyzer='char', ngram_range=(4,9)) # trans 3\n",
    "# cv = CountVectorizer(analyzer='char_wb', ngram_range=(4,9)) # trans 4\n",
    "# cv = CountVectorizer(analyzer='char', ngram_range=(2,8)) # trans 5\n",
    "# cv = CountVectorizer(min_df=NAME_MIN_DF,analyzer='char', ngram_range=(4,9)) # trans 8\n",
    "cv = CountVectorizer(analyzer='char', ngram_range=(3,8)) # trans 8\n",
    "X_name = cv.fit_transform(merge['name'])\n",
    "norm = Normalizer()\n",
    "X_name = norm.fit_transform(X_name)\n",
    "print('[{}] Finished count vectorize `name`'.format(time.time() - start_time))\n",
    "print(X_name.shape)\n",
    "print(np.min(X_name))\n",
    "print(np.max(X_name))\n",
    "del merge['name']\n",
    "print_memory_usage()\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_category = cv.fit_transform(merge['category_name'])\n",
    "norm = Normalizer()\n",
    "X_category = norm.fit_transform(X_category)\n",
    "print('[{}] Finished count vectorize `category_name`'.format(time.time() - start_time))\n",
    "print(X_category.shape)\n",
    "print(np.min(X_category))\n",
    "print(np.max(X_category))\n",
    "del merge['category_name']\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "# cv = CountVectorizer()\n",
    "# X_bci_cv = cv.fit_transform(merge['bci'])\n",
    "# norm = Normalizer()\n",
    "# X_bci_cv = norm.fit_transform(X_bci_cv)\n",
    "# print('[{}] Finished count vectorize `X_bci_cv`'.format(time.time() - start_time))\n",
    "# print(X_bci_cv.shape)\n",
    "# print(np.min(X_bci_cv))\n",
    "# print(np.max(X_bci_cv))\n",
    "# del merge['bci']\n",
    "# gc.collect()\n",
    "# print_memory_usage()\n",
    "\n",
    "\n",
    "tv = TfidfVectorizer(max_features=2 ** 14,\n",
    "\t\t\t\t\t ngram_range=(1, 4), # trans 6\n",
    "#                      analyzer='char_wb', ngram_range=(4, 9), # trans 7\n",
    "#                      analyzer='char_wb', ngram_range=(4, 6), # trans 8\n",
    "\t\t\t\t\t stop_words='english')\n",
    "X_description = tv.fit_transform(merge['item_description'])\n",
    "print('[{}] Finished TFIDF vectorize `item_description`'.format(time.time() - start_time))\n",
    "print(X_description.shape)\n",
    "print(np.min(X_description))\n",
    "print(np.max(X_description))\n",
    "del merge['item_description']\n",
    "print_memory_usage()\n",
    "\n",
    "# X_cos = cosine_similarity(X_description, dense_output=False)\n",
    "# X_cos = squareform(pdist(np.asarray(X_description.toarray()), 'cosine'))\n",
    "# print(X_cos.shape)\n",
    "# print('[{}] Finished cosine similarity'.format(time.time() - start_time))\n",
    "# print_memory_usage()\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb.fit_transform(merge['brand_name'])\n",
    "print('[{}] Finished label binarize `brand_name`'.format(time.time() - start_time))\n",
    "print(X_brand.shape)\n",
    "del merge['brand_name']\n",
    "print_memory_usage()\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_bci = lb.fit_transform(merge['bci'])\n",
    "print('[{}] Finished label binarize `bci`'.format(time.time() - start_time))\n",
    "print(X_bci.shape)\n",
    "del merge['bci']\n",
    "print_memory_usage()\n",
    "\n",
    "# lb = LabelBinarizer(sparse_output=True)\n",
    "# X_bc = lb.fit_transform(merge['bc'])\n",
    "# print('[{}] Finished label binarize `bc`'.format(time.time() - start_time))\n",
    "# print(X_bc.shape)\n",
    "# del merge['bc']\n",
    "# print_memory_usage()\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_bcis = lb.fit_transform(merge['bcis'])\n",
    "print('[{}] Finished label binarize `bcis`'.format(time.time() - start_time))\n",
    "print(X_bcis.shape)\n",
    "del merge['bcis']\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_bcs = lb.fit_transform(merge['bcs'])\n",
    "print('[{}] Finished label binarize `bcs`'.format(time.time() - start_time))\n",
    "print(X_bcs.shape)\n",
    "del merge['bcs']\n",
    "gc.collect()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping', \n",
    "\t\t\t\t\t\t\t\t\t\t\t'tag']], sparse=True).values)\n",
    "print('[{}] Finished to get dummies on `item_condition_id` and `shipping`'.format(time.time() - start_time))\n",
    "print(X_dummies.shape)\n",
    "print_memory_usage()\n",
    "\n",
    "del merge\n",
    "gc.collect()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('basic: ', X_basic.data.nbytes)\n",
    "print('bcis: ', X_bcis.data.nbytes)\n",
    "print('bci: ', X_bci.data.nbytes)\n",
    "print('dummies: ', X_dummies.data.nbytes)\n",
    "print('description: ', X_description.data.nbytes)\n",
    "print('brand: ', X_brand.data.nbytes)\n",
    "print('category: ', X_category.data.nbytes)\n",
    "print('name: ', X_name.data.nbytes)\n",
    "# print('name1: ', X_name1.data.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_merge = hstack((X_bci, X_bcis, X_dummies, X_description, X_brand, X_category, X_name)).tocsr()\n",
    "print('[{}] Finished to create sparse merge'.format(time.time() - start_time))\n",
    "\n",
    "del X_bcis, X_bci, X_bcs, X_dummies, X_description, X_brand, X_category, X_name\n",
    "gc.collect()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = sparse_merge[:nrow_train]\n",
    "X_test = sparse_merge[nrow_train:]\n",
    "\n",
    "print(X.shape)\n",
    "print_memory_usage()\n",
    "\n",
    "del sparse_merge\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "np.random.seed(0)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.01, random_state = 0) \n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def batch_generator(X, y, batch_size):\n",
    "#     number_of_batches = samples_per_epoch/batch_size\n",
    "#     counter=0\n",
    "#     shuffle_index = np.arange(np.shape(y)[0])\n",
    "#     np.random.shuffle(shuffle_index)\n",
    "#     X =  X[shuffle_index, :]\n",
    "#     y =  y[shuffle_index]\n",
    "#     while 1:\n",
    "#         index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "#         X_batch = X[index_batch,:].todense()\n",
    "#         y_batch = y[index_batch]\n",
    "#         counter += 1\n",
    "#         yield(np.array(X_batch),y_batch)\n",
    "#         if (counter < number_of_batches):\n",
    "#             np.random.shuffle(shuffle_index)\n",
    "#             counter=0\n",
    "\n",
    "# from keras.datasets import reuters\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Activation\n",
    "# from keras.utils import np_utils\n",
    "\n",
    "# epochs=3\n",
    "# BATCH_SIZE = 512 * 4\n",
    "# def rmsle_cust(y_true, y_pred):\n",
    "#     first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "#     second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "#     return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n",
    "\n",
    "# def eval_model(model):\n",
    "#     val_preds = model.predict_generator(generator=batch_generator_x(valid_X, BATCH_SIZE),\n",
    "#                                steps=valid_X['name'].shape[0]//BATCH_SIZE + 1,\n",
    "#                               )\n",
    "                              \n",
    "#     val_preds = np.expm1(val_preds)\n",
    "    \n",
    "#     y_true = np.array(valid_y)\n",
    "#     y_pred = val_preds[:, 0]\n",
    "#     v_rmsle = rmsle(y_true, y_pred)\n",
    "#     print(\" RMSLE error on dev test: \"+str(v_rmsle))\n",
    "#     return v_rmsle\n",
    "    \n",
    "# print('Building model...')\n",
    "# model = Sequential()\n",
    "# model.add(Dense(512))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(64))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('linear'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error', optimizer=\"adam\")\n",
    "\n",
    "# model.fit_generator(generator=batch_generator(train_X, train_y, BATCH_SIZE),\n",
    "#                     nb_epoch=epochs, \n",
    "#                     samples_per_epoch=train_X['name'].shape[0]//BATCH_SIZE + 1)\n",
    "                    \n",
    "# rmsle = eval_model(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# d_train = lgb.Dataset(X, label=y, max_bin=8192)\n",
    "d_train = lgb.Dataset(train_X, label=train_y, max_bin=8192)\n",
    "d_valid = lgb.Dataset(valid_X, label=valid_y, max_bin=8192)\n",
    "watchlist = [d_train, d_valid]\n",
    "print_memory_usage()\n",
    "\n",
    "# params = {\n",
    "# \t'learning_rate': 0.75,\n",
    "# \t'application': 'regression',\n",
    "# \t'max_depth': 3,\n",
    "# \t'num_leaves': 100,\n",
    "# \t'verbosity': -1,\n",
    "# \t'metric': 'RMSE',\n",
    "# \t'num_threads': 4\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    'num_leaves': 31,\n",
    "    'objective': 'regression',\n",
    "    'min_data_in_leaf': 300,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 2,\n",
    "    'metric': 'RMSE',\n",
    "    'num_threads': 4\n",
    "}\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, valid_sets=watchlist,\n",
    "\t\t\t\t\tnum_boost_round=5000,early_stopping_rounds=100,verbose_eval=500) \n",
    "print('[{}] Finished to train lgbm'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "# [3154]\ttraining's rmse: 0.427377\tvalid_1's rmse: 0.452007 (after trans1)\n",
    "# [3535]\ttraining's rmse: 0.419829\tvalid_1's rmse: 0.450013 (after trans2)\n",
    "# [3206]\ttraining's rmse: 0.42536\tvalid_1's rmse: 0.449582 (after trans3)\n",
    "# [3058]\ttraining's rmse: 0.426589\tvalid_1's rmse: 0.452027 (trans4)\n",
    "# [2895]\ttraining's rmse: 0.424372\tvalid_1's rmse: 0.451957 (trans5)\n",
    "# [3374]\ttraining's rmse: 0.423522\tvalid_1's rmse: 0.450309 (trans3)\n",
    "# [3214]\ttraining's rmse: 0.425341\tvalid_1's rmse: 0.450888 (trans3, 6)\n",
    "# [2442]\ttraining's rmse: 0.432894\tvalid_1's rmse: 0.456348 (trans7, 8)\n",
    "# [3433]\ttraining's rmse: 0.420995\tvalid_1's rmse: 0.449706 (trans 8, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', colsample_bytree=1.0, learning_rate=0.75,\n",
       "       max_bin=255, max_depth=3, min_child_samples=20,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=5000,\n",
       "       n_jobs=-1, num_leaves=100, objective='regression',\n",
       "       random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "       subsample=1.0, subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = lgb.LGBMRegressor(objective='regression',\n",
    "                        num_leaves=100,\n",
    "                        max_depth=3,\n",
    "                        learning_rate=0.75, n_estimators=5000)\n",
    "gbm.fit(train_X, train_y,\n",
    "        eval_set=[(valid_X, valid_y)],\n",
    "        eval_metric='rmse',\n",
    "        early_stopping_rounds=100)\n",
    "\n",
    "# [2271]\tvalid_0's rmse: 0.460077 - org\n",
    "# after negative word replacemet: 0,46678\n",
    "# [3336]\tvalid_0's rmse: 0.451571 (after trans1)\n",
    "# [3588]\tvalid_0's rmse: 0.448598 (after trans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-cf9a4e654607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0851\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_220775\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 87.61%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0430\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_241158\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 88.72%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0376\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_241974\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.14%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0310\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_241834\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 90.76%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0283\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_258843\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.13%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0266\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_222034\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 91.90%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0234\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_222833\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.51%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0209\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_241592\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 92.84%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0196\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_241683\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.23%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0144\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_241249\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.64%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0130\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_220791\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.97%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0118\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_220776\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.18%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0112\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_247195\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.40%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0104\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_239103\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.41%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0104\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_239346\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.57%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0099\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_241404\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.70%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0095\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_239322\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.71%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0094\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_242109\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.73%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0094\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_242068\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 95.78%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0092\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Column_239524\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 95.78%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 272587 more &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "Explanation(estimator=\"LGBMRegressor(boosting_type='gbdt', colsample_bytree=1.0, learning_rate=0.75,\\n       max_bin=255, max_depth=3, min_child_samples=20,\\n       min_child_weight=0.001, min_split_gain=0.0, n_estimators=5000,\\n       n_jobs=-1, num_leaves=100, objective='regression',\\n       random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\\n       subsample=1.0, subsample_for_bin=200000, subsample_freq=1,\\n       verbose_eval=500)\", description='\\nLightGBM feature importances; values are numbers 0 <= x <= 1;\\nall values sum to 1.\\n', error=None, method='feature importances', is_regression=True, targets=None, feature_importances=FeatureImportances(importances=[FeatureWeight(feature='Column_220775', weight=0.08511531711867748, std=None, value=None), FeatureWeight(feature='Column_241158', weight=0.042953963796661901, std=None, value=None), FeatureWeight(feature='Column_241974', weight=0.037577056399840505, std=None, value=None), FeatureWeight(feature='Column_241834', weight=0.030988391552491362, std=None, value=None), FeatureWeight(feature='Column_258843', weight=0.028259514234511365, std=None, value=None), FeatureWeight(feature='Column_222034', weight=0.026632174012854688, std=None, value=None), FeatureWeight(feature='Column_222833', weight=0.023395914639280625, std=None, value=None), FeatureWeight(feature='Column_241592', weight=0.02090976819764117, std=None, value=None), FeatureWeight(feature='Column_241683', weight=0.019609724453365583, std=None, value=None), FeatureWeight(feature='Column_241249', weight=0.014420989462552897, std=None, value=None), FeatureWeight(feature='Column_220791', weight=0.012964973170652922, std=None, value=None), FeatureWeight(feature='Column_220776', weight=0.011846947554443103, std=None, value=None), FeatureWeight(feature='Column_247195', weight=0.011153385638209549, std=None, value=None), FeatureWeight(feature='Column_239103', weight=0.010434067326696732, std=None, value=None), FeatureWeight(feature='Column_239346', weight=0.010384158339112069, std=None, value=None), FeatureWeight(feature='Column_241404', weight=0.0098664435321379092, std=None, value=None), FeatureWeight(feature='Column_239322', weight=0.0094787853432882797, std=None, value=None), FeatureWeight(feature='Column_242109', weight=0.0094482238160592206, std=None, value=None), FeatureWeight(feature='Column_242068', weight=0.0093717609406325925, std=None, value=None), FeatureWeight(feature='Column_239524', weight=0.0092127549593650996, std=None, value=None)], remaining=272587), decision_tree=None, highlight_spaces=None, transition_features=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "eli5.explain_weights(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "print('[{}] Finished to train predict lgbm'.format(time.time() - start_time))\n",
    "del model, d_train, d_valid\n",
    "print_memory_usage()\n",
    "\n",
    "# submission=pd.DataFrame()\n",
    "# submission['test_id'] = test_id\n",
    "# submission['price'] = np.expm1(preds)\n",
    "# submission.to_csv(\"submission_lgbm_nlp2.csv\", index=False)\n",
    "preds *= 0.6\n",
    "# print('[{}] Finished submission lgbm'.format(time.time() - start_time))\n",
    "if nrow_test < 700000:\n",
    "\tpreds = preds[:nrow_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_ids, valid_ids in cv.split(X):\n",
    "    model = Ridge(\n",
    "        solver='auto',\n",
    "        fit_intercept=True,\n",
    "        alpha=0.5,\n",
    "        max_iter=100,\n",
    "        normalize=False,\n",
    "        tol=0.05)\n",
    "    model.fit(X_train[train_ids], y_train[train_ids])\n",
    "    y_pred_valid = model.predict(X_train[valid_ids])\n",
    "    rmsle = get_rmsle(y_pred_valid, y_train[valid_ids])\n",
    "    print(f'valid rmsle: {rmsle:.5f}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/ridge.py:319: UserWarning: In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "  warnings.warn(\"In Ridge, only 'sag' solver can currently fit the \"\n"
     ]
    }
   ],
   "source": [
    "model = Ridge(solver=\"saga\", fit_intercept=True, random_state=205)\n",
    "model.fit(X, y)\n",
    "print('[{}] Finished to train ridge'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3540aecc69f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meli5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/eli5/ipython.py\u001b[0m in \u001b[0;36mshow_weights\u001b[0;34m(estimator, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m     \u001b[0mformat_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplain_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_split_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mexpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplain_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexplain_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_as_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/singledispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mregistry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/eli5/sklearn/explain_weights.py\u001b[0m in \u001b[0;36mexplain_linear_regressor_weights\u001b[0;34m(reg, vec, top, target_names, targets, feature_names, coef_scale, feature_re, feature_filter)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mfeature_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mfeature_re\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_re\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m     \u001b[0m_extra_caveats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mHASHING_CAVEATS\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_invhashing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/eli5/sklearn/utils.py\u001b[0m in \u001b[0;36mget_feature_names_filtered\u001b[0;34m(clf, vec, bias_name, feature_names, num_features, feature_filter, feature_re, estimator_feature_names)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mestimator_feature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator_feature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_re\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/eli5/sklearn/utils.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(clf, vec, bias_name, feature_names, num_features, estimator_feature_names)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_feature_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mFeatureNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnz\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             raise ValueError(\"The truth value of an array with more than one \"\n\u001b[0m\u001b[1;32m    259\u001b[0m                              \"element is ambiguous. Use a.any() or a.all().\")\n\u001b[1;32m    260\u001b[0m     \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "eli5.show_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds1 = model.predict(X=X_test)\n",
    "print('[{}] Finished to predict ridge'.format(time.time() - start_time))\n",
    "# submission['price'] = np.expm1(preds1)\n",
    "# submission.to_csv(\"submission_ridge_nlp2.csv\", index=False)\n",
    "print_memory_usage()\n",
    "if nrow_test < 700000:\n",
    "\tpreds1 = preds1[:nrow_test]\n",
    "\t\n",
    "preds += 0.4*preds1\n",
    "submission['price'] = np.expm1(preds)\n",
    "submission.to_csv(\"../cache/submission_lgbm_ridge_nlp2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
